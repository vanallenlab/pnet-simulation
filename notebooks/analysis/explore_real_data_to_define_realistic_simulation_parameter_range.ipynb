{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which allele frequency (AF) values should we choose for our simulated data?\n",
    "- look at the distribution of AF values from the germline and somatic mutation matrices (samples x gene-level; binary genotype matrix)\n",
    "- choose values near the extremes (can always fill in values later)\n",
    "\n",
    "In paper, will reference the \"control frequency\" instead of allele frequency, which we define as the proportion of samples in class 0 (primary cancer) that have an event (mutation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from pnet.data_processing import prostate_data_loaders\n",
    "\n",
    "# Setup Logging and Configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading genetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data (paired input data, germline and somatic)\n",
    "# data_dir = \"/mnt/disks/gmiller_data1/pnet_germline/data/prostate/paired_germline_somatic/\"\n",
    "# data_dir = \"/mnt/disks/gmiller_data1/pnet_germline/processed/wandb-group-data_prep_germline_tier12_and_somatic/converted-IDs-to-somatic_imputed-germline_True_imputed-somatic_False_paired-samples-True/wandb-run-id-q151d0zw\"\n",
    "# somatic_f = os.path.join(data_dir, \"somatic_mut.csv\")\n",
    "# germline_f = os.path.join(data_dir, \"germline_rare_common_lof_missense.csv\")\n",
    "\n",
    "somatic_datadir = \"/mnt/disks/gmiller_data1/pnet_germline/data/pnet_database/prostate/processed\"\n",
    "germline_datadir = \"/mnt/disks/gmiller_data1/pnet_germline/data\"\n",
    "\n",
    "somatic_df = prostate_data_loaders.get_somatic_mutation(\n",
    "            os.path.join(somatic_datadir, \"P1000_final_analysis_set_cross_important_only.csv\"))\n",
    "\n",
    "# Unfiltered germline data, accidentally used in my previous analysis\n",
    "# germline_df = prostate_data_loaders.get_germline_mutation(\n",
    "#             os.path.join(\n",
    "#                 germline_datadir,\n",
    "#                 \"prostate/prostate_germline_vcf_subset_to_germline_tier_12_and_somatic_passed-universal-filters_rare_common_high-impact_moderate-impact.txt\",\n",
    "#             ))\n",
    "\n",
    "\n",
    "germline_df = prostate_data_loaders.get_germline_mutation(\n",
    "            os.path.join(\n",
    "                germline_datadir,\n",
    "                \"prostate/prostate_germline_vcf_subset_to_germline_tier_12_and_somatic_passed-universal-filters_patho-vars-only_rare_common_high-impact_moderate-impact.txt\",\n",
    "            ))\n",
    "\n",
    "y = pd.read_csv(os.path.join(somatic_datadir, \"response_paper.csv\"))\n",
    "y.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonized_data_dir = \"/mnt/disks/gmiller_data1/pnet_germline/processed/wandb-group-data_prep_germline_tier12_and_somatic/converted-IDs-to-somatic_imputed-germline_True_imputed-somatic_False_paired-samples-True/wandb-run-id-u5yt90p1\"\n",
    "somatic_df = pd.read_csv(os.path.join(harmonized_data_dir, \"somatic_mut.csv\"), index_col=0)\n",
    "somatic_df = somatic_df = somatic_df.loc[:, (somatic_df != 0).any(axis=0)]\n",
    "\n",
    "germline_df = pd.read_csv(os.path.join(harmonized_data_dir, \"germline_rare_common_lof_missense.csv\"), index_col=0)\n",
    "germline_df = germline_df.loc[:, (germline_df != 0).any(axis=0)]\n",
    "y = pd.read_csv(os.path.join(harmonized_data_dir, \"y.csv\"), index_col=0)\n",
    "y.rename(columns={\"is_met\": \"class\"}, inplace=True)\n",
    "\n",
    "print(\"somatic_df.shape\", somatic_df.shape)\n",
    "print(\"germline_df.shape\", germline_df.shape)\n",
    "print(\"y.shape\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show basic info about the data\n",
    "print(\"Somatic data shape:\", somatic_df.shape)\n",
    "print(\"Germline data shape:\", germline_df.shape)\n",
    "print(\"First 5 columns of somatic data:\", somatic_df.columns[:5].tolist())\n",
    "print(\"First 5 columns of germline data:\", germline_df.columns[:5].tolist())\n",
    "print(\"First 5 rows of somatic data:\", somatic_df.index[:5].tolist())\n",
    "print(\"First 5 rows of germline data:\", germline_df.index[:5].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allele frequencies by class and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_afs(binary_df):\n",
    "    \"\"\"Compute allele frequencies for each gene (mean across samples).\"\"\"\n",
    "    return binary_df.mean(axis=0)\n",
    "\n",
    "def compute_deciles(afs):\n",
    "    \"\"\"Compute deciles (10th to 90th percentile) from allele frequencies.\"\"\"\n",
    "    deciles = afs.quantile([i / 10 for i in range(0, 11)]).reset_index()\n",
    "    deciles.columns = ['Decile', 'AF']\n",
    "    return deciles\n",
    "\n",
    "\n",
    "def plot_af_histogram(afs, title=\"Allele Frequency Distribution\"):\n",
    "    \"\"\"Plot histogram of allele frequencies.\"\"\"\n",
    "    plt.figure()\n",
    "    sns.histplot(afs, bins=30, kde=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Allele Frequency\")\n",
    "    plt.ylabel(\"Number of Genes\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_af_boxplot(afs, title=\"Allele Frequency Boxplot\"):\n",
    "    \"\"\"Plot boxplot of allele frequencies.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.boxplot(afs, vert=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Allele Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_af_distribution(binary_df, label=\"Dataset\"):\n",
    "    afs = compute_afs(binary_df)\n",
    "    deciles = compute_deciles(afs)\n",
    "    deciles.rename(columns={'AF': f'{label} AF'}, inplace=True)\n",
    "    plot_af_histogram(afs, title=f\"{label} - Histogram\")\n",
    "    plot_af_boxplot(afs, title=f\"{label} - Boxplot\")\n",
    "    return afs, deciles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call analyze_af_distribution on somatic and germline data\n",
    "afs_somatic, deciles_somatic = analyze_af_distribution(somatic_df, label=\"Somatic\")\n",
    "afs_germline, deciles_germline = analyze_af_distribution(germline_df, label=\"Germline\")\n",
    "display(deciles_somatic)\n",
    "display(deciles_germline)\n",
    "\n",
    "merged_df = pd.merge(deciles_somatic, deciles_germline, on='Decile')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proportions(somatic_mut, y, gene_list):\n",
    "    logger.info(\"Group by class and compute mean proportions\")\n",
    "    # Align y to somatic_mut by index\n",
    "    y_aligned = y.loc[somatic_mut.index]\n",
    "    \n",
    "    # Subset to genes of interest\n",
    "    gene_df = somatic_mut[gene_list]\n",
    "    \n",
    "    # Combine with response variable\n",
    "    combined = gene_df.copy()\n",
    "    combined['class'] = y_aligned\n",
    "    \n",
    "    # Group by class and compute proportions (mean of binary values)\n",
    "    proportions = combined.groupby('class').mean().T\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "def calculate_proportions_with_rr(somatic_mut, y, gene_list, metric=):\n",
    "    proportions = calculate_proportions(somatic_mut, y, gene_list)\n",
    "    \n",
    "    logger.warn(\"Compute OR (class 1 proportion / class 0 proportion) <-- warning this is relative risk not OR!!\")\n",
    "    proportions['OR'] = proportions[1] / proportions[0]\n",
    "    \n",
    "    return proportions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR, TP53, PTEN, APC, GNAS, RAC1, OBSCN, MAML3, MUC4\n",
    "# check AF / OR of genes P-NET finds important for somatic data\n",
    "af_by_class = calculate_proportions_with_or(somatic_df, y, ['AR', 'TP53', 'PTEN', 'APC', 'GNAS', 'RAC1', 'OBSCN', 'MAML3', 'MUC4'])\n",
    "af_by_class.sort_values(by='OR', ascending=False).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 20 germline by AF\")\n",
    "afs_germline.nlargest(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 genes with highest allele frequency\n",
    "top_5_somatic = afs_somatic.nlargest(10)\n",
    "top_5_germline = afs_germline.nlargest(10)\n",
    "print(\"Top 10 somatic genes with highest allele frequency:\")\n",
    "print(top_5_somatic)\n",
    "print(\"Top 10 germline genes with highest allele frequency:\")\n",
    "print(top_5_germline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# germline genes with AF = 0 (genes which have no mutations at all. Maybe we should exclude these genes from the analysis? But we'd zero-mpute them anyway when combining with somatic)\n",
    "afs_germline[afs_germline == 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore gene-gene correlation matrix by class and dataset\n",
    "- class: met vs primary (1 vs 0)\n",
    "- dataset: somatic vs germline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hcluster_corr_matrix(corr_matrix):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on a correlation matrix and plot the dendrogram.\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "    from scipy.spatial.distance import squareform\n",
    "\n",
    "    # Compute the distance matrix\n",
    "    dist_matrix = squareform(1 - corr_matrix)\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(dist_matrix, method='average')\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    dendrogram(Z, labels=corr_matrix.columns, leaf_rotation=90)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lowrank_matrix(X, top=20):\n",
    "    eigvals, eigvecs = np.linalg.eigh(X)\n",
    "    L = eigvecs[:, -top:] @ np.diag(np.sqrt(eigvals[-top:]))\n",
    "    Sigma_lowrank = L @ L.T\n",
    "    return Sigma_lowrank\n",
    "\n",
    "def get_scree_plot(X, max_num_eigenvalues=20, title='Scree Plot'):\n",
    "    \"\"\"\n",
    "    Plot the scree plot of the eigenvalues of the covariance matrix.\n",
    "    \"\"\"\n",
    "    eigvals, _ = np.linalg.eigh(X)\n",
    "    # Sort in descending order\n",
    "    eigvals_sorted = eigvals[::-1]\n",
    "\n",
    "    # Proportion of variance explained\n",
    "    explained_variance = eigvals_sorted / eigvals_sorted.sum()\n",
    "    explained_variance = explained_variance[:max_num_eigenvalues]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(explained_variance)+1), explained_variance, marker='o')\n",
    "    plt.xlabel('Eigenvalue Rank')\n",
    "    plt.ylabel('Proportion of Variance Explained')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_cumulative_variance_plot(X, max_num_eigenvalues=20, title='Cumulative Variance Plot'):\n",
    "    \"\"\"\n",
    "    Plot the cumulative variance explained by the eigenvalues of the covariance matrix.\n",
    "    \"\"\"\n",
    "    eigvals, _ = np.linalg.eigh(X)\n",
    "    # Sort in descending order\n",
    "    eigvals_sorted = eigvals[::-1]\n",
    "\n",
    "    # Cumulative variance\n",
    "    explained_variance = eigvals_sorted / eigvals_sorted.sum()\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    # cumulative_variance = np.cumsum(eigvals_sorted) / np.sum(eigvals_sorted)\n",
    "\n",
    "    cumulative_variance = cumulative_variance[:max_num_eigenvalues]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(np.arange(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "    plt.xlabel('Eigenvalue Rank')\n",
    "    plt.ylabel('Cumulative Variance Explained')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def num_components_for_variance(eigvals, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Returns the minimum number of eigenvalues needed to reach a given cumulative variance threshold.\n",
    "    \n",
    "    Parameters:\n",
    "        eigvals (array-like): Eigenvalues (not necessarily sorted).\n",
    "        threshold (float): Cumulative variance threshold (between 0 and 1).\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of components required to reach the threshold.\n",
    "    \"\"\"\n",
    "    eigvals = np.array(eigvals)\n",
    "    eigvals_sorted = np.sort(eigvals)[::-1]  # descending order\n",
    "    cumulative = np.cumsum(eigvals_sorted) / eigvals_sorted.sum()\n",
    "    num_components = np.searchsorted(cumulative, threshold) + 1\n",
    "    return num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = somatic_df.copy()\n",
    "denoise=False # Set to True to denoise the correlation matrices by low-rank approximation\n",
    "\n",
    "\n",
    "X_class0 = X.loc[y[y[\"class\"] == 0].index]\n",
    "X_class1 = X.loc[y[y[\"class\"] == 1].index]\n",
    "\n",
    "# Drop genes with all 0s in their respective classes\n",
    "X_class0 = X_class0.loc[:, X_class0.sum(axis=0) > 0]\n",
    "X_class1 = X_class1.loc[:, X_class1.sum(axis=0) > 0]\n",
    "\n",
    "# Ensure both classes have the same set of genes\n",
    "common_genes = X_class0.columns.intersection(X_class1.columns)\n",
    "X_class0 = X_class0[common_genes]\n",
    "X_class1 = X_class1[common_genes]\n",
    "\n",
    "\n",
    "corr_class0 = pd.DataFrame(np.corrcoef(X_class0, rowvar=False), columns=X_class0.columns, index=X_class0.columns)\n",
    "corr_class1 = pd.DataFrame(np.corrcoef(X_class1, rowvar=False), columns=X_class1.columns, index=X_class1.columns)\n",
    "\n",
    "if denoise:\n",
    "    corr_class0.values = get_lowrank_matrix(corr_class0.values, top=10)\n",
    "    corr_class1.values = get_lowrank_matrix(corr_class1.values, top=10)\n",
    "    \n",
    "corr_diff = corr_class1 - corr_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = somatic_df.copy()\n",
    "# denoise=False # Set to True to denoise the correlation matrices by low-rank approximation\n",
    "\n",
    "\n",
    "# X_class0 = X.loc[y[y[\"class\"] == 0].index]\n",
    "# X_class1 = X.loc[y[y[\"class\"] == 1].index]\n",
    "\n",
    "# print('MDM4' in X_class0.columns.tolist())\n",
    "\n",
    "# # get the mean of MDM4 column in class 0\n",
    "# mean_MDM4_class0 = X_class0.mean(axis=0)['MDM4']\n",
    "# # get the mean of MDM4 column in class 1\n",
    "# mean_MDM4_class1 = X_class1.mean(axis=0)['MDM4']\n",
    "# print(f\"Mean MDM4 in class 0: {mean_MDM4_class0}\")\n",
    "# print(f\"Mean MDM4 in class 1: {mean_MDM4_class1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue structure of the class-specific gene-gene correlation matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals = np.linalg.eigvalsh(corr_class0)\n",
    "k = num_components_for_variance(eigvals, threshold=0.6)\n",
    "print(f\"Number of components to explain 60% of variance in class 0 correlation matrix: {k}\")\n",
    "\n",
    "\n",
    "eigvals = np.linalg.eigvalsh(corr_class1)\n",
    "k = num_components_for_variance(eigvals, threshold=0.6)\n",
    "print(f\"Number of components to explain 60% of variance in class 1 correlation matrix: {k}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scree_plot(corr_class0, title='Scree Plot of Class 0')\n",
    "get_scree_plot(corr_class1, title='Scree Plot of Class 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cumulative_variance_plot(corr_class0, title='Cumulative Variance Plot of Class 0')\n",
    "get_cumulative_variance_plot(corr_class1, title='Cumulative Variance Plot of Class 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation structure of genes that P-NET assigns high importance score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"millergw/prostate_met_status\"\n",
    "GROUP_NAME = \"pnet_somatic_and_germline_exp_004\"\n",
    "SWEEP_ID = \"rv4lm363\" # germline filtered for pathogenicity\n",
    "# Define directories for saving results\n",
    "# FIGDIR = f\"../figures/{GROUP_NAME}/\"\n",
    "# RESULTS_DIR = f\"../results/{GROUP_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the runs\n",
    "def fetch_wandb_runs(project_name, sweep_id):\n",
    "    api = wandb.Api()\n",
    "    runs = api.runs(project_name, filters={\"sweep\": sweep_id, \"state\": \"finished\"})\n",
    "    return runs\n",
    "\n",
    "def fetch_feature_importance_paths(runs, group_name, who=\"validation\"):\n",
    "    # You can use this DataFrame to access the paths to the gene x modality importances files\n",
    "\n",
    "    # Create an empty list to store DataFrames\n",
    "    dfs = []\n",
    "    for run in runs:\n",
    "        model_type = run.config.get(\"model_type\", \"unknown\")\n",
    "        datasets = run.config.get(\"datasets\", \"unknown\")\n",
    "        subdir_name = f\"{model_type}_eval_set_{who}\"\n",
    "\n",
    "        # Paths to feature and gene importances files\n",
    "        feature_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_feature_importances.csv'\n",
    "        gene_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_importances.csv'\n",
    "\n",
    "        # Create a DataFrame for the current run\n",
    "        df = pd.DataFrame({\n",
    "            \"run_id\": [run.id],\n",
    "            \"model_type\": [model_type],\n",
    "            \"datasets\": [datasets],\n",
    "            \"feature_importances_path\": [feature_importances_path],\n",
    "            \"gene_importances_path\": [gene_importances_path],\n",
    "        })\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list into a single DataFrame\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.set_index(\"run_id\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_feature_importances(importances_path):\n",
    "    \"\"\"\n",
    "    Load feature importance data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        importances_path (str): Path to the feature importance file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of feature importances.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(importances_path):\n",
    "        raise FileNotFoundError(f\"File not found: {importances_path}\")\n",
    "    return pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "\n",
    "\n",
    "def process_importances(imps, response_df):\n",
    "    \"\"\"\n",
    "    Process feature importances by joining with response data and calculating differences between sample classes.\n",
    "    This function computes the mean feature importances for each response class and then calculates the difference between them.\n",
    "    The result is a Series of feature importance differences.\n",
    "\n",
    "    Args:\n",
    "        imps (pd.DataFrame): Feature importance DataFrame.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Processed feature importance differences.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"head of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().head()}\")\n",
    "    logger.debug(f\"shape of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().shape}\")\n",
    "    return imps.join(response_df).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "\n",
    "def process_feature_importances(df_feature_importance_paths, response_df):\n",
    "    \"\"\"\n",
    "    Process feature importance data for multiple runs and group by dataset.\n",
    "\n",
    "    Args:\n",
    "        df_feature_importance_paths (pd.DataFrame): DataFrame with feature importance paths and dataset info.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionaries of DataFrames for feature importances and ranks grouped by the input datasets used in the model.\n",
    "    \"\"\"\n",
    "    imps_by_key = {}\n",
    "    ranks_by_key = {}\n",
    "\n",
    "    for _, row in df_feature_importance_paths.iterrows():\n",
    "        try:\n",
    "            imps = load_feature_importances(row['feature_importances_path'])\n",
    "            processed_imps = process_importances(imps, response_df)\n",
    "            ranks = processed_imps.abs().rank(ascending=False)\n",
    "\n",
    "            key = row['datasets']\n",
    "            imps_by_key.setdefault(key, []).append(processed_imps)\n",
    "            ranks_by_key.setdefault(key, []).append(ranks)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    df_imps_by_key = {key: pd.DataFrame(imps_list) for key, imps_list in imps_by_key.items()}\n",
    "    df_ranks_by_key = {key: pd.DataFrame(ranks_list) for key, ranks_list in ranks_by_key.items()}\n",
    "\n",
    "    return df_imps_by_key, df_ranks_by_key\n",
    "\n",
    "\n",
    "def load_response_variable(response_path='../../pnet_germline/data/pnet_database/prostate/processed/response_paper.csv'):\n",
    "    # Load the response variable DataFrame\n",
    "    response_df = pd.read_csv(response_path)\n",
    "    response_df.rename(columns={'id': \"Tumor_Sample_Barcode\"}, inplace=True)\n",
    "    response_df.set_index('Tumor_Sample_Barcode', inplace=True)\n",
    "    return response_df\n",
    "\n",
    "\n",
    "def extract_top_features_from_df(df_per_dataset, top_n=10, keep_smallest_n=True, index_label=None):\n",
    "    \"\"\"\n",
    "    Extract the top N features by rank for each dataset.\n",
    "\n",
    "    Args:\n",
    "        df_per_dataset (dict): Dictionary containing feature-level pd DataFrames for each dataset.\n",
    "        top_n (int): Number of top features to extract.\n",
    "        keep_smallest_n (bool): Whether to sort in ascending order (lower rank is better).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the top N features for each dataset.\n",
    "    \"\"\"\n",
    "    top_features_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the dictionary to calculate top features\n",
    "    for dataset, df in df_per_dataset.items():\n",
    "        # Calculate the mean rank for each feature and select the top N\n",
    "        top_features = df.mean(axis=0).sort_values(ascending=keep_smallest_n)[:top_n]\n",
    "        # Add the top features as a column to the DataFrame\n",
    "        top_features_df[dataset] = top_features.index\n",
    "\n",
    "    # Set the index of the DataFrame to be 1 through top_n\n",
    "    top_features_df.index = range(1, top_n + 1)\n",
    "\n",
    "    if index_label is not None:\n",
    "        top_features_df.index.name = index_label\n",
    "\n",
    "    return top_features_df\n",
    "\n",
    "def extract_top_features_from_series(series_per_dataset, top_n=10, keep_smallest_n=True, index_label=None):\n",
    "    \"\"\"\n",
    "    Extract the top N features by rank for each dataset.\n",
    "\n",
    "    Args:\n",
    "        series_per_dataset (dict): Dictionary containing feature-level pd Series for each dataset.\n",
    "        top_n (int): Number of top features to extract.\n",
    "        keep_smallest_n (bool): Whether to sort in ascending order (lower rank is better).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the top N features for each dataset.\n",
    "    \"\"\"\n",
    "    top_features_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the dictionary to calculate top features\n",
    "    for dataset, values in series_per_dataset.items():\n",
    "        # Calculate the mean rank for each feature and select the top N\n",
    "        top_features = values.sort_values(ascending=keep_smallest_n)[:top_n]\n",
    "        # Add the top features as a column to the DataFrame\n",
    "        top_features_df[dataset] = top_features.index\n",
    "\n",
    "    # Set the index of the DataFrame to be 1 through top_n\n",
    "    top_features_df.index = range(1, top_n + 1)\n",
    "\n",
    "    if index_label is not None:\n",
    "        top_features_df.index.name = index_label\n",
    "\n",
    "    return top_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Get information on the sweep runs\")\n",
    "MODEL_TYPE = \"pnet\"\n",
    "runs = fetch_wandb_runs(PROJECT_NAME, SWEEP_ID)\n",
    "df_feature_importance_paths = fetch_feature_importance_paths(runs, GROUP_NAME)\n",
    "logger.info(f\"Filter to runs where model_type is {MODEL_TYPE}\")\n",
    "df_feature_importance_paths = df_feature_importance_paths[df_feature_importance_paths['model_type'] == MODEL_TYPE]\n",
    "df_feature_importance_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_importance_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Get information on the sweep runs\")\n",
    "MODEL_TYPE = \"pnet\"\n",
    "runs = fetch_wandb_runs(PROJECT_NAME, SWEEP_ID)\n",
    "df_feature_importance_paths = fetch_feature_importance_paths(runs, GROUP_NAME)\n",
    "logger.info(f\"Filter to runs where model_type is {MODEL_TYPE}\")\n",
    "df_feature_importance_paths = df_feature_importance_paths[df_feature_importance_paths['model_type'] == MODEL_TYPE]\n",
    "logger.info(\"Filter to just somatic datasets (somatic_amp, somatic_del, and somatic_mut)\")\n",
    "df_feature_importance_paths = df_feature_importance_paths[df_feature_importance_paths['datasets'] == \"somatic_amp somatic_del somatic_mut\"]\n",
    "display(df_feature_importance_paths.head())\n",
    "\n",
    "logger.info(\"Get the feature importances and ranks for each dataset combination\")\n",
    "response_df = load_response_variable()\n",
    "df_imps_by_key, df_ranks_by_key = process_feature_importances(df_feature_importance_paths, response_df)\n",
    "\n",
    "logger.info(\"Example df_imps_by_key DF:\")\n",
    "display(df_imps_by_key['somatic_amp somatic_del somatic_mut'].head())\n",
    "\n",
    "N = 40  # Number of top features to select\n",
    "top_N_features_by_rank = extract_top_features_from_df(df_ranks_by_key, top_n=N, keep_smallest_n=True, index_label=\"rank\")\n",
    "\n",
    "top_genes = top_N_features_by_rank['somatic_amp somatic_del somatic_mut'][:N].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract just the gene names from the top_genes list\n",
    "# keep the same order but remove duplicates\n",
    "top_genes = list(dict.fromkeys([gene.split(\"_\")[0] for gene in top_genes]))\n",
    "print(len(top_genes), \"unique genes in top_genes\")\n",
    "top_genes = [i for i in top_genes if i in common_genes]\n",
    "print(len(top_genes), \"unique genes in top_genes when filtered to common genes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(corr_matrix, title='Correlation Matrix', figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot a correlation matrix using seaborn heatmap.\n",
    "    \n",
    "    Args:\n",
    "        corr_matrix (pd.DataFrame): Correlation matrix to plot.\n",
    "        title (str): Title of the plot.\n",
    "        figsize (tuple): Size of the figure.\n",
    "    \"\"\"\n",
    "    g = sns.clustermap(\n",
    "        corr_matrix,\n",
    "        method='average',\n",
    "        metric='euclidean',\n",
    "        cmap='vlag',\n",
    "        center=0,\n",
    "        figsize=figsize,  # adjust height based on number of top_genes\n",
    "    )\n",
    "\n",
    "    g.ax_row_dendrogram.set_visible(False)\n",
    "    g.ax_col_dendrogram.set_visible(False)\n",
    "    g.ax_heatmap.set_xticks([])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# Plot the correlation matrix for the difference\n",
    "# Subset: rows = top_genes, columns = all genes\n",
    "subset_corr_diff = corr_diff.loc[top_genes, :]\n",
    "plot_corr_matrix(subset_corr_diff, title='Top Genes vs All Genes (Class 1 - Class 0)', figsize=(12, len(top_genes) * 0.5))\n",
    "\n",
    "# subset_corr_class1 = corr_class1.loc[top_genes, :]\n",
    "# plot_corr_matrix(subset_corr_class1, title='Top Genes vs All Genes (Class 1)', figsize=(12, len(top_genes) * 0.5))\n",
    "\n",
    "# subset_corr_class0 = corr_class0.loc[top_genes, :]\n",
    "# plot_corr_matrix(subset_corr_class0, title='Top Genes vs All Genes (Class 0)', figsize=(12, len(top_genes) * 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get deciles of subset_corr_diff\n",
    "deciles = np.percentile(subset_corr_diff.values.flatten(), np.arange(0, 101, 10))\n",
    "# display the deciles as a table\n",
    "deciles_df = pd.DataFrame(deciles, columns=[\"Decile\"])\n",
    "deciles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference in \"AF\" between classes for genes assigned high importance by P-NET?\n",
    "For same `top_genes` that we just examined the gene-gene correlation matrices, how do the rates of events differ between classes (met vs primary)?\n",
    "- Are there any genes that don't have a difference in rate between classes? If so, do these show different gene-gene correlation between classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact, false_discovery_control\n",
    "\n",
    "# Step 1: Compute frequencies (proportion of 1s) for each gene\n",
    "freq_class0 = X_class0.mean(axis=0)\n",
    "freq_class1 = X_class1.mean(axis=0)\n",
    "\n",
    "# Step 2: Compute frequency difference\n",
    "freq_diff = freq_class1 - freq_class0\n",
    "\n",
    "# Step 3: Perform statistical test (Fisher's exact test) for each gene\n",
    "p_values = []\n",
    "o_ratios = []\n",
    "r_risks = []\n",
    "for gene in common_genes:\n",
    "    # Build contingency table:\n",
    "    # [[#1s in class1, #0s in class1],\n",
    "    #  [#1s in class0, #0s in class0]]\n",
    "    a = X_class1[gene].sum()\n",
    "    b = len(X_class1) - a\n",
    "    c = X_class0[gene].sum()\n",
    "    d = len(X_class0) - c\n",
    "    contingency = [[a, b], [c, d]]\n",
    "    \n",
    "    oratio, p = fisher_exact(contingency, alternative='two-sided')  # or 'greater', 'less'\n",
    "    o_ratios.append(oratio)\n",
    "    p_values.append(p)\n",
    "\n",
    "    # Relative Risk calculation\n",
    "    risk_class1 = a / (a + b) if (a + b) > 0 else float('nan')\n",
    "    risk_class0 = c / (c + d) if (c + d) > 0 else float('nan')\n",
    "    rr = (risk_class1 / risk_class0) if risk_class0 > 0 else float('inf')\n",
    "    r_risks.append(rr)\n",
    "\n",
    "# Step 4: Assemble into DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    \"gene\": common_genes,\n",
    "    \"freq_class0\": freq_class0.values,\n",
    "    \"freq_class1\": freq_class1.values,\n",
    "    \"freq_diff\": freq_diff.values,\n",
    "    \"odds_ratio\": o_ratios,\n",
    "    \"relative_risk\": r_risks,\n",
    "    \"p_value\": p_values\n",
    "})\n",
    "\n",
    "# Optional: adjust p-values (e.g., Benjamini-Hochberg FDR)\n",
    "adjusted_pvals = false_discovery_control(p_values, method='bh')\n",
    "result_df[\"adj_p_value\"] = adjusted_pvals\n",
    "\n",
    "# Sort by largest absolute difference or significance\n",
    "result_df = result_df.sort_values(by=\"freq_diff\", key=abs, ascending=False).round(3)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter result_df to only include genes in top_genes\n",
    "result_df_top_genes = result_df[result_df['gene'].isin(top_genes)].sort_values(by=\"odds_ratio\", key=abs, ascending=True).round(3)\n",
    "print(\"Top genes with frequency differences and p-values:\")\n",
    "display(result_df_top_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corr matrix plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume corr_class0 is a cleaned DataFrame (no NaNs)\n",
    "g = sns.clustermap(\n",
    "    corr_class0,\n",
    "    method='average',     # linkage method: 'average', 'complete', etc.\n",
    "    metric='euclidean',   # or 'correlation', 'cityblock', etc.\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    figsize=(10, 10),\n",
    "    \n",
    ")\n",
    "g.ax_heatmap.set_xticks([])\n",
    "g.ax_heatmap.set_yticks([])\n",
    "plt.title(\"Clustered Gene-Gene Correlation (Class 0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume corr_class0 is a cleaned DataFrame (no NaNs)\n",
    "g = sns.clustermap(\n",
    "    corr_class1,\n",
    "    method='average',     # linkage method: 'average', 'complete', etc.\n",
    "    metric='euclidean',   # or 'correlation', 'cityblock', etc.\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    figsize=(10, 10),\n",
    "    \n",
    ")\n",
    "g.ax_heatmap.set_xticks([])\n",
    "g.ax_heatmap.set_yticks([])\n",
    "plt.title(\"Clustered Gene-Gene Correlation (Class 1)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume corr_diff is a cleaned DataFrame (no NaNs)\n",
    "g = sns.clustermap(\n",
    "    corr_diff,\n",
    "    method='average',     # linkage method: 'average', 'complete', etc.\n",
    "    metric='euclidean',   # or 'correlation', 'cityblock', etc.\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    figsize=(10, 10),\n",
    "    \n",
    ")\n",
    "g.ax_row_dendrogram.set_visible(False) #suppress row dendrogram\n",
    "g.ax_col_dendrogram.set_visible(False) #suppress column dendrogram\n",
    "g.ax_heatmap.set_xticks([])\n",
    "g.ax_heatmap.set_yticks([])\n",
    "plt.title(\"Clustered Gene-Gene Correlation (Class 1 - Class 0)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use distance threshold instead of fixed number of clusters\n",
    "# You can tune this (e.g., 0.5 → looser, 1.5 → tighter clusters)\n",
    "distance_threshold = 3\n",
    "\n",
    "cluster_labels = fcluster(g.dendrogram_row.linkage, t=distance_threshold, criterion='maxclust')\n",
    "\n",
    "# Get reordered gene list\n",
    "reordered_genes = corr_diff.index[g.dendrogram_row.reordered_ind]\n",
    "\n",
    "# Create cluster color labels\n",
    "cluster_df = pd.DataFrame({\n",
    "    'Gene': corr_diff.index,\n",
    "    'Cluster': cluster_labels\n",
    "})\n",
    "cluster_df.set_index('Gene', inplace=True)\n",
    "\n",
    "# Assign a color to each cluster\n",
    "unique_clusters = cluster_df['Cluster'].unique()\n",
    "palette = sns.color_palette(\"hsv\", len(unique_clusters))\n",
    "lut = dict(zip(unique_clusters, palette))\n",
    "cluster_colors = cluster_df['Cluster'].map(lut)\n",
    "\n",
    "# Assume corr_diff is a cleaned DataFrame (no NaNs)\n",
    "g = sns.clustermap(\n",
    "    corr_diff,\n",
    "    row_colors=cluster_colors,\n",
    "    col_colors=cluster_colors,\n",
    "    method='average',     # linkage method: 'average', 'complete', etc.\n",
    "    metric='euclidean',   # or 'correlation', 'cityblock', etc.\n",
    "    cmap='vlag',\n",
    "    center=0,\n",
    "    figsize=(10, 10),\n",
    "    \n",
    ")\n",
    "g.ax_row_dendrogram.set_visible(False) #suppress row dendrogram\n",
    "g.ax_col_dendrogram.set_visible(False) #suppress column dendrogram\n",
    "g.ax_heatmap.set_xticks([])\n",
    "g.ax_heatmap.set_yticks([])\n",
    "plt.title(\"Clustered Differential Gene-Gene Correlation with Cluster Annotations\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of gene-gene correlations are not different between classes. Only the top 20% or so seem at all relevant. Currently, around 285 genes per decile (because of how I filtered the data). And if I did some dimenionality/noise reduction on each of my class-specific correlation matrices first I might get even cleaner results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these genes in cluster 2 are very different between class 1 and class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df[cluster_df.Cluster == 2].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_diff.shape[0]*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.triu_indices to extract the upper triangle without the diagonal\n",
    "def get_upper_triangle_values(corr_matrix):\n",
    "    upper_tri_ix = np.triu_indices_from(corr_matrix, k=1)\n",
    "    return corr_matrix.values[upper_tri_ix]\n",
    "\n",
    "correlations = get_upper_triangle_values(corr_diff)\n",
    "\n",
    "deciles = np.percentile(correlations, np.arange(0, 101, 10))\n",
    "\n",
    "# Wrap in a readable format\n",
    "gene_corr_decile_df = pd.DataFrame({\n",
    "    'Decile': [f'{i}th' for i in range(0, 101, 10)],\n",
    "    'Correlation': deciles\n",
    "}).round(3)\n",
    "gene_corr_decile_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
