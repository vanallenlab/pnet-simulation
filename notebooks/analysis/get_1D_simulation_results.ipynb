{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generate Performance Boxplots and Feature Importance Calculations from WandB Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import necessary libraries such as `wandb`, `pandas`, `numpy`, `matplotlib`, and `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, \"../..\")  # add project_config to path\n",
    "import project_config\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Setup Logging and Configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setup Logging and Configuration\n",
    "Set up logging and define configurations such as WandB project name, sweep ID, and directories for saving results.\n",
    "\n",
    "Here, looking at single-gene perturbation on P1000 somatic mutation dataset backbone. Essentially, we spiked in signal on a single gene (AR) and then assigned these samples to class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file with performance metrics and feature importance paths for each run\n",
    "METRIC_CSV_PATH = project_config.RESULT_DIR / \"single_gene_spike_in_simulation_prediction_metrics_per_run_full.csv\"\n",
    "\n",
    "# Define directories for saving results\n",
    "FIGDIR = project_config.FIGURE_2_DIR\n",
    "RESULTS_DIR = project_config.RESULT_DIR_FIGURE_2\n",
    "\n",
    "# make directory if it doesn't exist\n",
    "os.makedirs(os.path.join(FIGDIR), exist_ok=True)\n",
    "\n",
    "# Figure parameters\n",
    "figsize_tuple = (2.1, 2)\n",
    "\n",
    "aspect_ratio_value = 1.7\n",
    "cmap_style = \"viridis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Performance Metrics (grouped by model_type, OR, and control_frequency)\n",
    "Use the WandB API to fetch runs and extract performance metrics grouped by `model_type`, `odds_ratio`, `control_frequency`, and `n_features` for purposes of assessing the 'power curve' of the model. Store the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METRIC_CSV_PATH)\n",
    "\n",
    "# Average over seeds\n",
    "group_by_cols = [\"model_type\", \"n_features\", \"odds_ratio\", \"control_frequency\"]\n",
    "\n",
    "# Add a counts column for each group\n",
    "df_counts = df.groupby(group_by_cols).size().reset_index(name=\"count\")\n",
    "\n",
    "# Average over seeds\n",
    "df_avg = df.groupby(group_by_cols, as_index=False).mean()\n",
    "\n",
    "# Merge counts into df_avg\n",
    "df_avg = df_avg.merge(df_counts, on=group_by_cols)\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_ticklabels(labels):\n",
    "    return [f\"{int(l)}\" if float(l).is_integer() else f\"{l:.1f}\" for l in labels]\n",
    "\n",
    "def get_smart_odds_ratio_labels(axessubplot_object):\n",
    "    \"\"\"Expects type <class 'matplotlib.axes._subplots.AxesSubplot'> as input\"\"\"\n",
    "    smart_yticks = smart_ticklabels([float(l.get_text()) for l in axessubplot_object.get_yticklabels()])\n",
    "    return smart_yticks\n",
    "\n",
    "def heatmap_no_counts(df, title, ax, eval_set, **kwargs):\n",
    "    precision = df.pivot(index=\"odds_ratio\", columns=\"control_frequency\", values=f\"{eval_set}_avg_precision\")\n",
    "    sns.heatmap(\n",
    "        precision, cmap=cmap_style, annot=True, fmt=\".2f\",\n",
    "        cbar=False, ax=ax, **kwargs\n",
    "    )\n",
    "    ax.set_aspect(aspect_ratio_value, adjustable='box')\n",
    "    ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"control_frequency\")\n",
    "    ax.set_ylabel(\"odds ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches feature importance plots in shape and style\n",
    "# adding x-axis labels to all\n",
    "eval_set = \"train\"\n",
    "# filter to n_features \n",
    "df_avg_to_plot = df_avg # df_avg[df_avg[\"n_features\"] == 10]\n",
    "\n",
    "# rename values in the model_type column: pnet --> P-NET, rf --> Random Forest\n",
    "df_avg_to_plot[\"model_type\"] = df_avg_to_plot[\"model_type\"].replace({\n",
    "    \"pnet\": \"P-NET\",\n",
    "    \"rf\": \"Random Forest\"\n",
    "})\n",
    "\n",
    "metric = f\"{eval_set}_avg_precision\" \n",
    "\n",
    "# Compute global color scale limits\n",
    "vmin = df_avg_to_plot[metric].min()\n",
    "vmax = df_avg_to_plot[metric].max()\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_avg_to_plot,\n",
    "    row=\"n_features\",\n",
    "    col=\"model_type\",\n",
    "    margin_titles=True,\n",
    "    height=figsize_tuple[0],\n",
    "    aspect=aspect_ratio_value\n",
    ")\n",
    "\n",
    "def draw_heatmap(data, **kwargs):\n",
    "    pivoted = data.pivot(index=\"odds_ratio\", columns=\"control_frequency\", values=metric)\n",
    "    sns.heatmap(pivoted, cmap=\"viridis\", annot=True, fmt=\".2f\", cbar=False, vmax=vmax, vmin=vmin,\n",
    "                **kwargs)\n",
    "\n",
    "g.map_dataframe(draw_heatmap)\n",
    "g.set_titles(row_template=\"# features = {row_name}\", col_template=\"model = {col_name}\")\n",
    "g.set_axis_labels(\"control frequency\", \"odds ratio\")\n",
    "# Adjust spacing\n",
    "g.figure.subplots_adjust(top=0.85, hspace=0.1)\n",
    "\n",
    "g.figure.suptitle(\n",
    "    f\"AUPRC on single-gene perturbation datasets ({eval_set} set)\",\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "# Ensure all x-axis tick labels are shown\n",
    "for col_idx, ax in enumerate(g.axes.flat):\n",
    "    ax.set_xticklabels(ax.get_xticklabels())\n",
    "    ax.tick_params(axis='x', labelbottom=True)\n",
    "    if col_idx == 0:\n",
    "        ax.tick_params(axis='y', labelleft=True)\n",
    "        ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_auprc_single_gene_perturbation_{eval_set}.svg\"), format='svg', dpi=1200)\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_auprc_single_gene_perturbation_{eval_set}.png\"), format='png', dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# How do P-NET feature rankings compare with ground truth?\n",
    "In this 1D (single gene) perturbation setting, we spiked in a signal to a single gene. We chose \"AR\". Now, we will investigate the P-NET assigned rank of this gene\n",
    "\n",
    "Plot: OR x control_frequency grid where the value is the rank (importance) assigned by the model to gene \"AR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want df with 1) y path 2) feature importances path 3) gene importances path 4) unique identifiers (odds ratio, control frequency, n_features, model_type)\n",
    "df_feature_importance_paths = df[[\"save_dir\", \"target_f\", \"model_type\", \"n_features\", \"odds_ratio\", \"control_frequency\"]].copy()\n",
    "\n",
    "# filter to only model_type == \"pnet\"\n",
    "df_feature_importance_paths = df_feature_importance_paths[df_feature_importance_paths[\"model_type\"] == \"pnet\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# in \"save_dir\" column, replace \"../../results/\" with \"/mnt/disks/gmiller_data1/pnet/results\"\n",
    "df_feature_importance_paths[\"save_dir\"] = df_feature_importance_paths[\"save_dir\"].str.replace(\"../../results/\", \"/mnt/disks/gmiller_data1/pnet/results/\", regex=False)\n",
    "\n",
    "eval_set = \"test\"\n",
    "df_feature_importance_paths[\"feature_importances_path\"] = df_feature_importance_paths.apply(lambda row: os.path.join(row[\"save_dir\"], f\"{eval_set}_gene_feature_importances.csv\"), axis=1)\n",
    "df_feature_importance_paths[\"gene_importances_path\"] = df_feature_importance_paths.apply(lambda row: os.path.join(row[\"save_dir\"], f\"{eval_set}_gene_importances.csv\"), axis=1)\n",
    "\n",
    "# Add a unique group identifer column by joining together the unique identifiers: [\"model_type\", \"n_features\", \"odds_ratio\", \"control_frequency\"]\n",
    "df_feature_importance_paths[\"group_identifier\"] = df_feature_importance_paths.apply(\n",
    "    lambda row: f\"OR-{row['odds_ratio']}_ctrlFreq-{row['control_frequency']}_nFeatures-{row['n_features']}\",\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_importances(importances_path):\n",
    "    \"\"\"\n",
    "    Load feature importance data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        importances_path (str): Path to the feature importance file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of feature importances.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(importances_path):\n",
    "        raise FileNotFoundError(f\"File not found: {importances_path}\")\n",
    "    logger.debug(f\"Loading feature importances from {importances_path}\")\n",
    "    return pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "\n",
    "\n",
    "def process_importances(imps, response_df):\n",
    "    \"\"\"\n",
    "    Process feature importances by joining with response data and calculating differences between sample classes.\n",
    "    This function computes the mean feature importances for each response class and then calculates the difference between them.\n",
    "    The result is a Series of feature importance differences.\n",
    "\n",
    "    Args:\n",
    "        imps (pd.DataFrame): Feature importance DataFrame.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Processed feature importance differences.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"head of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().head()}\")\n",
    "    logger.debug(f\"shape of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().shape}\")\n",
    "    return imps.join(response_df).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "\n",
    "def load_response_variable(response_path):\n",
    "    response_df = pd.read_csv(response_path).set_index('Tumor_Sample_Barcode')\n",
    "    # rename the column to 'response' for consistency\n",
    "    response_df.rename(columns={\"is_met\": \"response\"}, inplace=True)\n",
    "    return response_df\n",
    "\n",
    "def process_feature_importances(df_feature_importance_paths, response_df_path_column='target_f', importance_path_column='feature_importances_path', group_identifier_column='group_identifier'):\n",
    "    \"\"\"\n",
    "    Process feature importance data for multiple runs and group by dataset.\n",
    "\n",
    "    Args:\n",
    "        df_feature_importance_paths (pd.DataFrame): DataFrame with feature importance paths and dataset info.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionaries of DataFrames for feature importances and ranks grouped by the input datasets used in the model.\n",
    "    \"\"\"\n",
    "    imps_by_key = {}\n",
    "    ranks_by_key = {}\n",
    "\n",
    "    for _, row in df_feature_importance_paths.iterrows():\n",
    "        try:\n",
    "            # Load the response variable DataFrame from the specified path\n",
    "            response_df = load_response_variable(row[response_df_path_column])\n",
    "            # Load feature importances from the specified path\n",
    "            imps = load_feature_importances(row[importance_path_column])\n",
    "            processed_imps = process_importances(imps, response_df)\n",
    "            ranks = processed_imps.abs().rank(ascending=False)\n",
    "\n",
    "            key = row[group_identifier_column]\n",
    "            imps_by_key.setdefault(key, []).append(processed_imps)\n",
    "            ranks_by_key.setdefault(key, []).append(ranks)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    df_imps_by_key = {key: pd.DataFrame(imps_list) for key, imps_list in imps_by_key.items()}\n",
    "    df_ranks_by_key = {key: pd.DataFrame(ranks_list) for key, ranks_list in ranks_by_key.items()}\n",
    "    return df_imps_by_key, df_ranks_by_key\n",
    "\n",
    "\n",
    "\n",
    "def extract_top_features_from_df(df_per_dataset, top_n=10, keep_smallest_n=True, index_label=None):\n",
    "    \"\"\"\n",
    "    Extract the top N features by rank for each dataset.\n",
    "\n",
    "    Args:\n",
    "        df_per_dataset (dict): Dictionary containing feature-level pd DataFrames for each dataset.\n",
    "        top_n (int): Number of top features to extract.\n",
    "        keep_smallest_n (bool): Whether to sort in ascending order (lower rank is better).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the top N features for each dataset.\n",
    "    \"\"\"\n",
    "    top_features_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the dictionary to calculate top features\n",
    "    for dataset, df in df_per_dataset.items():\n",
    "        # Calculate the mean rank for each feature and select the top N\n",
    "        top_features = df.mean(axis=0).sort_values(ascending=keep_smallest_n)[:top_n]\n",
    "        # Add the top features as a column to the DataFrame\n",
    "        top_features_df[dataset] = top_features.index\n",
    "\n",
    "    # Set the index of the DataFrame to be 1 through top_n\n",
    "    top_features_df.index = range(1, top_n + 1)\n",
    "\n",
    "    if index_label is not None:\n",
    "        top_features_df.index.name = index_label\n",
    "\n",
    "    return top_features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Get the feature importances and ranks for each dataset combination\")\n",
    "# response_df = load_response_variable()\n",
    "# df_feature_importance_paths = fetch_feature_importance_paths(runs, GROUP_NAME)\n",
    "\n",
    "# # gene x modality\n",
    "# df_imps_by_key, df_ranks_by_key = process_feature_importances(df_feature_importance_paths)\n",
    "\n",
    "# gene\n",
    "df_imps_by_key, df_ranks_by_key = process_feature_importances(df_feature_importance_paths, importance_path_column='gene_importances_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features_by_rank = extract_top_features_from_df(df_ranks_by_key, top_n=10, keep_smallest_n=True, index_label=\"rank\")\n",
    "top_10_features_by_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract AR importances and ranks from the processed dicts\n",
    "AR_records = []\n",
    "\n",
    "for key, df_imp in df_imps_by_key.items():\n",
    "    mean_imp = df_imp.mean()\n",
    "    mean_rank = df_ranks_by_key[key].mean()\n",
    "    \n",
    "    if \"AR\" in mean_imp:\n",
    "        AR_records.append({\n",
    "            \"group_identifier\": key,\n",
    "            \"AR_importance\": mean_imp[\"AR\"],\n",
    "            \"AR_absolute_importance\": abs(mean_imp[\"AR\"]),\n",
    "            \"AR_rank\": mean_rank[\"AR\"]\n",
    "        })\n",
    "\n",
    "df_AR = pd.DataFrame(AR_records)\n",
    "# Merge AR stats into the plotting dataframe\n",
    "df_plot = df_feature_importance_paths.merge(df_AR, on=\"group_identifier\", how=\"left\")\n",
    "df_plot_avg = df_plot.groupby(group_by_cols, as_index=False).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current: using this for the rank/imps\n",
    "\n",
    "metric = \"AR_rank\"  # or \"AR_rank\", \"AR_absolute_importance\", \"AR_importance\"\n",
    "eval_set = \"test\"\n",
    "# df_plot_avg_tmp = df_plot_avg[df_plot_avg[\"n_features\"] == 100]\n",
    "df_plot_avg_tmp =df_plot_avg\n",
    "\n",
    "# rename values in the model_type column: pnet --> P-NET, rf --> Random Forest\n",
    "df_plot_avg_tmp[\"model_type\"] = df_plot_avg_tmp[\"model_type\"].replace({\n",
    "    \"pnet\": \"P-NET\",\n",
    "    \"rf\": \"Random Forest\"\n",
    "})\n",
    "\n",
    "if metric == \"AR_rank\":\n",
    "    cmap_tmp = \"cividis_r\"\n",
    "elif metric == \"AR_importance\":\n",
    "    cmap_tmp = \"cividis\"\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_plot_avg_tmp,\n",
    "    row=\"n_features\",\n",
    "    col=\"model_type\",\n",
    "    margin_titles=True,\n",
    "    height=figsize_tuple[0],\n",
    "    aspect=aspect_ratio_value\n",
    ")\n",
    "\n",
    "def draw_heatmap(data, **kwargs):\n",
    "    pivoted = data.pivot(index=\"odds_ratio\", columns=\"control_frequency\", values=metric)\n",
    "    sns.heatmap(pivoted, cmap=cmap_tmp, annot=True, fmt=\".1f\", cbar=False, **kwargs)\n",
    "\n",
    "g.map_dataframe(draw_heatmap)\n",
    "g.set_titles(row_template=\"# features = {row_name}\", col_template=\"model = {col_name}\")\n",
    "g.set_axis_labels(\"control frequency\", \"odds ratio\")\n",
    "# Adjust spacing\n",
    "g.figure.subplots_adjust(top=0.85, hspace=0.1)\n",
    "\n",
    "g.figure.suptitle(\n",
    "    f\"Perturbed gene {'importance' if metric == 'AR_importance' else 'rank'} ({eval_set} set)\",\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "# Ensure all x-axis tick labels are shown\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticklabels(ax.get_xticklabels())\n",
    "    ax.tick_params(axis='x', labelbottom=True)\n",
    "    ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_perturbed_gene_{metric}_{eval_set}.svg\"), format='svg', dpi=1200)\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_perturbed_gene_{metric}_{eval_set}.png\"), format='png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current: using this for the rank/imps\n",
    "\n",
    "metric = \"AR_importance\"  # or \"AR_rank\", \"AR_absolute_importance\", \"AR_importance\"\n",
    "eval_set = \"test\"\n",
    "# df_plot_avg_tmp = df_plot_avg[df_plot_avg[\"n_features\"] == 100]\n",
    "df_plot_avg_tmp =df_plot_avg\n",
    "\n",
    "# rename values in the model_type column: pnet --> P-NET, rf --> Random Forest\n",
    "df_plot_avg_tmp[\"model_type\"] = df_plot_avg_tmp[\"model_type\"].replace({\n",
    "    \"pnet\": \"P-NET\",\n",
    "    \"rf\": \"Random Forest\"\n",
    "})\n",
    "\n",
    "if metric == \"AR_rank\":\n",
    "    cmap_tmp = \"cividis_r\"\n",
    "elif metric == \"AR_importance\":\n",
    "    cmap_tmp = \"cividis\"\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    df_plot_avg_tmp,\n",
    "    row=\"n_features\",\n",
    "    col=\"model_type\",\n",
    "    margin_titles=True,\n",
    "    height=figsize_tuple[0],\n",
    "    aspect=aspect_ratio_value\n",
    ")\n",
    "\n",
    "def draw_heatmap(data, **kwargs):\n",
    "    pivoted = data.pivot(index=\"odds_ratio\", columns=\"control_frequency\", values=metric)\n",
    "    sns.heatmap(pivoted, cmap=cmap_tmp, annot=True, fmt=\".1f\", cbar=False, **kwargs)\n",
    "\n",
    "g.map_dataframe(draw_heatmap)\n",
    "g.set_titles(row_template=\"# features = {row_name}\", col_template=\"model = {col_name}\")\n",
    "g.set_axis_labels(\"control frequency\", \"odds ratio\")\n",
    "# Adjust spacing\n",
    "g.figure.subplots_adjust(top=0.85, hspace=0.1)\n",
    "\n",
    "g.figure.suptitle(\n",
    "    f\"Perturbed gene {'importance' if metric == 'AR_importance' else 'rank'} ({eval_set} set)\",\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "# Ensure all x-axis tick labels are shown\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticklabels(ax.get_xticklabels())\n",
    "    ax.tick_params(axis='x', labelbottom=True)\n",
    "    ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_perturbed_gene_{metric}_{eval_set}.svg\"), format='svg', dpi=1200)\n",
    "plt.savefig(os.path.join(FIGDIR, f\"heatmap_perturbed_gene_{metric}_{eval_set}.png\"), format='png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
