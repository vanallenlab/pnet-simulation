{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pnet.performance_and_feature_importance_stability as stability_utils\n",
    "from pnet import report_and_eval\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,  # Only needed if youâ€™re reconfiguring logging in a running session (e.g., Jupyter)\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb setup\n",
    "# import wandb\n",
    "# os.environ['WANDB_NOTEBOOK_NAME'] = \"stability_and_performance_analysis.ipynb\"\n",
    "# wandb.login()\n",
    "# run = wandb.init(\n",
    "#     project=\"prostate_met_status\",\n",
    "#     group=\"analysis_prostate_somatic_and_germline\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- [ ] examine stability of P-NET layers\n",
    "- [ ] try adding germline (and additional data aka confounders) to RF, BDT models. Prioritize the BDT since it is more promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PNET_DIR = '../../pnet/results/gene_rank_stability_v2' # (after improved P-NET stability)\n",
    "BDT_DIR = '../../pnet/results/somatic_bdt_eval_set_test'\n",
    "RF_DIR = '../../pnet/results/somatic_rf_eval_set_test'\n",
    "ARXIV_PNET_DIR = '../../cancer-net/reprod_report' # based on the other group's reproducibility report\n",
    "\n",
    "FIGDIR = '../figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative performance\n",
    "TODO:\n",
    "- load RF, BDT data from W&B.\n",
    "- load P-NET data from pickle files.\n",
    "- make standardized DF for plotting (rows = runs, columns = model type, values = performance metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-NET on different germline:somatic dataset combinations\n",
    "\n",
    "### Result: everything > all rare > all missense > all common > rare LOF > all LOF > just somatic\n",
    "Here we look at box-and-whisker plots of validation AUC (across 5 runs each), examining the ordering from highest to lowest validation performance (sorted by median).\n",
    "\n",
    "For germline:somatic we see\n",
    "everything > all rare > all missense > all common > rare LOF > all LOF > just somatic\n",
    "\n",
    "For germline only we see overall lower performance, and considerably different order:\n",
    "all > all missense > all LOF > all common > rare LOF > all rare\n",
    "Interestingly, we actually get pretty decent validation AUC with only germline data for the top two combinations: all LOF and missense, or just all rare LOF/missense variants. In fact, the mean AUC is around 0.72/0.71 for these groups compared to 0.59/0.56 in the worst two groups.\n",
    "\n",
    "Q: why does the order of \"usefullness\" change so much? Is it because of which signal is redundant with somatic data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define parameters\n",
    "project_name = \"millergw/prostate_met_status\"\n",
    "sweep_id = \"cmlmrw2s\"\n",
    "\n",
    "# Fetch the runs\n",
    "runs = api.runs(project_name, filters={\"sweep\": sweep_id, \"state\": \"finished\"})\n",
    "\n",
    "# Group AUC scores by model_type and datasets\n",
    "auc_scores_by_group = defaultdict(list)\n",
    "\n",
    "# Extract relevant information and group by model_type and datasets\n",
    "for run in runs:\n",
    "    model_type = run.config.get(\"model_type\", \"unknown\")\n",
    "    datasets = run.config.get(\"datasets\", \"unknown\")\n",
    "    auc_score = run.summary.get(\"validation_roc_auc_score\")\n",
    "    if auc_score is not None:\n",
    "        auc_scores_by_group[(model_type, datasets)].append(auc_score)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\"model_type\": model_type, \"datasets\": datasets, \"auc\": auc_score}\n",
    "    for (model_type, datasets), auc_scores in auc_scores_by_group.items()\n",
    "    for auc_score in auc_scores\n",
    "])\n",
    "\n",
    "logging.debug(\"Sorting model order by decreasing mean\")\n",
    "# Calculate the mean of each group\n",
    "group_means = df.groupby(['model_type', 'datasets'])['auc'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Reorder the DataFrame based on the descending group means\n",
    "df = df.set_index(['model_type', 'datasets']).loc[group_means.index].reset_index()\n",
    "display(df)\n",
    "\n",
    "# Calculate mean, standard deviation, and sample size of the AUC scores for each group\n",
    "grouped_stats = df.groupby(['model_type', 'datasets'])['auc'].agg(\n",
    "    mean='mean',\n",
    "    std='std',\n",
    "    num_samples='count'\n",
    ").loc[group_means.index].reset_index()\n",
    "\n",
    "display(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_ordered_by_statistic(df, group_col_name, order_col_name='auc', stat=\"median\", ascending=False):\n",
    "    if stat==\"mean\":\n",
    "        # Calculate mean for each dataset\n",
    "        sorted_df = df.groupby(group_col_name)[order_col_name].mean().sort_values(ascending=ascending)\n",
    "    elif stat==\"median\":\n",
    "        # Calculate median for each dataset\n",
    "        sorted_df = df.groupby(group_col_name)[order_col_name].median().sort_values(ascending=ascending)\n",
    "    else:\n",
    "        print(\"TODO: not implemented\")\n",
    "        return\n",
    "    # Order the datasets by descending mean AUC\n",
    "    group_order = sorted_df.index.tolist()\n",
    "    return group_order\n",
    "\n",
    "\n",
    "def plot_auc_boxplot_for_grouped_df(df, group_order=None):\n",
    "    # TODO: wip. Start here.\n",
    "    if group_order is None:\n",
    "        # Extract unique dataset orders\n",
    "        group_order = df['datasets'].unique()\n",
    "\n",
    "    # Set color pallette and plot size\n",
    "    pnet_palette = sns.color_palette('colorblind', n_colors=len(group_order))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Box plot for 'pnet' model type\n",
    "    pnet_df = df[df['model_type'] == 'pnet']\n",
    "    pnet_order = get_group_ordered_by_statistic(pnet_df, group_col_name=\"datasets\")\n",
    "    sns.boxplot(data=pnet_df, x='datasets', y='auc', order=pnet_order, dodge=True, palette=pnet_palette)\n",
    "    sns.stripplot(data=pnet_df, x='datasets', y='auc', order=pnet_order, color='black', jitter=0.2, alpha=0.3)\n",
    "    plt.title('Validation AUC (P-NET)')\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xticks([])  # Hide xticks\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Set y-axis limits manually?\n",
    "    plt.set_ylim(plt.get_ylim()[0], plt.get_ylim()[1])\n",
    "\n",
    "    # Custom legend\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in pnet_palette]\n",
    "    plt.legend(handles, group_order, title='Datasets', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    return plt\n",
    "\n",
    "def plot_boxplots(df, group_order=None):\n",
    "    if group_order is None:\n",
    "        # Extract unique dataset orders\n",
    "        group_order = df['datasets'].unique()\n",
    "\n",
    "    # Create color palette\n",
    "    pnet_palette = sns.color_palette('colorblind', n_colors=len(group_order))\n",
    "\n",
    "    # Create separate box-and-whisker plots for each model type\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Box plot for 'pnet' model type\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    pnet_df = df[df['model_type'] == 'pnet']\n",
    "    pnet_order = get_group_ordered_by_statistic(pnet_df, group_col_name=\"datasets\")\n",
    "    sns.boxplot(data=pnet_df, x='datasets', y='auc', order=pnet_order, dodge=True, palette=pnet_palette)\n",
    "    sns.stripplot(data=pnet_df, x='datasets', y='auc', order=pnet_order, color='black', jitter=0.2, alpha=0.3)\n",
    "    plt.title('Validation AUC (P-NET)')\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xticks([])  # Hide xticks\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Box plot for 'rf' model type\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    rf_df = df[df['model_type'] == 'rf']\n",
    "    sns.boxplot(data=rf_df, x='datasets', y='auc', order=pnet_order, dodge=True, palette=pnet_palette)\n",
    "    sns.stripplot(data=rf_df, x='datasets', y='auc', order=pnet_order, color='black', jitter=0.2, alpha=0.3)\n",
    "    plt.title('Validation AUC (rf)')\n",
    "    plt.xlabel('Datasets')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Set y-axis limits to be the same for both plots\n",
    "    ymin = min(ax1.get_ylim()[0], ax2.get_ylim()[0])\n",
    "    ymax = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",
    "    ax1.set_ylim(ymin, ymax)\n",
    "    ax2.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Custom legend\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=color) for color in pnet_palette]\n",
    "    plt.legend(handles, group_order, title='Datasets', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    return plt\n",
    "\n",
    "logging.info(\"Only germline data\")\n",
    "filtered_data = df[(~df['datasets'].str.contains('somatic'))] # Filtered dataset\n",
    "plt = plot_boxplots(filtered_data)\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"Somatic and germline data, plus somatic only\")\n",
    "filtered_data = df[(df['datasets'].str.contains('somatic') & df['datasets'].str.contains('germline')) | (df['datasets'] == \"somatic_amp somatic_del somatic_mut\")] # Filtered dataset\n",
    "plt = plot_boxplots(filtered_data)\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"All data combos\")\n",
    "filtered_data = df\n",
    "plt = plot_boxplots(filtered_data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box-and-whisker plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='datasets', y='auc', hue='model_type', dodge=True)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend(title='Model Type')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Datasets')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC grouped by datasets, for P-NET and RF')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-NET feature importance (can also examine gene-level importance)\n",
    "1. SNR (signal to noise ratio) as Marc G uses\n",
    "2. Manual look at top-ranked genes (just using scores assigned by P-NET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNR for P-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top N features by SNR for each dataset\n",
    "def get_top_n_features(df, n=10):\n",
    "    top_features = {}\n",
    "    for dataset in df.columns:\n",
    "        top_features[dataset] = df[[dataset]].nlargest(n, dataset)\n",
    "    return top_features\n",
    "\n",
    "\n",
    "# Function to calculate the proportion of total mean importance score contributed by top N features for each dataset\n",
    "def get_top_n_features_proportion(df, n=10):\n",
    "    raise AssertionError(\"the function get_top_n_features_proportion() is not implemented correctly yet.\")\n",
    "    top_n_features_proportion = {}\n",
    "    total_mean_importance = df.mean(axis=0)\n",
    "    print(total_mean_importance)\n",
    "    for dataset in df.columns:\n",
    "        top_n_features_proportion[dataset] = df.nlargest(n, dataset)[dataset].sum() / total_mean_importance[dataset]\n",
    "    return top_n_features_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the impact of different input data\n",
    "prostate_response = pd.read_csv('../../pnet_germline/data/pnet_database/prostate/processed/response_paper.csv')\n",
    "prostate_response.rename(columns={'id': \"Tumor_Sample_Barcode\"}, inplace=True)\n",
    "prostate_response.set_index('Tumor_Sample_Barcode', inplace=True)\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define parameters\n",
    "who = \"validation\"\n",
    "project_name = \"prostate_met_status\"\n",
    "group_name = \"pnet_somatic_and_germline_exp_003\"\n",
    "sweep_id = \"cmlmrw2s\" \n",
    "\n",
    "# Fetch the runs\n",
    "runs = api.runs(project_name, filters={\"sweep\": sweep_id, \"state\": \"finished\"})\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "for run in runs:\n",
    "    model_type = run.config.get(\"model_type\", \"unknown\")\n",
    "    datasets = run.config.get(\"datasets\", \"unknown\")\n",
    "    subdir_name = f\"{model_type}_eval_set_{who}\"\n",
    "\n",
    "    # Paths to feature and gene importances files\n",
    "    feature_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_feature_importances.csv'\n",
    "    gene_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_importances.csv'\n",
    "\n",
    "    # Create a DataFrame for the current run\n",
    "    df = pd.DataFrame({\n",
    "        \"run_id\": [run.id],\n",
    "        \"model_type\": [model_type],\n",
    "        \"datasets\": [datasets],\n",
    "        \"feature_importances_path\": [feature_importances_path],\n",
    "        # \"gene_importances_path\": [gene_importances_path]\n",
    "    })\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "pnet_df = pd.concat(dfs, ignore_index=True)\n",
    "pnet_df.set_index(\"run_id\", inplace=True)\n",
    "\n",
    "# Now you have a DataFrame containing the run information\n",
    "# You can use this DataFrame to access the paths to feature and gene importances files\n",
    "display(pnet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the h1 regularization experiment results\n",
    "prostate_response = pd.read_csv('../../pnet_germline/data/pnet_database/prostate/processed/response_paper.csv')\n",
    "prostate_response.rename(columns={'id': \"Tumor_Sample_Barcode\"}, inplace=True)\n",
    "prostate_response.set_index('Tumor_Sample_Barcode', inplace=True)\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define parameters\n",
    "who = \"validation\"\n",
    "project_name = \"prostate_met_status\"\n",
    "group_name = \"pnet_h1_regularization_001\"\n",
    "sweep_id = \"g8cl6qur\" \n",
    "\n",
    "# Fetch the runs\n",
    "runs = api.runs(project_name, filters={\"sweep\": sweep_id, \"state\": \"finished\"})\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "for run in runs:\n",
    "    model_type = run.config.get(\"model_type\", \"unknown\")\n",
    "    h1_alpha = run.config.get(\"h1_alpha\", \"unknown\")\n",
    "    datasets = run.config.get(\"datasets\", \"unknown\")\n",
    "    h1_regularization_method = run.config.get(\"h1_regularization_method\", \"unknown\")\n",
    "    subdir_name = f\"{model_type}_eval_set_{who}\"\n",
    "\n",
    "    # Paths to feature and gene importances files\n",
    "    feature_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_feature_importances.csv'\n",
    "    gene_importances_path = f'../results/{group_name}/{subdir_name}/wandbID_{run.id}/{who}_gene_importances.csv'\n",
    "\n",
    "    # Create a DataFrame for the current run\n",
    "    df = pd.DataFrame({\n",
    "        \"run_id\": [run.id],\n",
    "        \"model_type\": [model_type],\n",
    "        \"datasets\": [datasets],\n",
    "        \"h1_alpha\": [h1_alpha],\n",
    "        \"h1_regularization_method\": [h1_regularization_method],\n",
    "        \"feature_importances_path\": [feature_importances_path],\n",
    "        # \"gene_importances_path\": [gene_importances_path]\n",
    "    })\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "h1_df = pd.concat(dfs, ignore_index=True)\n",
    "h1_df.set_index(\"run_id\", inplace=True)\n",
    "\n",
    "# Now you have a DataFrame containing the run information\n",
    "# You can use this DataFrame to access the paths to feature and gene importances files\n",
    "display(h1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code:\n",
    "1. We use dictionaries imps_by_dataset and ranks_by_dataset to store feature importances and ranks grouped by dataset.\n",
    "1. For each row in pnet_df, we append the importances and ranks to the appropriate dataset list.\n",
    "1. After collecting the data, we convert each dataset's list into a DataFrame.\n",
    "1. We compute the SNR for each dataset and store the results in a dictionary.\n",
    "1. Finally, we convert the SNR dictionary to a DataFrame for easier handling and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_sweep_df = h1_df #pnet_df\n",
    "wandb_sweep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize dictionaries to store data by key\n",
    "imps_by_key = {}\n",
    "ranks_by_key = {}\n",
    "\n",
    "\n",
    "# Ensure the DataFrame has a 'dataset' column or a column to group by\n",
    "if 'h1_alpha' not in wandb_sweep_df.columns:\n",
    "    raise ValueError(\"The DataFrame must contain a 'h1_alpha' column to group by\")\n",
    "\n",
    "# Loop through the rows of the DataFrame containing run information\n",
    "for index, row in wandb_sweep_df.iterrows():\n",
    "    importances_path = row['feature_importances_path']\n",
    "    imps = pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "    \n",
    "    # Read feature importances from the specified file path\n",
    "    imps = imps.join(prostate_response).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "    \n",
    "    ranks = imps.rank(ascending=False)\n",
    "    \n",
    "    key = row['h1_regularization_method']+'_'+row['h1_alpha']\n",
    "    \n",
    "    # Initialize lists for the key if not already present\n",
    "    if key not in imps_by_key:\n",
    "        imps_by_key[key] = []\n",
    "        ranks_by_key[key] = []\n",
    "    \n",
    "    # Store the results in the corresponding key lists\n",
    "    imps_by_key[key].append(imps)\n",
    "    ranks_by_key[key].append(ranks)\n",
    "\n",
    "# Create DataFrames for feature importances and ranks by key\n",
    "df_imps_by_key = {key: pd.DataFrame(imps_list) for key, imps_list in imps_by_key.items()}\n",
    "df_ranks_by_key = {key: pd.DataFrame(ranks_list) for key, ranks_list in ranks_by_key.items()}\n",
    "\n",
    "# Calculate signal-to-noise ratio (SNR) for each key\n",
    "snr_per_key = {}\n",
    "for key, df_imps in df_imps_by_key.items():\n",
    "    snr = df_imps.mean(axis=0) / (df_imps.std(axis=0) + 1e-9)\n",
    "    snr_per_key[key] = snr\n",
    "\n",
    "# Convert the SNR dictionary to a DataFrame for easier handling\n",
    "snr_df = pd.DataFrame(snr_per_key).transpose().reset_index().rename(columns={'index': 'key'})\n",
    "\n",
    "display(snr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the top 10 values by P-NET rank for each dataset\n",
    "top_10_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over df_ranks_by_dataset dictionary\n",
    "for dataset, df in df_imps_by_dataset.items():\n",
    "    # Calculate the mean ranking for each run and select the top 10\n",
    "    top_10 = df.mean(axis=0).sort_values(ascending=False)[:10]\n",
    "    \n",
    "    # Add the top 10 values as a column to the DataFrame\n",
    "    top_10_df[dataset] = top_10.index\n",
    "\n",
    "# Set the index of the DataFrame to be 1 through 10\n",
    "top_10_df.index = range(1, 11)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Top features by P-NET importance score:\")\n",
    "display(top_10_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INTS3_somatic_del: High SNR score even though not important due to low variability run-to-run\")\n",
    "print(df_imps_by_dataset['somatic_amp somatic_del somatic_mut']['INTS3_somatic_del'])\n",
    "print('')\n",
    "print(\"AR_somatic_amp: Less high SNR score even though very important due to higher variability\")\n",
    "print(df_imps_by_dataset['somatic_amp somatic_del somatic_mut']['AR_somatic_amp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the top 10 values by P-NET rank for each dataset\n",
    "top_10_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over df_ranks_by_dataset dictionary\n",
    "for dataset, rankdf in df_ranks_by_dataset.items():\n",
    "    # Calculate the mean ranking for each run and select the top 10\n",
    "    top_10 = rankdf.mean(axis=0).sort_values(ascending=True)[:10]\n",
    "    \n",
    "    # Add the top 10 values as a column to the DataFrame\n",
    "    top_10_df[dataset] = top_10.index\n",
    "\n",
    "# Set the index of the DataFrame to be 1 through 10\n",
    "top_10_df.index = range(1, 11)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Top features by P-NET ranking:\")\n",
    "display(top_10_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get top 10 features by P-NET ranking for each dataset\n",
    "# top_n_features_pnet_rank = get_top_n_features(df_ranks_by_dataset, n=10)\n",
    "\n",
    "# # Display the results\n",
    "# for dataset, top_features in top_n_features_pnet_rank.items():\n",
    "#     print(f\"Top {len(top_features)} P-NET ranked features for {dataset}:\")\n",
    "#     display(top_features)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_features['somatic_amp somatic_del somatic_mut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 features by SNR for each dataset\n",
    "top_n_features = get_top_n_features(pd.DataFrame(snr_per_dataset), n=10)\n",
    "\n",
    "# Display the results\n",
    "for dataset, top_features in top_n_features.items():\n",
    "    print(f\"Top {len(top_features)} features for {dataset}:\")\n",
    "    display(top_features)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we calculate SNR for each group (\"datasets\") separately to see how the top-ranked genes change with different inputs\n",
    "\n",
    "# Initialize empty dictionaries to store results for each dataset group\n",
    "snr_dict = {}\n",
    "\n",
    "# Restrict to just P-NET models for now\n",
    "pnet_df = pnet_df[pnet_df['model_type'] == \"pnet\"]\n",
    "\n",
    "# Loop through each dataset group\n",
    "for dataset_group in pnet_df['datasets'].unique():\n",
    "    # Filter the DataFrame to include only rows corresponding to the current dataset group\n",
    "    dataset_group_df = pnet_df[pnet_df['datasets'] == dataset_group]\n",
    "    \n",
    "    # Initialize empty DataFrames to store results for the current dataset group\n",
    "    df_imps = pd.DataFrame()\n",
    "    df_ranks = pd.DataFrame()\n",
    "    \n",
    "    # Loop through the rows of the DataFrame containing run information for the current dataset group\n",
    "    for index, row in dataset_group_df.iterrows():\n",
    "        importances_path = row['feature_importances_path']\n",
    "        imps = pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "\n",
    "        # Read feature importances from the specified file path\n",
    "        imps = imps.join(prostate_response).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "\n",
    "        ranks = imps.rank(ascending=False)\n",
    "\n",
    "        # Store results in separate DataFrames for the current dataset group\n",
    "        df_imps[f'{index}'] = imps\n",
    "        df_ranks[f'{index}'] = ranks\n",
    "\n",
    "    # Calculate signal-to-noise ratio (SNR) for the current dataset group\n",
    "    snr = df_imps.mean(axis=1) / (df_imps.std(axis=1) + 1e-9)\n",
    "    \n",
    "    # Store the SNR results for the current dataset group in the dictionary\n",
    "    snr_dict[dataset_group] = snr.sort_values(ascending=False)\n",
    "\n",
    "# Now, snr_dict contains SNR results for each dataset group\n",
    "snr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store the top 10 values by P-NET rank for each dataset\n",
    "top_10_snr_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over df_ranks_by_dataset dictionary\n",
    "for dataset, df in snr_dict.items():\n",
    "    # Calculate the mean ranking for each run and select the top 10\n",
    "    top_10 = df.sort_values(ascending=False)[:10]\n",
    "    \n",
    "    # Add the top 10 values as a column to the DataFrame\n",
    "    top_10_snr_df[dataset] = top_10.index\n",
    "\n",
    "# Set the index of the DataFrame to be 1 through 10\n",
    "top_10_snr_df.index = range(1, 11)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Top features by SNR score:\")\n",
    "display(top_10_snr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, s in snr_dict.items():\n",
    "    print(f\"{k}\")\n",
    "    display(s[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the SNR on all pnet model runs combined\n",
    "# Initialize empty DataFrames to store results\n",
    "df_imps = pd.DataFrame()\n",
    "df_ranks = pd.DataFrame()\n",
    "\n",
    "# Restrict to just P-NET models for now\n",
    "pnet_df = pnet_df[pnet_df['model_type']==\"pnet\"]\n",
    "# Loop through the rows of the DataFrame containing run information\n",
    "for index, row in pnet_df.iterrows():\n",
    "    importances_path = row['feature_importances_path']\n",
    "    imps = pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "\n",
    "    # Read feature importances from the specified file path\n",
    "    imps = imps.join(prostate_response).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "    \n",
    "    ranks = imps.rank(ascending=False)\n",
    "\n",
    "    # Store results in separate DataFrames\n",
    "    df_imps[f'{index}'] = imps\n",
    "    df_ranks[f'{index}'] = ranks\n",
    "\n",
    "# Calculate signal-to-noise ratio (SNR) -- calculate the average score for a gene across all runs, then divide by its stdev across all runs\n",
    "snr = df_imps.mean(axis=1) / (df_imps.std(axis=1) + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks correct (fits my model for positive control success).\n",
    "display(snr.sort_values(ascending=False)[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-NET: impact of extra regularization on the first hidden layer (aka genes)\n",
    "Sweep `g8cl6qur` was just looking at the somatic datasets (restricted to the ~943 paired samples only) and run on the vanilla P-NET model.\\\n",
    "\n",
    "See the run information for this W&B sweep at https://wandb.ai/millergw/prostate_met_status/sweeps/g8cl6qur/table?nw=nwusermillergw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Define parameters\n",
    "project_name = \"millergw/prostate_met_status\"\n",
    "sweep_id = \"g8cl6qur\"\n",
    "\n",
    "# Fetch the runs\n",
    "runs = api.runs(project_name, filters={\"sweep\": sweep_id, \"state\": \"finished\"})\n",
    "print(\"We have found {} finished runs in project {} and sweep_id {}\".format(len(runs), project_name, sweep_id))\n",
    "\n",
    "# Group AUC scores by the h1 regularization parameters\n",
    "auc_scores_by_group = defaultdict(list)\n",
    "\n",
    "# Extract relevant information and group by model_type and datasets\n",
    "for run in runs:\n",
    "    h1_alpha = run.config.get(\"h1_alpha\", \"unknown\")\n",
    "    h1_regularization_method = run.config.get(\"h1_regularization_method\", \"unknown\")\n",
    "    l1_ratio = run.config.get(\"l1_ratio\", \"unknown\")\n",
    "    auc_score = run.summary.get(\"validation_roc_auc_score\")\n",
    "    if auc_score is not None:\n",
    "        auc_scores_by_group[(h1_regularization_method, h1_alpha, l1_ratio)].append(auc_score)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\"h1_regularization_method\":h1_regularization_method, \"h1_alpha\":h1_alpha, \"l1_ratio\":l1_ratio,\n",
    "     \"auc\": auc_score}\n",
    "    for (h1_regularization_method, h1_alpha, l1_ratio), auc_scores in auc_scores_by_group.items()\n",
    "    for auc_score in auc_scores\n",
    "])\n",
    "\n",
    "logging.debug(\"Sorting model order by decreasing mean\")\n",
    "# Calculate the mean of each group\n",
    "group_means = df.groupby(['h1_regularization_method', 'h1_alpha'])['auc'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Reorder the DataFrame based on the descending group means\n",
    "df = df.set_index(['h1_regularization_method', 'h1_alpha']).loc[group_means.index].reset_index()\n",
    "print(df.shape)\n",
    "\n",
    "# Calculate mean, standard deviation, and sample size of the AUC scores for each group\n",
    "grouped_stats = df.groupby(['h1_regularization_method', 'h1_alpha'])['auc'].agg(\n",
    "    mean='mean',\n",
    "    std='std',\n",
    "    num_samples='count'\n",
    ").loc[group_means.index].reset_index()\n",
    "\n",
    "display(grouped_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv GAN reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\"millergw/init_variance\", filters={\"sweep\": \"9sob1jgy\", \"state\": \"Finished\"})\n",
    "\n",
    "# TODO: fix/recover. Need to pull all of these metrics from wandb\n",
    "d = {}\n",
    "# Iterate over each dictionary in the list and extract metric values\n",
    "for run in runs:\n",
    "    aucs.append(d['auc'])\n",
    "    accs.append(d['accuracy'])\n",
    "    auprs.append(d['aupr'])\n",
    "    f1_scores.append(d['f1'])\n",
    "    precisions.append(d['precision'])\n",
    "    recalls.append(d['recall'])\n",
    "\n",
    "# Create a new dictionary with metric lists\n",
    "arxiv_gcn_metrics_dict = {\n",
    "    'auc': d['aucs'],\n",
    "    'acc': d['accs'],\n",
    "    'aupr': d['auprs'],\n",
    "    'f1': d['f1_scores'],\n",
    "    'precision': d['precisions'],\n",
    "    'recall': d['recalls']\n",
    "}\n",
    "\n",
    "# Now metrics_dict contains the desired structure with lists of metric values\n",
    "logging.info(f\"Computing performance metric stats using data from {len(runs)} runs\")\n",
    "for k, v in arxiv_gcn_metrics_dict.items():\n",
    "    print(\"Arxiv GCN %s: %.2f +/- %.2f\" % (k, np.mean(v), np.std(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv P-NET reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Grabbing the performance metrics for arxiv P-NET from saved pickle file\")\n",
    "# with open(os.path.join(ARXIV_PNET_DIR, 'pnet_results.h6.torch.num_workers_16.pkl'), 'rb') as file:\n",
    "with open(os.path.join(ARXIV_PNET_DIR, 'pnet_results.h6.torch.pkl'), 'rb') as file:\n",
    "    arxiv_pnet_results = pickle.load(file)\n",
    "\n",
    "arxiv_pnet_results[0].keys()\n",
    "# arxiv_pnet_aucs = arxiv_pnet_results['auc']\n",
    "# arxiv_pnet_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for each metric\n",
    "aucs = []\n",
    "accs = []\n",
    "auprs = []\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Iterate over each dictionary in the list and extract metric values\n",
    "for d in arxiv_pnet_results:\n",
    "    aucs.append(d['auc'])\n",
    "    accs.append(d['accuracy'])\n",
    "    auprs.append(d['aupr'])\n",
    "    f1_scores.append(d['f1'])\n",
    "    precisions.append(d['precision'])\n",
    "    recalls.append(d['recall'])\n",
    "\n",
    "# Create a new dictionary with metric lists\n",
    "arxiv_pnet_metrics_dict = {\n",
    "    'auc': aucs,\n",
    "    'acc': accs,\n",
    "    'aupr': auprs,\n",
    "    'f1': f1_scores,\n",
    "    'precision': precisions,\n",
    "    'recall': recalls\n",
    "}\n",
    "\n",
    "# Now metrics_dict contains the desired structure with lists of metric values\n",
    "logging.info(f\"Computing performance metric stats using data from {len(arxiv_pnet_results)} runs\")\n",
    "for k, v in arxiv_pnet_metrics_dict.items():\n",
    "    print(\"Arxiv P-NET %s: %.2f +/- %.2f\" % (k, np.mean(v), np.std(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BDT, RF, and Marc's torch P-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Grabbing the performance metrics for BDT and RF from W&B\")\n",
    "# Specify your project and run group\n",
    "entity = \"millergw\"\n",
    "project_name = \"prostate_met_status\"\n",
    "run_group = \"bdt_stability_experiment_004\"\n",
    "metric = \"test_roc_auc_score\"\n",
    "bdt_eval_auc = stability_utils.get_summary_metric_from_wandb(entity, project_name, metric, run_group=run_group)\n",
    "print(\"BDT AUC:\", np.mean(bdt_eval_auc))\n",
    "\n",
    "# Specify your project and run group\n",
    "run_group = \"rf_stability_experiment_003\"\n",
    "metric = \"test_roc_auc_score\"\n",
    "rf_eval_auc = stability_utils.get_summary_metric_from_wandb(entity, project_name, metric, run_group=run_group)\n",
    "print(\"RF AUC:\", np.mean(rf_eval_auc))\n",
    "\n",
    "logging.info(\"Grabbing the performance metrics for P-NET from saved pickle file\")\n",
    "# Read gene_imps from a Pickle file (format: len 20 list --> pandas DFs, samples x genes?)\n",
    "with open(os.path.join(PNET_DIR, 'aucs.pkl'), 'rb') as file:\n",
    "    pnet_aucs = pickle.load(file)\n",
    "print(\"P-NET AUC:\", np.mean(pnet_aucs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Constructing a DF of AUCs\")\n",
    "auc_df = pd.DataFrame({\n",
    "    'Group': ['P-NET'] * len(pnet_aucs) +\n",
    "             ['RF'] * len(rf_eval_auc) +\n",
    "             ['BDT'] * len(bdt_eval_auc) +\n",
    "             ['arxiv P-NET (h=6)'] * len(arxiv_pnet_metrics_dict['auc']) +\n",
    "             ['arxiv GCN'] * len(arxiv_gcn_metrics_dict['auc']),\n",
    "    'Value': pnet_aucs + rf_eval_auc + bdt_eval_auc + arxiv_pnet_metrics_dict['auc'] + arxiv_gcn_metrics_dict['auc']\n",
    "})\n",
    "\n",
    "logging.debug(\"Sorting model order by decreasing mean\")\n",
    "# Calculate the mean of each group\n",
    "group_means = auc_df.groupby('Group')['Value'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Reorder the DataFrame based on the descending group means\n",
    "auc_df = auc_df.set_index('Group').loc[group_means.index].reset_index()\n",
    "grouped_auc_df = auc_df.groupby('Group')\n",
    "display(auc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\\n\",auc_df.mean())\n",
    "print(\"median\\n\",auc_df.median())\n",
    "print(\"stdev\\n\",auc_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"mean\\n\",auc_df.mean())\n",
    "# print(\"median\\n\",auc_df.median())\n",
    "# print(\"stdev\\n\",auc_df.std())\n",
    "# display(auc_df.groupby('Group').median())\n",
    "display(grouped_auc_df.median())\n",
    "display(grouped_auc_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVENAME = 'pnet_rf_bdt_performance_benchmark_AUC_w_points'\n",
    "SAVENAME = 'pnet_gcn_arxivPnet_rf_bdt_performance_benchmark_AUC_w_points'\n",
    "# SAVENAME_nopoints = 'pnet_rf_bdt_performance_benchmark_AUC'\n",
    "SAVENAME_nopoints = 'pnet_gcn_arxivPnet_rf_bdt_performance_benchmark_AUC'\n",
    "\n",
    "smallest_median_val = auc_df.groupby('Group').median().min().min()\n",
    "\n",
    "# Set up the boxplot with points using Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x='Group', y='Value',\n",
    "    data=auc_df, color='gray', showfliers=False, boxprops={'facecolor': 'lightgrey', 'color': 'lightgrey'},\n",
    "                      whiskerprops={'color': 'gainsboro'},\n",
    "                      capprops={'color': 'gainsboro'},\n",
    "                      medianprops={'color': 'dimgrey'})\n",
    "sns.stripplot(x='Group', y='Value', data=auc_df, color='black', jitter=0.2, alpha=0.3)\n",
    "\n",
    "# Customize plot appearance\n",
    "ax = plt.gca()\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "plt.axhline(y=smallest_median_val, color='coral', linestyle='--', label=f'y_min = {smallest_median_val}', alpha=0.5)\n",
    "\n",
    "# Calculate the number of samples in each group\n",
    "sample_counts = auc_df['Group'].value_counts()\n",
    "# Create custom x-axis labels with sample counts\n",
    "x_labels = [f\"{group} (n={sample_counts[group]})\" for group in sample_counts.index]\n",
    "plt.xticks(range(len(sample_counts.index)), x_labels, rotation=45)\n",
    "\n",
    "ax.set_ylabel('AUC', size=14)\n",
    "ax.set_xlabel('Model', size=14)\n",
    "ax.set_ylim((0.5, 1))\n",
    "plt.title(\"Relative performance on test set\")\n",
    "report_and_eval.savefig(plt, os.path.join(FIGDIR, SAVENAME), png=True, pdf=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Set up the boxplot WITHOUT points using Seaborn\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x='Group', y='Value',\n",
    "    data=auc_df, color='gray', showfliers=True, boxprops={'facecolor': 'lightgrey', 'color': 'lightgrey'},\n",
    "                      whiskerprops={'color': 'gainsboro'},\n",
    "                      capprops={'color': 'gainsboro'},\n",
    "                      medianprops={'color': 'dimgrey'})\n",
    "\n",
    "# Customize plot appearance\n",
    "ax = plt.gca()\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "plt.axhline(y=smallest_median_val, color='coral', linestyle='--', label=f'y_min = {smallest_median_val}', alpha=0.5)\n",
    "\n",
    "# Calculate the number of samples in each group\n",
    "sample_counts = auc_df['Group'].value_counts()\n",
    "# Create custom x-axis labels with sample counts\n",
    "x_labels = [f\"{group} (n={sample_counts[group]})\" for group in sample_counts.index]\n",
    "plt.xticks(range(len(sample_counts.index)), x_labels, rotation=45)\n",
    "\n",
    "\n",
    "ax.set_ylabel('AUC', size=14)\n",
    "ax.set_xlabel('Model', size=14)\n",
    "ax.set_ylim((0.5, 1))\n",
    "plt.title(\"Relative performance on test set\")\n",
    "report_and_eval.savefig(plt, os.path.join(FIGDIR, SAVENAME_nopoints), png=True, pdf=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative stability of feature importance\n",
    "- have gene-level for P-NET, RF, and BDT\n",
    "- also have layer/pathway level information for P-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Read gene_imps from a Pickle file (format: len 20 list --> pandas DFs, samples x genes?)\n",
    "with open(os.path.join(PNET_DIR, 'gene_imps.pkl'), 'rb') as file:\n",
    "    gene_imps = pickle.load(file)\n",
    "\n",
    "# Read layerwise_imps from a Pickle file (format: len 20 list --> len 5 list --> pandas DF, samples x features)\n",
    "with open(os.path.join(PNET_DIR, 'layerwise_imps.pkl'), 'rb') as file:\n",
    "    layerwise_imps = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in layerwise_imps[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnet_gene_imps = stability_utils.get_pnet_gene_imps(PNET_DIR)\n",
    "pnet_patient_stabs = stability_utils.calc_perpatient_stability_metric(pnet_gene_imps)\n",
    "\n",
    "logging.info(\"Plotting histogram of patient-level stability metric\")\n",
    "# median stdev of top 50 genes (top 50 relative to each patient). \n",
    "# Pretty darn unstable. \n",
    "# For example, a gene at rank 50 with a stdev of 50 means that 68% of the time, its rank was between 0 and 100.\n",
    "plt.hist(pnet_patient_stabs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"rf\"\n",
    "EVAL_SET = 'test' # val, validation\n",
    "SAVEDIR = f'../../pnet/results/somatic_{MODEL_TYPE}_eval_set_{EVAL_SET}'\n",
    "\n",
    "rf_gene_imps = stability_utils.get_sklearn_feature_imps(SAVEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM: RF and BDT don't have per-patient feature importances. How did Marc deal with this? He said that he normally grouped by the response variable.\n",
    "# Maybe I'll just look at the overall model-level stability here. That'll be easiest to compare anyway. :/\n",
    "print(type(pnet_gene_imps[0]))\n",
    "print(type(rf_gene_imps[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_stability = stability_utils.calc_model_stability(rf_gene_imps, n_top_genes=50)\n",
    "rf_model_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf_gene_imps).apply(lambda row: row.abs().rank(ascending=False).sort_values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create sorted DF across all runs\n",
    "for i in range(len(rf_gene_imps)):\n",
    "    print(f\"Run {i}\")\n",
    "    print(rf_gene_imps[i].rank(ascending=False).sort_values()[:10])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"bdt\"\n",
    "EVAL_SET = 'test' # val\n",
    "SAVEDIR = f'../../pnet/results/somatic_{MODEL_TYPE}_eval_set_{EVAL_SET}'\n",
    "\n",
    "bdt_gene_imps = stability_utils.get_sklearn_feature_imps(SAVEDIR)\n",
    "bdt_model_stability = stability_utils.calc_model_stability(bdt_gene_imps, n_top_genes=50)\n",
    "bdt_model_stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we create a runs x feature rank DF (values = feature name). This is useful for comparing across runs. The input was a list of series, where each series is the gene imp list from a given run.\n",
    "# Create a DataFrame to store the ranks\n",
    "rank_df = pd.DataFrame()\n",
    "tmp_ranks = []\n",
    "series_list = bdt_gene_imps\n",
    "# series_list = rf_gene_imps\n",
    "# Iterate through each series, calculate ranks, and add to the DataFrame\n",
    "for i in range(len(series_list)):\n",
    "    series = series_list[i]\n",
    "    # Calculate ranks and convert them to integers\n",
    "    ranks = series.abs().rank(ascending=False, method='dense').astype(int).sort_values()\n",
    "    tmp_ranks.append(ranks.index.tolist())\n",
    "    # Add ranks to the DataFrame\n",
    "    # rank_df[series.name] = ranks\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "N = 10\n",
    "rank_df = pd.DataFrame(tmp_ranks)\n",
    "display(rank_df.loc[:,:N])\n",
    "\n",
    "print(rank_df.loc[:,:N].stack().value_counts())\n",
    "top_unique_features = rank_df.loc[:,:N].stack().unique()\n",
    "print([i.split(\"_\")[0] for i in top_unique_features])\n",
    "print(len(top_unique_features))\n",
    "\n",
    "\n",
    "# Display a gene x rank DF (value = # times that gene had that rank across the N=20 runs)\n",
    "top_gene_by_rank_consistency_df = rank_df.loc[:,:N].apply(lambda col: col.value_counts()).fillna('').reindex(top_unique_features)\n",
    "display(top_gene_by_rank_consistency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(len(rf_gene_imps)):\n",
    "    top_rank_run1 = rf_gene_imps[run].rank(ascending=False).sort_values().index[:100]\n",
    "    plt.bar([i.split('_')[0] for i in top_rank_run1], rf_gene_imps[run].loc[top_rank_run1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(len(bdt_gene_imps)):\n",
    "    top_rank_run1 = bdt_gene_imps[run].rank(ascending=False).sort_values().index[:100]\n",
    "    plt.bar([i.split('_')[0] for i in top_rank_run1], bdt_gene_imps[run].loc[top_rank_run1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Exploring the ranks particular features of interest\")\n",
    "tmp = []\n",
    "for run in range(len(bdt_gene_imps)):\n",
    "    tmp.append(bdt_gene_imps[run].rank(ascending=False).sort_values()[\n",
    "        ['MDM4_somatic_amp', 'MDM4_somatic_del', 'MDM4_somatic_mut']\n",
    "        # ['FGFR1_somatic_amp', 'FGFR1_somatic_del', 'FGFR1_somatic_mut']\n",
    "        ].tolist())\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring magnitude of feature importances\n",
    "- How change from model to model?\n",
    "- What does the distribution look like across top genes? All genes? Flat, or large drop-off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=1\n",
    "top_rank_run1 = bdt_gene_imps[run].rank(ascending=False).sort_values().index[:10]\n",
    "plt.bar([i.split('_')[0] for i in top_rank_run1], bdt_gene_imps[run].loc[top_rank_run1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create sorted DF across all runs\n",
    "for i in range(len(bdt_gene_imps)):\n",
    "    print(f\"Run {i}\")\n",
    "    print(bdt_gene_imps[i].rank(ascending=False).sort_values()[:10])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ranking = pd.DataFrame(rf_gene_imps).apply(lambda row: row.abs().rank(ascending=False), axis=1)\n",
    "tmp_ranking.loc[:,['MDM4_somatic_mut', 'MDM4_somatic_amp', 'MDM4_somatic_del']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Checking that both methods return the same result.\")\n",
    "pnet_rankings = stability_utils.make_perpatient_rankings_dfs(pnet_gene_imps)\n",
    "print(pnet_rankings[0].loc[:,'MDM4'])\n",
    "pnet_patient_0_rank = stability_utils.make_pnet_gene_ranking_df(pnet_gene_imps, 0)\n",
    "pnet_patient_0_rank.loc[:,'MDM4']\n",
    "\n",
    "# calc_stability_metric_on_runs_by_generank_df(rankings_df, n_top_genes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
