{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Purpose of this file is to save down relevant information from the W&B runs that constitute the data for the paper. These will become Supplementary Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "sys.path.insert(0, \"../..\")  # add project_config to path\n",
    "import project_config\n",
    "\n",
    "# Setup Logging and Configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define WandB project and sweep details\n",
    "PROJECT_NAME = \"millergw/prostate_met_status\"\n",
    "\n",
    "# # Define directories for saving results\n",
    "RESULTS_DIR = project_config.SUPPLEMENTARY_TABLES_DIR\n",
    "\n",
    "joint_simu_result_savepath = os.path.join(RESULTS_DIR, \"joint_simulation_prediction_metrics_per_run.csv\")\n",
    "single_gene_simu_result_savepath = os.path.join(RESULTS_DIR, \"single_gene_spike_in_simulation_prediction_metrics_per_run.csv\")\n",
    "p1000_result_savepath = os.path.join(RESULTS_DIR, \"p1000_empirical_prediction_metrics_per_run.csv\")\n",
    "# Define paths for grouped summary results\n",
    "grouped_joint_simu_result_savepath = os.path.join(RESULTS_DIR, \"joint_simulation_prediction_metrics_per_group.csv\")\n",
    "grouped_single_gene_simu_result_savepath = os.path.join(RESULTS_DIR, \"single_gene_spike_in_simulation_prediction_metrics_per_group.csv\")\n",
    "grouped_p1000_result_savepath = os.path.join(RESULTS_DIR, \"p1000_empirical_prediction_metrics_per_group.csv\")\n",
    "\n",
    "# make directory if it doesn't exist\n",
    "os.makedirs(os.path.join(RESULTS_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tags to final runs\n",
    "# Initialize W&B API\n",
    "api = wandb.Api() \n",
    "columns_to_retrieve = [\n",
    "    \"run_id\", \"run_name\", \"model_type\", \n",
    "    \"datasets\",\n",
    "    \"sample_binary\", \"n_samples\",  \"sigma\", \"odds_ratio\", \"num_class1_samples\", \"num_class0_samples\", \"control_frequency\",\n",
    "    \"deltaMuGenes\", \"mod0_genes\", \"mod1_genes\",\n",
    "    \"save_dir\",\n",
    "]\n",
    "\n",
    "performance_metric_columns_to_retrieve = [\n",
    "    \"train_average_precision_score\", \"validation_average_precision_score\", \"test_average_precision_score\",\n",
    "    \"train_roc_auc_score\", \"validation_roc_auc_score\", \"test_roc_auc_score\",\n",
    "    \"train_f1_score\", \"validation_f1_score\", \"test_f1_score\", \n",
    "    \"train_balanced_acc\", \"validation_balanced_acc\", \"test_balanced_acc\", \n",
    "    \"train_acc\", \"validation_acc\", \"test_acc\",\n",
    "    \"train_confusion_matrix\", \"validation_confusion_matrix\", \"test_confusion_matrix\",\n",
    "]\n",
    "\n",
    "columns_to_retrieve.extend(performance_metric_columns_to_retrieve)\n",
    "\n",
    "logging.info(f\"Retrieving data from {len(columns_to_retrieve)} columns: {columns_to_retrieve}\")\n",
    "\n",
    "performance_metric_col_order = [\n",
    "    \"train_avg_precision\", \"validation_avg_precision\", \"test_avg_precision\",\n",
    "    \"train_roc_auc_score\", \"validation_roc_auc_score\", \"test_roc_auc_score\",\n",
    "    \"train_f1_score\", \"validation_f1_score\", \"test_f1_score\", \n",
    "    \"train_balanced_acc\", \"validation_balanced_acc\", \"test_balanced_acc\", \n",
    "    \"train_acc\", \"validation_acc\", \"test_acc\",\n",
    "    # \"train_confusion_matrix\", \"validation_confusion_matrix\", \"test_confusion_matrix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_over_replicates(df, group_by_cols):\n",
    "    # Average over seeds\n",
    "    # Add a counts column for each group\n",
    "    df_counts = df.groupby(group_by_cols).size().reset_index(name=\"count\")\n",
    "    df_avg = df.groupby(group_by_cols, as_index=False).mean()\n",
    "    df_avg = df_avg.merge(df_counts, on=group_by_cols)\n",
    "    return df_avg\n",
    "\n",
    "def make_grouped_summary_with_mean_and_stdev(df, param_cols, metric_cols):\n",
    "    # 1) Count per group\n",
    "    counts = df.groupby(param_cols).size().reset_index(name=\"count\")\n",
    "\n",
    "    # 2) Mean & std per group\n",
    "    summary = (\n",
    "        df.groupby(param_cols)[metric_cols]\n",
    "        .agg(['mean', 'std'])\n",
    "    )\n",
    "\n",
    "    # Flatten multi-index columns\n",
    "    summary.columns = [f\"{m}_{stat}\" for m, stat in summary.columns]\n",
    "    summary = summary.reset_index()\n",
    "\n",
    "    # 3) Merge counts back in\n",
    "    summary_numeric = pd.merge(counts, summary, on=param_cols)\n",
    "\n",
    "    # 4) Formatted as \"mean ± std\"\n",
    "    summary_formatted = summary_numeric.assign(**{\n",
    "        m: summary_numeric[f\"{m}_mean\"].round(3).astype(str) \n",
    "        + \" ± \" + summary_numeric[f\"{m}_std\"].round(3).astype(str)\n",
    "        for m in metric_cols\n",
    "    })[param_cols + [\"count\"] + metric_cols]\n",
    "    return summary_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Joint sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "runs = api.runs(f\"{PROJECT_NAME}\", filters={\"tags\": \"2D-simu-20250822\"})\n",
    "    # \"$and\": [{\"tags\": \"pnet-simu-paper-20250822\"}, {\"tags\": \"1D-simu-20250822\"}, {\"tags\": \"2D-simu-20250822\"}, {\"tags\": \"p1000-20250822\"}]})\n",
    "logging.info(f\"Working on joint simulation results: {len(runs)} runs found\")\n",
    "\n",
    "for run in runs: \n",
    "    config = run.config\n",
    "    summary = run.summary\n",
    "    all_records.append({\n",
    "        \"run_id\": run.id,\n",
    "        \"run_name\": run.name,\n",
    "        \"model_type\": config.get(\"model_type\"),\n",
    "        \"sample_binary\": config.get(\"sample_binary\"),\n",
    "        \"n_samples\": config.get(\"num_samples\"),\n",
    "        \"datasets\": config.get(\"datasets\"),\n",
    "        \"sigma\": float(config.get(\"sigma\")),\n",
    "        \"odds_ratio\": float(config.get(\"odds_ratio\")),\n",
    "        \"save_dir\": config.get(\"save_dir\"),\n",
    "        \"num_class1_samples\": config.get(\"num_class1_samples\"),\n",
    "        \"num_class0_samples\": config.get(\"num_class0_samples\"),\n",
    "        \"deltaMuGenes\": config.get(\"deltaMuGenes\"),\n",
    "        \"mod0_genes\": config.get(\"mod0_genes\"),\n",
    "        \"mod1_genes\": config.get(\"mod1_genes\"),\n",
    "    })\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_roc_auc_score\"] = summary.get(f\"{split}_roc_auc_score\")\n",
    "        all_records[-1][f\"{split}_balanced_acc\"] = summary.get(f\"{split}_balanced_acc\")\n",
    "        all_records[-1][f\"{split}_avg_precision\"] = summary.get(f\"{split}_average_precision_score\")\n",
    "        all_records[-1][f\"{split}_acc\"] = summary.get(f\"{split}_acc\")\n",
    "        all_records[-1][f\"{split}_f1_score\"] = summary.get(f\"{split}_f1_score\")\n",
    "        # all_records[-1][f\"{split}_confusion_matrix\"] = summary.get(f\"{split}_confusion_matrix\")\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_feature_importances_path\"] = os.path.join(config.get(\"save_dir\"), f\"{split}_gene_feature_importances.csv\")\n",
    "        all_records[-1][f\"{split}_gene_importances_path\"] = os.path.join(config.get(\"save_dir\"), f\"{split}_gene_importances.csv\")\n",
    "\n",
    "logging.debug(\"Convert to a single DataFrame\")\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "group_by_cols = [\"model_type\", \"odds_ratio\", \"sigma\", \"sample_binary\", \"n_samples\"]\n",
    "logging.debug(f'Add a unique group identifer column by joining together the unique identifiers: {group_by_cols}')\n",
    "df[\"group_identifier\"] = df.apply(\n",
    "    lambda row: f\"OR-{row['odds_ratio']}_sigma-{row['sigma']}_nSamples-{row['n_samples']}_sampleBinary-{row['sample_binary']}\",\n",
    "    axis=1)\n",
    "\n",
    "logging.debug(\"Changing the column order for the final, column-filtered DataFrame\")\n",
    "col_order = group_by_cols + performance_metric_col_order\n",
    "full_cols = col_order + [c for c in df.columns if c not in col_order]\n",
    "\n",
    "df_full = df[full_cols].copy()\n",
    "df = df[col_order]\n",
    "\n",
    "logging.debug(\"Make results DF collapsed over replicates\")\n",
    "df_group_summary = make_grouped_summary_with_mean_and_stdev(df, param_cols=group_by_cols, metric_cols=performance_metric_col_order)\n",
    "\n",
    "logging.info(f\"Saving the joint simulation results DataFrame to CSV at {joint_simu_result_savepath}\")\n",
    "df_full.to_csv(joint_simu_result_savepath.replace(\".csv\", \"_full.csv\"), float_format=\"%.3f\", index=False, )\n",
    "df.to_csv(joint_simu_result_savepath, float_format=\"%.3f\", index=False)\n",
    "df_group_summary.to_csv(grouped_joint_simu_result_savepath, index=False)\n",
    "\n",
    "display(df.round(3).head(2))\n",
    "display(df_group_summary.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Single-gene spike-in simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "runs = api.runs(f\"{PROJECT_NAME}\", filters={\"tags\": \"1D-simu-20250822\"})\n",
    "    # \"$and\": [{\"tags\": \"pnet-simu-paper-20250822\"}, {\"tags\": \"1D-simu-20250822\"}, {\"tags\": \"2D-simu-20250822\"}, {\"tags\": \"p1000-20250822\"}]})\n",
    "logging.info(f\"Working on single-gene simulation results: {len(runs)} runs found\")\n",
    "\n",
    "for run in runs: \n",
    "    config = run.config\n",
    "    summary = run.summary\n",
    "    all_records.append({\n",
    "        \"run_id\": run.id,\n",
    "        \"model_type\": config.get(\"model_type\"),\n",
    "        \"datasets\": config.get(\"datasets\"),\n",
    "        \"n_features\": config.get(\"n_features\"),\n",
    "        \"odds_ratio\": float(config.get(\"odds_ratio\")),\n",
    "        \"control_frequency\": float(config.get(\"control_frequency\")),\n",
    "        \"save_dir\": config.get(\"save_dir\").replace(\"../../results/\", \"/mnt/disks/gmiller_data1/pnet/results/\"),\n",
    "\n",
    "        \"perturbation_suffix\": config.get(\"perturbation_suffix\"),\n",
    "        \"perturbed_data_dir\": config.get(\"perturbed_data_dir\"),\n",
    "        \"target_f\": os.path.join(config.get(\"perturbed_data_dir\"), f\"y_{config.get('perturbation_suffix')}.csv\"),\n",
    "    })\n",
    "\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_roc_auc_score\"] = summary.get(f\"{split}_roc_auc_score\")\n",
    "        all_records[-1][f\"{split}_balanced_acc\"] = summary.get(f\"{split}_balanced_acc\")\n",
    "        all_records[-1][f\"{split}_avg_precision\"] = summary.get(f\"{split}_average_precision_score\")\n",
    "        all_records[-1][f\"{split}_acc\"] = summary.get(f\"{split}_acc\")\n",
    "        all_records[-1][f\"{split}_f1_score\"] = summary.get(f\"{split}_f1_score\")\n",
    "        # all_records[-1][f\"{split}_confusion_matrix\"] = summary.get(f\"{split}_confusion_matrix\")\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_feature_importances_path\"] = os.path.join(all_records[-1][\"save_dir\"], f\"{split}_gene_feature_importances.csv\")\n",
    "        all_records[-1][f\"{split}_gene_importances_path\"] = os.path.join(all_records[-1][\"save_dir\"], f\"{split}_gene_importances.csv\")\n",
    "\n",
    "logging.debug(\"Convert to a single DataFrame\")\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "logging.debug(\"Changing the column order for the final, column-filtered DataFrame\")\n",
    "group_by_cols = [\"model_type\", \"n_features\", \"odds_ratio\", \"control_frequency\"]\n",
    "col_order = group_by_cols + performance_metric_col_order\n",
    "full_cols = col_order + [c for c in df.columns if c not in col_order]\n",
    "df_full = df[full_cols].copy()\n",
    "\n",
    "df = df[col_order]\n",
    "\n",
    "logging.debug(\"Make results DF collapsed over replicates\")\n",
    "df_group_summary = make_grouped_summary_with_mean_and_stdev(df, param_cols=group_by_cols, metric_cols=performance_metric_col_order)\n",
    "\n",
    "logging.info(f\"Saving the single-gene simulation results DataFrame to CSV at {single_gene_simu_result_savepath}\")\n",
    "df_full.to_csv(single_gene_simu_result_savepath.replace(\".csv\", \"_full.csv\"), float_format=\"%.3f\", index=False, )\n",
    "df.to_csv(single_gene_simu_result_savepath, float_format=\"%.3f\", index=False)\n",
    "df_group_summary.to_csv(grouped_single_gene_simu_result_savepath, index=False)\n",
    "\n",
    "display(df.round(3).head(2))\n",
    "display(df_group_summary.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Empirical results: P1000 somatic +/- germline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "runs = api.runs(f\"{PROJECT_NAME}\", filters={\"tags\": \"p1000-20250822\"})\n",
    "    # \"$and\": [{\"tags\": \"pnet-simu-paper-20250822\"}, {\"tags\": \"1D-simu-20250822\"}, {\"tags\": \"2D-simu-20250822\"}, {\"tags\": \"p1000-20250822\"}]})\n",
    "logging.info(f\"Working on empirical P1000 results: {len(runs)} runs found\")\n",
    "\n",
    "for run in runs: \n",
    "    config = run.config\n",
    "    summary = run.summary\n",
    "    all_records.append({\n",
    "        \"run_id\": run.id,\n",
    "        \"model_type\": config.get(\"model_type\"),\n",
    "        \"datasets\": config.get(\"datasets\"),\n",
    "        \"save_dir\": config.get(\"save_dir\").replace(\"../../results/\", \"/mnt/disks/gmiller_data1/pnet/results/\"),\n",
    "        \"input_data_dir\": config.get(\"input_data_dir\"),\n",
    "    })\n",
    "\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_roc_auc_score\"] = summary.get(f\"{split}_roc_auc_score\")\n",
    "        all_records[-1][f\"{split}_balanced_acc\"] = summary.get(f\"{split}_balanced_acc\")\n",
    "        all_records[-1][f\"{split}_avg_precision\"] = summary.get(f\"{split}_average_precision_score\")\n",
    "        all_records[-1][f\"{split}_acc\"] = summary.get(f\"{split}_acc\")\n",
    "        all_records[-1][f\"{split}_f1_score\"] = summary.get(f\"{split}_f1_score\")\n",
    "        # all_records[-1][f\"{split}_confusion_matrix\"] = summary.get(f\"{split}_confusion_matrix\")\n",
    "    for split in [\"validation\", \"test\"]:\n",
    "        all_records[-1][f\"{split}_feature_importances_path\"] = os.path.join(all_records[-1][\"save_dir\"], f\"{split}_gene_feature_importances.csv\")\n",
    "        all_records[-1][f\"{split}_gene_importances_path\"] = os.path.join(all_records[-1][\"save_dir\"], f\"{split}_gene_importances.csv\")\n",
    "\n",
    "logging.debug(\"Convert to a single DataFrame\")\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "logging.debug(\"Changing the column order for the final, column-filtered DataFrame\")\n",
    "group_by_cols = [\"model_type\",\"datasets\"]\n",
    "col_order = group_by_cols + performance_metric_col_order\n",
    "full_cols = col_order + [c for c in df.columns if c not in col_order]\n",
    "\n",
    "df_full = df[full_cols].copy()\n",
    "df = df[col_order]\n",
    "logging.debug(\"Make results DF collapsed over replicates\")\n",
    "df_group_summary = make_grouped_summary_with_mean_and_stdev(df, param_cols=group_by_cols, metric_cols=performance_metric_col_order)\n",
    "\n",
    "logging.info(f\"Saving the empirical P1000 results DataFrame to CSV at {p1000_result_savepath}\")\n",
    "df_full.to_csv(p1000_result_savepath.replace(\".csv\", \"_full.csv\"), float_format=\"%.3f\", index=False, )\n",
    "df.to_csv(p1000_result_savepath, float_format=\"%.3f\", index=False, )\n",
    "df_group_summary.to_csv(grouped_p1000_result_savepath, index=False)\n",
    "\n",
    "display(df.round(3).head(2))\n",
    "display(df_group_summary.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_full.shape)\n",
    "print(df.shape)\n",
    "print(df_group_summary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
