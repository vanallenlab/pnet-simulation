{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generate figures for the empirical assessment on P1000 matched somatic +/- germline data\n",
    "Here, we analyze results of running models on empirical data: matched somatic +/- germline data from the P1000 dataset.\n",
    "\n",
    "Prerequisites: \n",
    "- you ran the empirical assessment experiment\n",
    "- the results are saved as a CSV (you can generate this by running the relevant portion of the `notebooks/analysis/make_supplementary_tables.ipynb` notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, \"../..\")  # add project_config to path\n",
    "import project_config\n",
    "\n",
    "# Setup Logging and Configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Hyperparameter definition (inputs / outputs / figure details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file with performance metrics and feature importance paths for each run\n",
    "METRIC_CSV_PATH = \"/mnt/disks/gmiller_data1/pnet-simu-private/results/p1000_empirical_prediction_metrics_per_run_full.csv\"\n",
    "\n",
    "# Define directories for saving results\n",
    "SAVE_RESULTS_NAME = \"p1000_somatic_germline\"\n",
    "# FIGDIR = f\"../../figures/{SAVE_RESULTS_NAME}/\"\n",
    "FIGDIR = project_config.FIGURE_3_DIR\n",
    "RESULTS_DIR = f\"../../results/{SAVE_RESULTS_NAME}/\"\n",
    "\n",
    "# make directory if it doesn't exist\n",
    "os.makedirs(os.path.join(FIGDIR), exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_DIR), exist_ok=True)\n",
    "\n",
    "# Figure parameters\n",
    "figsize_tuple = (2.1, 2)\n",
    "aspect_ratio_value = 1.7\n",
    "cmap_style = \"colorblind\"\n",
    "\n",
    "# NOTE: # ['germline_rare_lof'] equivalent to ['germline_rare_common_lof'] bc there are no common LOF variants\n",
    "\n",
    "# germline lighter version\n",
    "germline_lighter_colorblind_split = {\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_lof_missense': '#0173B2',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_lof_missense': '#DE8F05',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_missense': '#029E73',\n",
    "    'somatic_amp somatic_del somatic_mut germline_common_lof_missense': '#D55E00',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_lof': '#CC78BC',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_lof': '#CA9161', # duplicate of somatic_amp somatic_del somatic_mut germline_rare_lof\n",
    "    'somatic_amp somatic_del somatic_mut': '#949494',\n",
    "    'germline_rare_common_lof_missense': '#018ad6',\n",
    "    'germline_rare_lof_missense': '#f1a934',\n",
    "    'germline_rare_common_missense': '#02be8a',\n",
    "    'germline_common_lof_missense': '#ff7101',\n",
    "    'germline_rare_lof': '#dea7d3',\n",
    "    'germline_rare_common_lof': '#d9b08e' # duplicate of germline_rare_lof\n",
    "}\n",
    "\n",
    "custom_colors = germline_lighter_colorblind_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Fetch WandB Runs and Performance Metrics (grouped by model_type, OR, and control_frequency)\n",
    "Use the WandB API to fetch runs and extract performance metrics grouped by `model_type`, `odds_ratio`, `control_frequency`, and `n_features` for purposes of assessing the 'power curve' of the model. Store the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METRIC_CSV_PATH)\n",
    "\n",
    "\n",
    "print(df.datasets.value_counts())\n",
    "# Filter out unintentional equality (bc no common LOF variants exist), germline_common_lof_missense (=germline_common_missense) and germline_rare_common_lof (= germline_rare_lof)\n",
    "df = df[df[\"datasets\"] != \"somatic_amp somatic_del somatic_mut germline_rare_common_lof\"]\n",
    "\n",
    "# TODO: rename germline_common_lof_missense to germline_common_missense since these are actually the same dataset and do not contain any common LOF variants\n",
    "# TODO: would also need to update the color mapping for the datasets\n",
    "# df = df.replace({\"datasets\": {\"germline_common_lof_missense\": \"germline_common_missense\",\n",
    "#                               \"somatic_amp somatic_del somatic_mut germline_common_lof_missense\": \"somatic_amp somatic_del somatic_mut germline_common_missense\"}})\n",
    "\n",
    "# Average over seeds\n",
    "group_by_cols = [\"model_type\", \"datasets\"]\n",
    "\n",
    "# Add a counts column for each group\n",
    "df_counts = df.groupby(group_by_cols).size().reset_index(name=\"count\")\n",
    "\n",
    "# Average over seeds\n",
    "df_avg = df.groupby(group_by_cols, as_index=False).mean()\n",
    "\n",
    "# Merge counts into df_avg\n",
    "df_avg = df_avg.merge(df_counts, on=group_by_cols)\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prevalence of the positive class (equal to random model's AP or AUPRC)\n",
    "p1000_metastatic_prevalence = pd.read_csv(os.path.join(df.input_data_dir[0], \"y.csv\"), index_col=0)['is_met'].mean()\n",
    "print(f\"Prevalence of 1s (metastatic samples): {p1000_metastatic_prevalence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.datasets.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_ticklabels(labels):\n",
    "    return [f\"{int(l)}\" if float(l).is_integer() else f\"{l:.1f}\" for l in labels]\n",
    "\n",
    "def get_smart_odds_ratio_labels(axessubplot_object):\n",
    "    \"\"\"Expects type <class 'matplotlib.axes._subplots.AxesSubplot'> as input\"\"\"\n",
    "    smart_yticks = smart_ticklabels([float(l.get_text()) for l in axessubplot_object.get_yticklabels()])\n",
    "    return smart_yticks\n",
    "\n",
    "def get_feature_matrix(datasets, features):\n",
    "    \"Build upset-style indicator matrix to accompany box plots\"\n",
    "    logger.debug(\"features:\", features)\n",
    "    for dataset in datasets:\n",
    "        logger.debug(f\"working on {dataset}: {[int(feature in dataset) for feature in features]}\")\n",
    "    \n",
    "    indicator_df = pd.DataFrame({\n",
    "        feature: [int(feature in dataset) for dataset in datasets]\n",
    "        for feature in features\n",
    "    }, index=datasets)\n",
    "\n",
    "    return indicator_df\n",
    "\n",
    "\n",
    "def generate_fake_data(datasets):\n",
    "    np.random.seed(123)\n",
    "    data = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for _ in range(20):\n",
    "            value = np.random.normal(loc=5 + i, scale=1.0)\n",
    "            data.append({\"datasets\": dataset, \"value\": value})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def plot_boxplot(data, x_name=\"datasets\", y_name=\"value\", color_map=None, ax=None, \n",
    "                 no_x_labels=False, showfliers=True, dataset_order=None):\n",
    "    \"\"\"\n",
    "    Plot a seaborn boxplot with optional custom color mapping.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    valid_order = get_valid_group_order(data, dataset_order, group_col=x_name) if dataset_order else None\n",
    "    sns.boxplot(data=data, x=x_name, y=y_name, ax=ax, showfliers=showfliers, \n",
    "                palette=color_map if color_map else None,\n",
    "                order=valid_order\n",
    "    )\n",
    "    if not showfliers:\n",
    "        sns.stripplot(data=data, x=x_name, y=y_name, ax=ax, color=\"black\", jitter=0.2, alpha=0.3,\n",
    "                    order=valid_order\n",
    "                    )\n",
    "    if no_x_labels:\n",
    "        ax.tick_params(axis=\"x\", labelbottom=False)\n",
    "    else:\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def plot_feature_matrix(feature_matrix, x_labels, ax=None, fig_size=(10,2)):\n",
    "    \"\"\"\n",
    "    Plot a binary feature matrix below a boxplot using scatter plot markers.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=fig_size)\n",
    "    \n",
    "    x_positions = list(range(len(x_labels)))\n",
    "    feature_matrix = feature_matrix.loc[x_labels] # ensure order matches\n",
    "\n",
    "    features = feature_matrix.columns\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        y = [len(features) - i - 1] * len(x_labels)  # Flip Y order\n",
    "        values = feature_matrix[feature].tolist()\n",
    "        ax.scatter(\n",
    "            x_positions,\n",
    "            y,\n",
    "            c=values,\n",
    "            cmap=\"Greys\",\n",
    "            vmin=0, vmax=1,   # force full contrast\n",
    "            marker=\"o\",\n",
    "            s=100,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "    \n",
    "    ax.set_yticks(range(len(features)))\n",
    "    ax.set_yticklabels(features[::-1])  # Reverse to match flipped Y\n",
    "    ax.set_xticks([])  # Hide x-axis labels\n",
    "    ax.set_ylim(-0.5, len(features) - 0.5)\n",
    "    ax.set_xlim(-0.5, len(x_labels) - 0.5)\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def plot_boxplot_with_features(data, datasets, feature_matrix, color_map=None, dataset_order=None,\n",
    "                                x_name=\"datasets\", y_name=\"value\", title=\"Boxplot with UpSet-Style Feature Labels\",\n",
    "                                figsize_tuple=(6, 3), fig_ratio=[4,1]):\n",
    "\n",
    "    # valid data order: restrict the ordered list to the groups we actually have in our data; default to datasets\n",
    "    valid_dataset_order = get_valid_group_order(data, dataset_order, group_col=x_name) if dataset_order else datasets\n",
    "\n",
    "    fig, (ax_box, ax_matrix) = plt.subplots(\n",
    "        2, 1, figsize=figsize_tuple, gridspec_kw={\"height_ratios\": fig_ratio}, sharex=True\n",
    "    )\n",
    "\n",
    "    plot_boxplot(data, x_name, y_name, color_map=color_map, ax=ax_box, no_x_labels=True, dataset_order=valid_dataset_order)\n",
    "    ax_box.set_title(title)\n",
    "\n",
    "    ax_matrix = plot_feature_matrix(feature_matrix, x_labels=valid_dataset_order, ax=ax_matrix)\n",
    "\n",
    "    for i in range(len(valid_dataset_order)):\n",
    "        ax_box.axvline(x=i, color='lightgray', linestyle='--', linewidth=1, zorder=0)\n",
    "        ax_matrix.axvline(x=i, color='lightgray', linestyle='--', linewidth=1, zorder=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def get_group_ordered_by_statistic(df, group_col_name, order_col_name='auc', stat=\"median\", ascending=False):\n",
    "    if stat==\"mean\":\n",
    "        # Calculate mean for each dataset\n",
    "        sorted_df = df.groupby(group_col_name)[order_col_name].mean().sort_values(ascending=ascending)\n",
    "    elif stat==\"median\":\n",
    "        # Calculate median for each dataset\n",
    "        sorted_df = df.groupby(group_col_name)[order_col_name].median().sort_values(ascending=ascending)\n",
    "    else:\n",
    "        print(\"TODO: not implemented\")\n",
    "        return\n",
    "    # Order the datasets by descending mean AUC\n",
    "    group_order = sorted_df.index.tolist()\n",
    "    return group_order\n",
    "\n",
    "def get_valid_group_order(df, group_order, group_col=\"datasets\"):\n",
    "    \"\"\"\n",
    "    Return a filtered dataset order that includes only the group_col values present in df.\n",
    "    \"\"\"\n",
    "    available = df[group_col].unique().tolist()\n",
    "    return [d for d in group_order if d in available]\n",
    "\n",
    "\n",
    "def plot_model_comparison_with_features(df, eval_set, feature_matrix, color_map=None, dataset_order=None,\n",
    "                                        figsize_tuple=(8,6), fig_ratio=[6,6,3], shareY=True, \n",
    "                                        title=\"Performance boxplots\", random_performance_value=None, setMinYToRandomPerformance=False):\n",
    "    metric_col = f\"{eval_set}_avg_precision\"\n",
    "    datasets = df[\"datasets\"].unique().tolist()\n",
    "    valid_dataset_order = get_valid_group_order(df, dataset_order, group_col=\"datasets\") if dataset_order else datasets\n",
    "\n",
    "    # Split data\n",
    "    df_pnet = df[df[\"model_type\"] == \"pnet\"]\n",
    "    df_rf = df[df[\"model_type\"] == \"rf\"]\n",
    "\n",
    "    # Set up figure\n",
    "    fig, (ax_pnet, ax_rf, ax_matrix) = plt.subplots(\n",
    "        3, 1,\n",
    "        figsize=figsize_tuple,\n",
    "        gridspec_kw={\"height_ratios\": fig_ratio},\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    # Plot PNet\n",
    "    plot_boxplot(\n",
    "        data=df_pnet,\n",
    "        x_name=\"datasets\",\n",
    "        y_name=metric_col,\n",
    "        color_map=color_map,\n",
    "        ax=ax_pnet,\n",
    "        dataset_order=valid_dataset_order\n",
    "    )\n",
    "    ax_pnet.set_title(\"model = P-NET\")\n",
    "    \n",
    "    # Plot RF\n",
    "    plot_boxplot(\n",
    "        data=df_rf,\n",
    "        x_name=\"datasets\",\n",
    "        y_name=metric_col,\n",
    "        color_map=color_map,\n",
    "        ax=ax_rf,\n",
    "        dataset_order=valid_dataset_order\n",
    "    )\n",
    "    ax_rf.set_title(\"model = Random Forest\")\n",
    "\n",
    "    if shareY:\n",
    "        # Sync y-axis limits between ax_pnet and ax_rf\n",
    "        y_min = min(ax_pnet.get_ylim()[0], ax_rf.get_ylim()[0])\n",
    "        if setMinYToRandomPerformance and random_performance_value is not None:\n",
    "            y_min = min(y_min, random_performance_value-0.03)  # add a little buffer below the random performance line\n",
    "        y_max = max(ax_pnet.get_ylim()[1], ax_rf.get_ylim()[1])\n",
    "        ax_pnet.set_ylim(y_min, y_max)\n",
    "        ax_rf.set_ylim(y_min, y_max)\n",
    "\n",
    "    # add horizontal line to represent random performance\n",
    "    if random_performance_value is not None:\n",
    "        ax_pnet.axhline(y=random_performance_value, linestyle='--', color='black', linewidth=1)\n",
    "        ax_rf.axhline(y=random_performance_value, linestyle='--', color='black', linewidth=1)\n",
    "\n",
    "    # Draw vertical lines for each dataset to draw the eye to the x-axis legend\n",
    "    for i in range(len(datasets)):\n",
    "        ax_pnet.axvline(x=i, color='lightgray', linestyle='--', linewidth=1, zorder=0)\n",
    "        ax_rf.axvline(x=i, color='lightgray', linestyle='--', linewidth=1, zorder=0)\n",
    "        ax_matrix.axvline(x=i, color='lightgray', linestyle='--', linewidth=1, zorder=0)\n",
    "\n",
    "\n",
    "    # Plot shared feature matrix\n",
    "    plot_feature_matrix(feature_matrix, x_labels=valid_dataset_order, ax=ax_matrix)\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Performance boxplot of P-NET and RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all 13 datasets\n",
    "datasets_all = [\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_lof_missense',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_lof_missense',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_missense',\n",
    "    'somatic_amp somatic_del somatic_mut germline_common_lof_missense',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_lof',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_lof',\n",
    "    'somatic_amp somatic_del somatic_mut',\n",
    "    'germline_rare_common_lof_missense',\n",
    "    'germline_rare_common_missense',\n",
    "    'germline_rare_common_lof',\n",
    "    'germline_common_lof_missense',\n",
    "    'germline_rare_lof',\n",
    "    'germline_rare_lof_missense'\n",
    "]\n",
    "\n",
    "features = ['somatic', 'rare', 'common', 'lof', 'missense']\n",
    "feature_matrix = get_feature_matrix(datasets_all, features)\n",
    "data = generate_fake_data(datasets_all) # needs a shared group col w/the feature_matrix, e.g. \"datasets\" and col containing the values you want to plot\n",
    "# plot_boxplot_with_features(data, datasets_all, feature_matrix, color_map=custom_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### P1000 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### boxplots on all dataset combos combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = \"test\" # \"test\" or \"train\"\n",
    "metric_col = f\"{eval_set}_avg_precision\"\n",
    "df_pnet = df[df[\"model_type\"] == \"pnet\"]\n",
    "df_rf = df[df[\"model_type\"] == \"rf\"]\n",
    "datasets = df['datasets'].unique().tolist()\n",
    "\n",
    "# force datasets to follow this order (decreasing median(metric) in P-NET runs)\n",
    "dataset_order = get_group_ordered_by_statistic(df_pnet, group_col_name=\"datasets\", stat=\"median\", \n",
    "                                            order_col_name=metric_col, ascending=False)\n",
    "\n",
    "\n",
    "features = ['somatic', 'rare', 'common', 'lof', 'missense']\n",
    "feature_matrix = get_feature_matrix(datasets, features)\n",
    "\n",
    "p1 = plot_boxplot_with_features(df_pnet, datasets, feature_matrix, color_map=custom_colors, \n",
    "                           y_name=metric_col, x_name=\"datasets\",\n",
    "                           dataset_order=dataset_order,\n",
    "                           title=f\"AUPRC with model = P-NET ({eval_set} set, n=5)\", \n",
    "                           figsize_tuple=(6, 3.2), fig_ratio=[2, 1])\n",
    "p1.savefig(os.path.join(FIGDIR, f\"P1000_auprc_pnet_{eval_set}_set.png\"), format='png', dpi=600)\n",
    "p1.show()\n",
    "\n",
    "p2 = plot_model_comparison_with_features(df, eval_set, feature_matrix, color_map=custom_colors, \n",
    "                                    dataset_order=dataset_order,\n",
    "                                    figsize_tuple=(6,6), fig_ratio=[6, 6, 3], \n",
    "                                    title=f\"AUPRC ({eval_set} set, n=5)\")\n",
    "p2.savefig(os.path.join(FIGDIR, f\"P1000_auprc_model_comparison_{eval_set}_set.png\"), format='png', dpi=600)\n",
    "p2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### boxplots: germline only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_col = f\"{eval_set}_avg_precision\"\n",
    "df_germline_only = df[~df[\"datasets\"].str.contains(\"somatic\")]\n",
    "datasets = df_germline_only['datasets'].unique().tolist()\n",
    "\n",
    "\n",
    "features = ['somatic', 'rare', 'common', 'lof', 'missense']\n",
    "feature_matrix = get_feature_matrix(datasets, features)\n",
    "\n",
    "p3 = plot_model_comparison_with_features(df_germline_only, eval_set, feature_matrix, color_map=custom_colors, \n",
    "                                    dataset_order=dataset_order,\n",
    "                                    figsize_tuple=(5,5), fig_ratio=[5, 5, 3], \n",
    "                                    title=f\"AUPRC on germline datasets ({eval_set} set, n=5)\",\n",
    "                                    random_performance_value=p1000_metastatic_prevalence,\n",
    "                                    setMinYToRandomPerformance=True)\n",
    "p3.savefig(os.path.join(FIGDIR, f\"P1000_auprc_germline_only_model_comparison_{eval_set}_set.png\"), format='png', dpi=600)\n",
    "p3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### boxplot: somatic:germline combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_col = f\"{eval_set}_avg_precision\"\n",
    "df_combos = df[df[\"datasets\"].str.contains(\"somatic\")]\n",
    "datasets = df_combos['datasets'].unique().tolist()\n",
    "\n",
    "\n",
    "features = ['somatic', 'rare', 'common', 'lof', 'missense']\n",
    "feature_matrix = get_feature_matrix(datasets, features)\n",
    "\n",
    "p4 = plot_model_comparison_with_features(df_combos, eval_set, feature_matrix, color_map=custom_colors, \n",
    "                                    dataset_order=dataset_order,\n",
    "                                    figsize_tuple=(5,5), fig_ratio=[5, 5, 3], \n",
    "                                    title=f\"AUPRC on somatic +/- germline datasets ({eval_set} set, n=5)\",\n",
    "                                    random_performance_value=p1000_metastatic_prevalence)\n",
    "p4.savefig(os.path.join(FIGDIR, f\"P1000_auprc_somatic_germline_model_comparison_{eval_set}_set.png\"), format='png', dpi=600)\n",
    "p4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# P-NET feature rankings\n",
    "- top 10 gene features for each \"datasets\" value?\n",
    "- table of top 5 genes, then ellipses, then BRCA2 at appropriate rank\n",
    "    - first: get top 10 genes per model, their rank, absolute imp\n",
    "    - second: get BRCA2 rank, abs imp\n",
    "    - third: select the \"datasets\" values of interest. Proposed: somatic only, one germline only with low BRCA2 rank, som:germ combo equivalent\n",
    "    - finally: will probably need to manually create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_importances(importances_path):\n",
    "    \"\"\"\n",
    "    Load feature importance data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        importances_path (str): Path to the feature importance file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of feature importances.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(importances_path):\n",
    "        raise FileNotFoundError(f\"File not found: {importances_path}\")\n",
    "    logger.debug(f\"Loading feature importances from {importances_path}\")\n",
    "    return pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "\n",
    "\n",
    "def process_importances(imps, response_df):\n",
    "    \"\"\"\n",
    "    Process feature importances by joining with response data and calculating differences between sample classes.\n",
    "    This function computes the mean feature importances for each response class and then calculates the difference between them.\n",
    "    The result is a Series of feature importance differences.\n",
    "\n",
    "    Args:\n",
    "        imps (pd.DataFrame): Feature importance DataFrame.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Processed feature importance differences.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"head of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().head()}\")\n",
    "    logger.debug(f\"shape of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().shape}\")\n",
    "    return imps.join(response_df).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "\n",
    "def load_response_variable(response_path='../../pnet_germline/data/pnet_database/prostate/processed/response_paper.csv'):\n",
    "    # Load the response variable DataFrame\n",
    "    response_df = pd.read_csv(response_path)\n",
    "    response_df.rename(columns={'id': \"Tumor_Sample_Barcode\"}, inplace=True)\n",
    "    response_df.set_index('Tumor_Sample_Barcode', inplace=True)\n",
    "    return response_df\n",
    "\n",
    "def process_feature_importances(df_feature_importance_paths, response_df, importance_path_column='feature_importances_path', group_identifier_column='group_identifier'):\n",
    "    \"\"\"\n",
    "    Process feature importance data for multiple runs and group by dataset.\n",
    "\n",
    "    Args:\n",
    "        df_feature_importance_paths (pd.DataFrame): DataFrame with feature importance paths and dataset info.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Dictionaries of DataFrames for feature importances and ranks grouped by the input datasets used in the model.\n",
    "    \"\"\"\n",
    "    imps_by_key = {}\n",
    "    ranks_by_key = {}\n",
    "\n",
    "    logger.info(f\"Processing feature importances for {df_feature_importance_paths.shape[0]} runs...\")\n",
    "    for _, row in df_feature_importance_paths.iterrows():\n",
    "        try:\n",
    "            # Load feature importances from the specified path\n",
    "            imps = load_feature_importances(row[importance_path_column])\n",
    "            processed_imps = process_importances(imps, response_df)\n",
    "            ranks = processed_imps.abs().rank(ascending=False)\n",
    "\n",
    "            key = row[group_identifier_column]\n",
    "            imps_by_key.setdefault(key, []).append(processed_imps)\n",
    "            ranks_by_key.setdefault(key, []).append(ranks)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    df_imps_by_key = {key: pd.DataFrame(imps_list) for key, imps_list in imps_by_key.items()}\n",
    "    df_ranks_by_key = {key: pd.DataFrame(ranks_list) for key, ranks_list in ranks_by_key.items()}\n",
    "    return df_imps_by_key, df_ranks_by_key\n",
    "\n",
    "\n",
    "\n",
    "def extract_top_features_from_df(df_per_dataset, top_n=10, keep_smallest_n=True, index_label=None):\n",
    "    \"\"\"\n",
    "    Extract the top N features by rank for each dataset.\n",
    "\n",
    "    Args:\n",
    "        df_per_dataset (dict): Dictionary containing feature-level pd DataFrames for each dataset.\n",
    "        top_n (int): Number of top features to extract.\n",
    "        keep_smallest_n (bool): Whether to sort in ascending order (lower rank is better).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the top N features for each dataset.\n",
    "    \"\"\"\n",
    "    top_features_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over the dictionary to calculate top features\n",
    "    for dataset, df in df_per_dataset.items():\n",
    "        # Calculate the mean rank for each feature and select the top N\n",
    "        top_features = df.mean(axis=0).sort_values(ascending=keep_smallest_n)[:top_n]\n",
    "        # Add the top features as a column to the DataFrame\n",
    "        top_features_df[dataset] = top_features.index\n",
    "\n",
    "    # Set the index of the DataFrame to be 1 through top_n\n",
    "    top_features_df.index = range(1, top_n + 1)\n",
    "\n",
    "    if index_label is not None:\n",
    "        top_features_df.index.name = index_label\n",
    "\n",
    "    return top_features_df\n",
    "\n",
    "def build_runwise_gene_importance_rank_df(df, response_df, importance_path_column='gene_importances_path', top_n=10):\n",
    "    \"\"\"\n",
    "    Build a DataFrame summarizing gene importance and rank for each run.\n",
    "\n",
    "    For each run in df, this function:\n",
    "      - Loads the gene importances and computes the class-difference importance vector.\n",
    "      - Computes the absolute rank of each gene (lower rank = more important).\n",
    "      - Restricts to the top_n ranked genes.\n",
    "      - For each of those genes, records run ID, datasets column (as gene_perturbation_group), gene name, rank, and importance.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with one row per run, containing columns for run_id, datasets, and path to gene importances.\n",
    "        response_df (pd.DataFrame): DataFrame with sample responses, indexed by sample name, with column 'response'.\n",
    "        importance_path_column (str): Column name in df with the path to the gene importances CSV.\n",
    "        top_n (int): Number of top-ranked genes to retain per run.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [run_id, gene_perturbation_group, gene, rank, imp], \n",
    "                      where rank and imp are the rank and importance for each gene.\n",
    "    \"\"\"\n",
    "    logger.info(f\"For each of the {df.shape[0]} model runs, extracting top {top_n} ranked genes based on absolute importance\")\n",
    "    records = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        run_id = row.get(\"run_id\") or row.get(\"wandb_run_id\") or idx\n",
    "        datasets = row.get(\"datasets\", \"unknown_dataset\")\n",
    "        logger.debug(f\"Processing run {run_id} with row: {row.to_dict()}\")\n",
    "\n",
    "        try:\n",
    "            imps = load_feature_importances(row[importance_path_column])  # DataFrame: samples x genes\n",
    "            processed_imps = process_importances(imps, response_df)       # Series: gene -> score\n",
    "            ranks = processed_imps.abs().rank(ascending=False)\n",
    "\n",
    "            top_genes = ranks.nsmallest(top_n).index\n",
    "\n",
    "            for gene in top_genes:\n",
    "                records.append({\n",
    "                    \"run_id\": run_id,\n",
    "                    \"datasets\": datasets,\n",
    "                    \"gene\": gene,\n",
    "                    \"rank\": ranks[gene],\n",
    "                    \"importance\": processed_imps[gene],\n",
    "                    \"absolute_importance\": abs(processed_imps[gene]),\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping run {run_id} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "\n",
    "def build_datasetwise_gene_importance_rank_df_old(df, response_df, importance_path_column='gene_importances_path', top_n=10):\n",
    "    \"\"\" Does average-then-rank WARNING\n",
    "    Build a DataFrame summarizing average gene importance and rank across runs, grouped by datasets.\n",
    "\n",
    "    For each dataset group in df, this function:\n",
    "      - Loads and processes gene importances from all runs in that dataset group.\n",
    "      - Averages signed and absolute importances across runs.\n",
    "      - Computes ranks based on mean absolute importance.\n",
    "      - Selects top_n genes per group.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with one row per run, containing columns for run_id, datasets, and gene importance path.\n",
    "        response_df (pd.DataFrame): DataFrame with sample responses, indexed by sample name, with column 'response'.\n",
    "        importance_path_column (str): Column name in df with the path to the gene importances CSV.\n",
    "        top_n (int): Number of top-ranked genes to retain per dataset group.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [gene_perturbation_group, gene, rank, imp, absolute_importance],\n",
    "                      where rank and imp are computed from average values across runs.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Aggregating gene importances across {df.shape[0]} runs grouped by dataset\")\n",
    "\n",
    "    # Store processed importance vectors for each dataset group\n",
    "    grouped_imps = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        run_id = row.get(\"run_id\") or row.get(\"wandb_run_id\") or idx\n",
    "        datasets = row.get(\"datasets\", \"unknown_dataset\")\n",
    "        logger.debug(f\"Processing run {run_id} from dataset group {datasets}\")\n",
    "\n",
    "        try:\n",
    "            imps = load_feature_importances(row[importance_path_column])  # DataFrame: samples x genes\n",
    "            processed = process_importances(imps, response_df)            # Series: gene -> score\n",
    "\n",
    "            if datasets not in grouped_imps:\n",
    "                grouped_imps[datasets] = []\n",
    "\n",
    "            grouped_imps[datasets].append(processed)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping run {run_id} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Aggregate and rank\n",
    "    records = []\n",
    "\n",
    "    for dataset_group, imps_list in grouped_imps.items():\n",
    "        logger.info(f\"Computing averages for dataset group: {dataset_group}\")\n",
    "\n",
    "        # Combine all processed importance Series into a DataFrame (runs x genes)\n",
    "        imp_df = pd.DataFrame(imps_list)\n",
    "        mean_imp = imp_df.mean()\n",
    "        mean_abs_imp = imp_df.abs().mean()\n",
    "\n",
    "        ranks = mean_abs_imp.rank(ascending=False)\n",
    "\n",
    "        top_genes = ranks.nsmallest(top_n).index\n",
    "\n",
    "        for gene in top_genes:\n",
    "            records.append({\n",
    "                \"datasets\": dataset_group,\n",
    "                \"gene\": gene,\n",
    "                \"rank\": ranks[gene],\n",
    "                \"imp\": mean_imp[gene],\n",
    "                \"absolute_importance\": mean_abs_imp[gene],\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def build_datasetwise_gene_importance_rank_df(\n",
    "    df,\n",
    "    response_df,\n",
    "    importance_path_column: str = \"gene_importances_path\",\n",
    "    top_n: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Aggregate gene importances **run-wise first, then average ranks across runs**.\n",
    "\n",
    "    Workflow per dataset group\n",
    "    --------------------------\n",
    "    1. For each run:\n",
    "         • load per-sample importances\n",
    "         • collapse to a single importance score per gene via `process_importances`\n",
    "         • convert those |scores| to ranks **within that run**\n",
    "    2. Stack the per-run rank vectors into a DataFrame (runs × genes) and\n",
    "       take the **mean rank** for every gene.\n",
    "    3. Select the `top_n` genes with the smallest mean rank.\n",
    "       Also report the mean signed importance and mean absolute importance\n",
    "       (useful for interpretation even though ranking is done first).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns:\n",
    "        datasets  : dataset group key\n",
    "        gene      : gene symbol / feature name\n",
    "        rank      : *mean* rank across runs (lower = better)\n",
    "        imp       : mean signed importance across runs\n",
    "        absolute_importance : mean absolute importance across runs\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"Aggregating gene importances across {df.shape[0]} runs \"\n",
    "        f\"grouped by dataset (rank-then-average)\"\n",
    "    )\n",
    "\n",
    "    # Collect one Series-of-scores per run, keyed by dataset group\n",
    "    grouped_scores: dict[str, list[pd.Series]] = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        run_id   = row.get(\"run_id\") or row.get(\"wandb_run_id\") or idx\n",
    "        datasets = row.get(\"datasets\", \"unknown_dataset\")\n",
    "\n",
    "        try:\n",
    "            imps = load_feature_importances(row[importance_path_column])      # samples × genes\n",
    "            scores = process_importances(imps, response_df)                   # gene → score\n",
    "\n",
    "            grouped_scores.setdefault(datasets, []).append(scores)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping run {run_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for dataset_group, score_list in grouped_scores.items():\n",
    "        logger.info(f\"Computing rank-then-average for dataset group: {dataset_group}\")\n",
    "\n",
    "        # (runs × genes) DataFrame of *scores*\n",
    "        score_df = pd.DataFrame(score_list)\n",
    "\n",
    "        # ── 1. Per-run ranking on |score| ───────────────────────────────────────\n",
    "        per_run_ranks = score_df.abs().rank(axis=1, ascending=False)\n",
    "\n",
    "        # ── 2. Average across runs ─────────────────────────────────────────────\n",
    "        mean_rank      = per_run_ranks.mean()    \n",
    "        recomputed_rank   = mean_rank.rank(ascending=True)    # 1 = best, 2 = next…\n",
    "        # smaller = more important\n",
    "        mean_imp       = score_df.mean()                   # signed\n",
    "        mean_abs_imp   = score_df.abs().mean()\n",
    "\n",
    "        # ── 3. Pick top_n genes by mean rank ──────────────────────────────────\n",
    "        top_genes = recomputed_rank.nsmallest(top_n).index\n",
    "\n",
    "        for gene in top_genes:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"datasets\": dataset_group,\n",
    "                    \"gene\": gene,\n",
    "                    \"rank\": float(recomputed_rank[gene]),\n",
    "                    \"importance\": float(mean_imp[gene]),\n",
    "                    \"absolute_importance\": float(mean_abs_imp[gene]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame.from_records(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Get the feature importances and ranks for each dataset combination\")\n",
    "eval_set = \"test\"\n",
    "importance_path = f'{eval_set}_feature_importances_path' # f'{eval_set}_gene_importances_path\n",
    "importance_path = f'{eval_set}_gene_importances_path' # f'{eval_set}_gene_importances_path\n",
    "logger.info(\"Filter to runs where model_type is 'pnet'\")\n",
    "df_filt = df[df[\"model_type\"]==\"pnet\"]\n",
    "response_f = \"/mnt/disks/gmiller_data1/pnet_germline/data/pnet_database/prostate/processed/response_paper.csv\"\n",
    "response_df = load_response_variable(response_f)\n",
    "# df_feature_importance_paths = fetch_feature_importance_paths(runs, GROUP_NAME)\n",
    "\n",
    "# # gene x modality\n",
    "# df_imps_by_key, df_ranks_by_key = process_feature_importances(df_feature_importance_paths)\n",
    "\n",
    "# gene \n",
    "df_imps_by_key, df_ranks_by_key = process_feature_importances(df_filt, response_df, \n",
    "                                                              importance_path_column=importance_path, # \n",
    "                                                              group_identifier_column=\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### TODO: importantly, this gives slightly different top 10 lists compared to my code for getting df_ranks_by_key using process_feature_importances.\n",
    "This is because rank(mean(x)) != mean(rank(x)) in general. If I compute rank and then average (aka mean(rank(x)))\n",
    "\n",
    "Check: I think that I used something similar to build_datasetwise_gene_importance_rank_df for my 1D and 2D simulation analyses. In that case, probably wise to do analogous version here on the empirical results. Similar for aggregate_topk_recovery in 2D simulation: this is rank-then-average.\n",
    "\n",
    "The notion of \"top-K recovery\" is inherently rank-based. You're interested in whether known signal genes are ranked high — not how large their scores are.\n",
    "\n",
    "Averaging importances before ranking (i.e., average-then-rank) would obscure per-run behavior. **A gene might rank high on average because it’s very strong in a few runs, but that doesn’t mean it is consistently recovered in any individual top-K.**\n",
    "\n",
    "| Desired interpretation                                                                        | Recommended approach                                                                                    |\n",
    "| --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n",
    "| “Which genes are *consistently* near the top, run-after-run?”                                 | **Rank-then-average** (your first pipeline). A feature must earn good rank in many runs to stay on top. |\n",
    "| “Which genes have the *largest average magnitude* of effect, even if they occasionally drop?” | **Average-then-rank** (the second pipeline aka build_datasetwise_gene_importance_rank_df). It rewards high overall contribution, not consistency.     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_run_imps_df = build_runwise_gene_importance_rank_df(df_filt, response_df, importance_path_column=importance_path)\n",
    "per_data_combo_imps_df = build_datasetwise_gene_importance_rank_df(df_filt, response_df, importance_path_column=importance_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_data_combo_imps_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_data_combo_imps_df.datasets.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_to_visualize = [\n",
    "     'somatic_amp somatic_del somatic_mut',\n",
    "     'somatic_amp somatic_del somatic_mut germline_rare_lof',\n",
    "     'germline_rare_lof',\n",
    "     'germline_rare_common_lof',\n",
    "     'germline_rare_common_lof_missense',\n",
    "     'somatic_amp somatic_del somatic_mut germline_rare_common_lof_missense'\n",
    "     \n",
    "]\n",
    "per_data_combo_imps_df.style.format({\n",
    "    \"importance\": \"{:.2f}\",\n",
    "    \"absolute_importance\": \"{:.2f}\",\n",
    "    \"rank\": \"{:.2f}\"\n",
    "})\n",
    "\n",
    "per_data_combo_imps_df[per_data_combo_imps_df[\"datasets\"] == dsets_to_visualize[4]].style.format({\n",
    "    \"importance\": \"{:.2f}\",\n",
    "    \"absolute_importance\": \"{:.2f}\",\n",
    "    \"rank\": \"{:.2f}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### suppl figure: top 10 genes by dataset group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features_by_rank = extract_top_features_from_df(df_ranks_by_key, top_n=20, keep_smallest_n=True, index_label=\"rank\")\n",
    "\n",
    "dsets = sorted(top_10_features_by_rank.columns.tolist())[::-1]\n",
    "display(top_10_features_by_rank[dsets])\n",
    "\n",
    "# plot feature matrix to accompany the table\n",
    "features = ['somatic', 'rare', 'common', 'lof', 'missense']\n",
    "feature_matrix = get_feature_matrix(dsets, features)\n",
    "plot_feature_matrix(feature_matrix, dsets, fig_size=(8,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Table: OR, mu0 (class 0 = primary = control frequency) for the top N ranked genes from each major dataset group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proportions(somatic_mut, y, gene_list):\n",
    "    logger.info(\"Group by class and compute mean proportions\")\n",
    "    # Align y to somatic_mut by index\n",
    "    y_aligned = y.loc[somatic_mut.index]\n",
    "    \n",
    "    # Subset to genes of interest\n",
    "    gene_df = somatic_mut[gene_list]\n",
    "    \n",
    "    # Combine with response variable\n",
    "    combined = gene_df.copy()\n",
    "    combined['class'] = y_aligned\n",
    "    \n",
    "    # Group by class and compute proportions (mean of binary values)\n",
    "    proportions = combined.groupby('class').mean().T\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "def calc_OR_from_frequencies(mu1, mu0):\n",
    "    \"\"\"\n",
    "    Calculate odds ratio (OR) from event frequencies in two groups.\n",
    "\n",
    "    Args:\n",
    "        mu1 (float): Event frequency in group 1 (e.g., treatment or case group).\n",
    "        mu0 (float): Event frequency in group 0 (e.g., control group).\n",
    "\n",
    "    Returns:\n",
    "        float: Odds ratio (OR) = (mu1 / (1 - mu1)) / (mu0 / (1 - mu0))\n",
    "    \"\"\"\n",
    "    # Convert to Series if needed\n",
    "    mu1 = pd.Series(mu1)\n",
    "    mu0 = pd.Series(mu0)\n",
    "    # Initialize output with NaN\n",
    "    OR = pd.Series(index=mu1.index, dtype=float)\n",
    "\n",
    "    odds1 = mu1 / (1 - mu1)\n",
    "    odds0 = mu0 / (1 - mu0)\n",
    "    OR = odds1 / odds0\n",
    "    return OR\n",
    "\n",
    "\n",
    "def build_modality_stats(modality_dfs: dict[str, pd.DataFrame],\n",
    "                         y,\n",
    "                         response_col=None) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For every modality DataFrame, compute:\n",
    "        • control (class 0) frequency μ0\n",
    "        • case    (class 1) frequency μ1\n",
    "        • odds ratio OR = (μ1 / (1-μ1)) ÷ (μ0 / (1-μ0))\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stats_by_modality : dict\n",
    "        key   = modality name (e.g. 'somatic_mut')\n",
    "        value = DataFrame with columns\n",
    "                ['gene', 'mu0', 'mu1', 'odds_ratio']\n",
    "    \"\"\"\n",
    "    # ── 1. Ensure `y_series` is a simple Series ────────────────────────────\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        if response_col is not None:\n",
    "            y_series = y[response_col]\n",
    "        elif y.shape[1] == 1:\n",
    "            y_series = y.iloc[:, 0]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"y has multiple columns; please specify `response_col`.\"\n",
    "            )\n",
    "    else:\n",
    "        y_series = y\n",
    "\n",
    "    stats_by_modality = {}\n",
    "\n",
    "    for modality, df_mod in modality_dfs.items():\n",
    "        # all genes in this modality\n",
    "        genes = df_mod.columns.tolist()\n",
    "\n",
    "        # get frequencies by class\n",
    "        props = calculate_proportions(df_mod, y_series, genes)  # index = gene, cols = {0,1}\n",
    "        mu0 = props[0]\n",
    "        mu1 = props[1]\n",
    "\n",
    "        # Compute OR gene-wise\n",
    "        OR = calc_OR_from_frequencies(mu1, mu0)\n",
    "\n",
    "        # Pack into a tidy DataFrame\n",
    "        stats_by_modality[modality] = (\n",
    "            pd.DataFrame({\n",
    "                \"gene\": props.index,\n",
    "                \"mu0\":  mu0.values,\n",
    "                \"mu1\":  mu1.values,\n",
    "                \"odds_ratio\": OR.values,\n",
    "            })\n",
    "            .set_index(\"gene\")          # easier look-ups later\n",
    "        )\n",
    "\n",
    "    return stats_by_modality\n",
    "\n",
    "def annotate_top_features(top_10_features_by_rank: pd.DataFrame,\n",
    "                          dsets_to_visualize: list[str],\n",
    "                          modality_stats: dict[str, pd.DataFrame]\n",
    "                         ) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each dataset group column requested, return a DataFrame\n",
    "    (rank 1–10) augmented with μ0, μ1, and OR looked up from the\n",
    "    modality-specific summaries built in step 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        key   = dataset group name\n",
    "        value = DataFrame with columns\n",
    "                ['feature', 'gene', 'modality', 'mu0', 'mu1', 'odds_ratio']\n",
    "                indexed by rank (1–10)\n",
    "    \"\"\"\n",
    "    annotated = {}\n",
    "\n",
    "    for dset in dsets_to_visualize:\n",
    "        feats = top_10_features_by_rank[dset].dropna()\n",
    "        out_rows = []\n",
    "\n",
    "        for rank, modality in feats.items():\n",
    "            # Expect everything before first _ is the gene. Everything after the first _ is the modality name.\n",
    "            try:\n",
    "                gene, modality = modality.split(\"_\", 1)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"modality name '{modality}' not in gene_modality form\")\n",
    "\n",
    "            # Look up stats\n",
    "            try:\n",
    "                stats_row = modality_stats[modality].loc[gene]\n",
    "            except KeyError:\n",
    "                raise KeyError(f\"Stats not found for {gene} in modality {modality}\")\n",
    "\n",
    "            out_rows.append({\n",
    "                \"modality\": modality,\n",
    "                \"gene\": gene,\n",
    "                \"modality\": modality,\n",
    "                \"mu0\": stats_row.mu0,\n",
    "                \"mu1\": stats_row.mu1,\n",
    "                \"odds_ratio\": stats_row.odds_ratio\n",
    "            })\n",
    "\n",
    "        annotated[dset] = pd.DataFrame(out_rows, index=feats.index)\n",
    "\n",
    "    return annotated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on biggest (least sparse germline dataset) tested\n",
    "dsets_to_visualize = [\n",
    "    'somatic_amp somatic_del somatic_mut',\n",
    "    # 'somatic_amp somatic_del somatic_mut germline_rare_lof',\n",
    "    # 'germline_rare_lof',\n",
    "    'germline_rare_common_lof_missense',\n",
    "    'somatic_amp somatic_del somatic_mut germline_rare_common_lof_missense'\n",
    "    ]\n",
    "\n",
    "# focus on biggest (least sparse germline dataset) tested\n",
    "modalities_to_visualize = [\n",
    "    'somatic_amp',\n",
    "    'somatic_del',\n",
    "    'somatic_mut',\n",
    "    # 'germline_rare_lof',\n",
    "    'germline_rare_common_lof_missense',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_path = f'{eval_set}_feature_importances_path' # f'{eval_set}_gene_importances_path\n",
    "df_feat_imps_by_key, df_feat_ranks_by_key = process_feature_importances(df_filt, response_df, \n",
    "                                                              importance_path_column=importance_path, # \n",
    "                                                              group_identifier_column=\"datasets\")\n",
    "N=10\n",
    "top_N_features_by_rank = extract_top_features_from_df(df_feat_ranks_by_key, top_n=N, keep_smallest_n=True, index_label=\"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Load the response (aka y) DF\")\n",
    "response_f = \"/mnt/disks/gmiller_data1/pnet_germline/data/pnet_database/prostate/processed/response_paper.csv\"\n",
    "response_df = load_response_variable(response_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the top features, extract the OR and control frequency from the associated modality. \n",
    "# E.g., if feature is AR_somatic_mut --> extract OR and control frequency from the somatic_mut DF\n",
    "\n",
    "logger.info(\"1. Build modality-level summaries\")\n",
    "input_data_dir = df.input_data_dir.unique()[0]\n",
    "dset_df_f = [os.path.join(input_data_dir, i+\".csv\") for i in modalities_to_visualize]\n",
    "dset_dfs = [pd.read_csv(p, index_col=0) for p in dset_df_f]\n",
    "\n",
    "named_modality_dfs = dict(zip(modalities_to_visualize, dset_dfs))\n",
    "modality_stats = build_modality_stats(named_modality_dfs, response_df)\n",
    "\n",
    "logger.info(\"2. Annotate each dataset group's top-N table\")\n",
    "annotated_topN = annotate_top_features(top_N_features_by_rank,\n",
    "                                        dsets_to_visualize,\n",
    "                                        modality_stats)\n",
    "for dset in dsets_to_visualize:\n",
    "    annotated_topN[dset].drop(columns='mu1', inplace=True)\n",
    "    # annotated_topN[dset].drop(columns='feature', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Figure S4: top ranked gene tables with OR and control_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Save and display top ranked gene tables with OR and control_freq\")\n",
    "for dset in dsets_to_visualize:\n",
    "    logger.info(f\"\\ndset: {dset}\")\n",
    "\n",
    "    # --- rename mu0 -> control_freq (leave all other cols intact) ------------\n",
    "    df_out = annotated_topN[dset].rename(columns={\"mu0\": \"control_freq\"}).round({\n",
    "    \"control_freq\": 3,\n",
    "    \"odds_ratio\": 2\n",
    "})\n",
    "\n",
    "    # --- save to CSV ---------------------------------------------------------\n",
    "    out_path = os.path.join(\n",
    "        RESULTS_DIR,\n",
    "        f\"{dset}_top_ranked_genes_with_class_freq_and_odds_ratio.csv\"\n",
    "    )\n",
    "    logger.info(f\"Saving to {out_path}\")\n",
    "    df_out.to_csv(out_path, index_label=\"rank\")\n",
    "\n",
    "    # --- pretty-print --------------------------------------------------------\n",
    "    display(\n",
    "        df_out.style.format({\n",
    "            \"control_freq\": \"{:.3f}\",\n",
    "            \"mu1\":          \"{:.3f}\",\n",
    "            \"odds_ratio\":   \"{:.2f}\"\n",
    "        }).background_gradient(\n",
    "    subset=[\"control_freq\"], cmap=\"Blues\"\n",
    ").background_gradient(\n",
    "    subset=[\"odds_ratio\"], cmap=\"OrRd\").set_caption(f\"Top ranked gene for data combination: {dset}\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Figure 4: BRCA2 rank trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a single gene, extract importances and ranks from the processed dicts for each dataset combination\n",
    "def get_single_gene_info(df_imps_by_key, df_ranks_by_key, gene_name, group_identifier_col=\"group_identifier\"):\n",
    "    records = []\n",
    "\n",
    "    for key, df_imp in df_imps_by_key.items():\n",
    "        mean_imp = df_imp.mean()\n",
    "        mean_rank = df_ranks_by_key[key].mean()\n",
    "        recomputed_mean   = mean_rank.rank(ascending=True)    # 1 = best, 2 = next…\n",
    "\n",
    "        \n",
    "        matches = mean_imp[mean_imp.index.str.startswith(gene_name)]\n",
    "\n",
    "        for match_name in matches.index:\n",
    "            records.append({\n",
    "                group_identifier_col: key,\n",
    "                \"gene_name\": gene_name,\n",
    "                \"gene_name_group\": match_name,\n",
    "                \"importance\": mean_imp[match_name],\n",
    "                \"absolute_importance\": abs(mean_imp[match_name]),\n",
    "                \"rank\": recomputed_mean[match_name]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "# Merge AR stats into the plotting dataframe\n",
    "df_BRCA2 = get_single_gene_info(df_imps_by_key, df_ranks_by_key, gene_name = \"BRCA2\", group_identifier_col=\"datasets\")\n",
    "df_plot = df.merge(df_BRCA2, on=\"datasets\", how=\"left\")\n",
    "df_plot_avg = df_plot.groupby(\"datasets\", as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BRCA2[df_BRCA2[\"datasets\"] == \"somatic_amp somatic_del somatic_mut\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_BRCA2.sort_values(by=\"rank\", ascending=True)\n",
    "# df_BRCA2[(df_BRCA2[\"gene_name_group\"].str.contains(\"germline\")) | (df_BRCA2[\"datasets\"] == \"somatic_amp somatic_del somatic_mut\")].sort_values(by=[\"absolute_importance\", \"rank\"], ascending=[False, True])[[\"datasets\", \"gene_name_group\", \"rank\", \"absolute_importance\"]].style.format({\n",
    "#     \"importance\": \"{:.2f}\",\n",
    "#     \"absolute_importance\": \"{:.4f}\",\n",
    "#     \"rank\": \"{:.2f}\"\n",
    "# })\n",
    "\n",
    "logger.info(\"Any combo of germline only that has rare_lof ranks BRCA2 highly\")\n",
    "df_BRCA2.sort_values(by=[\"rank\", \"absolute_importance\"], ascending=[True, False])[[\"datasets\", \"gene_name_group\", \"rank\", \"absolute_importance\", \"importance\"]].style.format({\n",
    "    \"importance\": \"{:.4f}\",\n",
    "    \"absolute_importance\": \"{:.4f}\",\n",
    "    \"rank\": \"{:.2f}\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
