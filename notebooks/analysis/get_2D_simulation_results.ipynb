{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Generate figures for the joint simulation experiment\n",
    "Here, we analyze results of running models on simulated datasets with a combination of linear and nonlinear signal.\n",
    "\n",
    "Prerequisites: \n",
    "- you ran the joint simulation experiment\n",
    "- the results are saved as a CSV (you can generate this by running the relevant portion of the `notebooks/analysis/make_supplementary_tables.ipynb` notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import ast\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, \"../..\")  # add project_config to path\n",
    "import project_config\n",
    "\n",
    "# Setup Logging and Configuration\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s [%(name)s] %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup Logging and Configuration\n",
    "Set up logging and define configurations such as WandB project name, sweep ID, and directories for saving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to file with performance metrics and feature importance paths for each run\n",
    "METRIC_CSV_PATH = project_config.RESULT_DIR / \"joint_simulation_prediction_metrics_per_run_full.csv\"\n",
    "\n",
    "# # Define directories for saving results\n",
    "FIGDIR = project_config.FIGURE_1_DIR\n",
    "RESULTS_DIR = \"../../results/joint_simulation_results/\"\n",
    "# make directory if it doesn't exist\n",
    "os.makedirs(os.path.join(FIGDIR), exist_ok=True)\n",
    "\n",
    "# Figure parameters\n",
    "figsize_tuple = (2.4, 2.1)\n",
    "aspect_ratio_value = 0.5\n",
    "cmap_style = \"viridis\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Fetch WandB Runs and Performance Metrics (grouped by model_type, OR, and sigma)\n",
    "Use the WandB API to fetch runs and extract performance metrics grouped by `model_type`, `odds_ratio`, `sigma`, and `n_samples` for purposes of assessing the 'power curve' of the model. Store the results in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METRIC_CSV_PATH, converters={\"deltaMuGenes\": ast.literal_eval,\n",
    "                                              \"mod0_genes\": ast.literal_eval,\n",
    "                                              \"mod1_genes\": ast.literal_eval,\n",
    "                                              \"datasets\": ast.literal_eval,\n",
    "                                              })  \n",
    "\n",
    "# Add a counts column for each group\n",
    "group_cols = [\"model_type\", \"n_samples\", \"odds_ratio\", \"sigma\", \"sample_binary\"]\n",
    "df_counts = df.groupby(group_cols).size().reset_index(name=\"count\")\n",
    "\n",
    "# filter to only model_type == \"pnet\"\n",
    "df = df[df[\"model_type\"] == \"pnet\"].reset_index(drop=True)\n",
    "\n",
    "# in \"save_dir\" column, replace \"../../results/\" with \"/mnt/disks/gmiller_data1/pnet/results\"\n",
    "df[\"save_dir\"] = df[\"save_dir\"].str.replace(\"../../results/\", \"/mnt/disks/gmiller_data1/pnet/results/\", regex=False)\n",
    "\n",
    "# Add a unique group identifer column by joining together the unique identifiers: [\"model_type\", \"n_features\", \"odds_ratio\", \"control_frequency\", \"sample_binary\"]\n",
    "df[\"group_identifier\"] = df.apply(\n",
    "    lambda row: f\"OR-{row['odds_ratio']}_sigma-{row['sigma']}_nSamples-{row['n_samples']}_sampleBinary-{row['sample_binary']}\",\n",
    "    axis=1)\n",
    "\n",
    "# Create df_avg by averaging over runs with the same group identifiers\n",
    "df_avg = df.groupby(group_cols, as_index=False).mean()\n",
    "df_avg = df_avg.merge(df_counts, on=group_cols)\n",
    "\n",
    "# Create df_stdev by calculating the standard deviation over runs with the same group identifiers\n",
    "df_stdev = df.groupby(group_cols, as_index=False).std()\n",
    "df_stdev = df_stdev.merge(df_counts, on=group_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'run_id' in df.columns\n",
    "'run_id' in df_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_ticklabels(labels):\n",
    "    return [f\"{int(l)}\" if float(l).is_integer() else f\"{l:.1f}\" for l in labels]\n",
    "\n",
    "def get_smart_odds_ratio_labels(axessubplot_object):\n",
    "    \"\"\"Expects type <class 'matplotlib.axes._subplots.AxesSubplot'> as input\"\"\"\n",
    "    smart_yticks = smart_ticklabels([float(l.get_text()) for l in axessubplot_object.get_yticklabels()])\n",
    "    return smart_yticks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 3 different results/subresults\n",
    "Here we make performance heatmaps for three different subsets of the data:\n",
    "\n",
    "- (A) sample_binary = True, n_samples=1000\n",
    "- (B) sample_binary = False, n_samples=1000\n",
    "- (C) sample_binary = False, n_samples=10000\n",
    "\n",
    "Plot these side-by-side in one figure: \"Mean Average Precision under varied joint sampling strategies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best: one 2x2 heatmap\n",
    "def heatmap_no_counts(df, title, ax):\n",
    "    precision = df.pivot(index=\"odds_ratio\", columns=\"sigma\", values=f\"{eval_set}_avg_precision\")\n",
    "    sns.heatmap(\n",
    "        precision, cmap=cmap_style, annot=True, fmt=\".2f\",\n",
    "        cbar=False, ax=ax\n",
    "    )\n",
    "    ax.set_aspect(aspect_ratio_value, adjustable='box')\n",
    "    ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"sigma\")\n",
    "    ax.set_ylabel(\"odds ratio\")\n",
    "\n",
    "eval_set = \"test\"\n",
    "fig, axes = plt.subplots(2, 2, figsize=(figsize_tuple[0]*2, figsize_tuple[1]*2))\n",
    "\n",
    "# Row 0: N = 1k\n",
    "heatmap_no_counts(\n",
    "    df_avg[(df_avg[\"sample_binary\"] == True) & (df_avg[\"n_samples\"] == 1000)],\n",
    "    \"Binary, N=1k\",\n",
    "    ax=axes[0, 0]\n",
    ")\n",
    "\n",
    "heatmap_no_counts(\n",
    "    df_avg[(df_avg[\"sample_binary\"] == False) & (df_avg[\"n_samples\"] == 1000)],\n",
    "    \"Continuous, N=1k\",\n",
    "    ax=axes[0, 1]\n",
    ")\n",
    "\n",
    "# Row 1: N = 10k\n",
    "heatmap_no_counts(\n",
    "    df_avg[(df_avg[\"sample_binary\"] == True) & (df_avg[\"n_samples\"] == 10000)],\n",
    "    \"Binary, N=10k\",\n",
    "    ax=axes[1, 0]\n",
    ")\n",
    "\n",
    "heatmap_no_counts(\n",
    "    df_avg[(df_avg[\"sample_binary\"] == False) & (df_avg[\"n_samples\"] == 10000)],\n",
    "    \"Continuous, N=10k\",\n",
    "    ax=axes[1, 1]\n",
    ")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "axes[1,1].set_ylabel(\"\")\n",
    "axes[0,1].set_yticklabels(\"\")\n",
    "axes[1,1].set_yticklabels(\"\")\n",
    "\n",
    "fig.suptitle(f\"AUPRC on all simulated datasets ({eval_set} set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGDIR, f\"2D_simulations_heatmap_auprc_all_simulated_datasets_{eval_set}.png\"), format='png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best: one 2x2 heatmap\n",
    "def heatmap_no_counts(df, title, ax):\n",
    "    precision = df.pivot(index=\"odds_ratio\", columns=\"sigma\", values=f\"{eval_set}_avg_precision\")\n",
    "    sns.heatmap(\n",
    "        precision, cmap=cmap_style, annot=True, fmt=\".2f\",\n",
    "        cbar=False, ax=ax\n",
    "    )\n",
    "    ax.set_aspect(aspect_ratio_value, adjustable='box')\n",
    "    ax.set_yticklabels(get_smart_odds_ratio_labels(ax))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"sigma\")\n",
    "    ax.set_ylabel(\"odds ratio\")\n",
    "\n",
    "eval_set = \"test\"\n",
    "fig, axes = plt.subplots(2, 2, figsize=(figsize_tuple[0]*2, figsize_tuple[1]*2))\n",
    "\n",
    "# Row 0: N = 1k\n",
    "heatmap_no_counts(\n",
    "    df_stdev[(df_stdev[\"sample_binary\"] == True) & (df_stdev[\"n_samples\"] == 1000)],\n",
    "    \"Binary, N=1k\",\n",
    "    ax=axes[0, 0]\n",
    ")\n",
    "\n",
    "heatmap_no_counts(\n",
    "    df_stdev[(df_stdev[\"sample_binary\"] == False) & (df_stdev[\"n_samples\"] == 1000)],\n",
    "    \"Continuous, N=1k\",\n",
    "    ax=axes[0, 1]\n",
    ")\n",
    "\n",
    "# Row 1: N = 10k\n",
    "heatmap_no_counts(\n",
    "    df_stdev[(df_stdev[\"sample_binary\"] == True) & (df_stdev[\"n_samples\"] == 10000)],\n",
    "    \"Binary, N=10k\",\n",
    "    ax=axes[1, 0]\n",
    ")\n",
    "\n",
    "heatmap_no_counts(\n",
    "    df_stdev[(df_stdev[\"sample_binary\"] == False) & (df_stdev[\"n_samples\"] == 10000)],\n",
    "    \"Continuous, N=10k\",\n",
    "    ax=axes[1, 1]\n",
    ")\n",
    "axes[0,1].set_ylabel(\"\")\n",
    "axes[1,1].set_ylabel(\"\")\n",
    "axes[0,1].set_yticklabels(\"\")\n",
    "axes[1,1].set_yticklabels(\"\")\n",
    "\n",
    "fig.suptitle(f\"Std dev of AUPRC on all simulated datasets ({eval_set} set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGDIR, f\"2D_simulations_heatmap_auprc_stddev_all_simulated_datasets_{eval_set}.png\"), format='png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## scratch: WIP get feat imps\n",
    "\n",
    "Goal: heatmap-style plot with the mean/median rank (importance) of the:\n",
    "\n",
    "- deltaMu genes\n",
    "- genes only in corr module\n",
    "- genes in both corr module that also have deltaMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Functions to extract gene imp and rank ##########\n",
    "def load_feature_importances(importances_path):\n",
    "    \"\"\"\n",
    "    Load feature importance data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        importances_path (str): Path to the feature importance file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of feature importances.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(importances_path):\n",
    "        raise FileNotFoundError(f\"File not found: {importances_path}\")\n",
    "    logger.debug(f\"Loading feature importances from {importances_path}\")\n",
    "    imps = pd.read_csv(importances_path).set_index('Unnamed: 0')\n",
    "    logger.debug(f\"Loaded imps with shape {imps.shape}\")\n",
    "    return imps\n",
    "\n",
    "\n",
    "def process_importances(imps, response_df):\n",
    "    \"\"\"\n",
    "    Process feature importances by joining with response data and calculating differences between sample classes.\n",
    "    This function computes the mean feature importances for each response class and then calculates the difference between them.\n",
    "    The result is a Series of feature importance differences.\n",
    "\n",
    "    Args:\n",
    "        imps (pd.DataFrame): Feature importance DataFrame.\n",
    "        response_df (pd.DataFrame): Response variable DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Processed feature importance differences.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"head of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().head()}\")\n",
    "    logger.debug(f\"shape of imps.join(response_df).groupby('response').mean(): {imps.join(response_df).groupby('response').mean().shape}\")\n",
    "    return imps.join(response_df).groupby('response').mean().diff(axis=0).iloc[1]\n",
    "\n",
    "\n",
    "def build_runwise_gene_importance_rank_df(df, importance_path_column='gene_importances_path'):\n",
    "    \"\"\"\n",
    "    Build a DataFrame summarizing gene importance and rank for each run and gene set.\n",
    "\n",
    "    For each run in df, this function:\n",
    "      - Loads the gene importances and computes the class-difference importance vector.\n",
    "      - Computes the absolute rank of each gene (lower rank = more important).\n",
    "      - Restricts to genes in each of three sets: \"deltaMuGenes\", \"mod1_genes\", \"mod0_genes\".\n",
    "      - For each gene in each set, records the run ID, gene set, gene name, mean rank, and mean importance.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with one row per run, containing columns for run_id, gene sets, and path to gene importances.\n",
    "        response_df (pd.DataFrame): DataFrame with sample responses, indexed by sample name, with column 'response'.\n",
    "        importance_path_column (str): Column name in df with the path to the gene importances CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [run_id, gene_perturbation_group, gene, rank, imp], where rank and imp are the mean rank and mean importance for each gene in each run and gene set.\n",
    "    \"\"\"\n",
    "    logger.info(f\"For each of the {df.shape[0]} model runs, we are getting gene importance and rank for each gene set\")\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Use index as run_id if no explicit run_id column\n",
    "        run_id = row.get(\"run_id\") or row.get(\"wandb_run_id\") or idx\n",
    "        logger.debug(f\"Processing run {run_id} with row: {row.to_dict()}\")\n",
    "\n",
    "        logger.debug(\"Parse gene sets\")\n",
    "        deltaMuGenes = set(row[\"deltaMuGenes\"])\n",
    "        mod1_genes = set(row[\"mod1_genes\"])\n",
    "        mod0_genes = set(row[\"mod0_genes\"])\n",
    "        gene_perturbation_groups = {\n",
    "            \"deltaMuGenes\": deltaMuGenes,\n",
    "            \"mod1_genes\": mod1_genes,\n",
    "            \"mod0_genes\": mod0_genes,\n",
    "        }\n",
    "        logger.debug(f\"genes in both deltaMuGenes and mod1_genes: {deltaMuGenes.intersection(mod1_genes)}\")\n",
    "        # raise error if any of the gene sets are empty\n",
    "        for group_name, genes in gene_perturbation_groups.items():\n",
    "            if len(genes) == 0:\n",
    "                raise ValueError(f\"Gene set '{group_name}' is empty for run {run_id}, cannot proceed.\")\n",
    "            else:\n",
    "                logger.debug(f\"Gene set '{group_name}' has {len(genes)} genes: {genes}\")\n",
    "\n",
    "        logger.debug(f\"Construct response_df from the number of samples, knowing that we had even split (first half are class 1)\")\n",
    "        n1 = row.get(\"num_class1_samples\")\n",
    "        n0 = row.get(\"num_class0_samples\")\n",
    "        response_df = np.concatenate([np.ones(n1), np.zeros(n0)])\n",
    "        response_df = pd.DataFrame(response_df.astype(int), index=[f\"Sample_{i}\" for i in range(n1+n0)], columns=[\"response\"])\n",
    "        logger.debug(f\"constructed response_df with shape {response_df.shape}\")\n",
    "\n",
    "        try:\n",
    "            logger.debug(\"Loading importances\")\n",
    "            imps = load_feature_importances(row[importance_path_column])\n",
    "            logger.debug(\"Processing importances\")\n",
    "            processed_imps = process_importances(imps, response_df)\n",
    "            # processed_imps: pd.Series, index=gene names\n",
    "            logger.debug(\"Ranks are calculated from the absolute(importance)\")\n",
    "            ranks = processed_imps.abs().rank(ascending=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Skipping run {run_id} due to error: {e}. \\nCheck: does a file exist at {row[importance_path_column]}?\")\n",
    "            continue        \n",
    "\n",
    "        logger.debug(\"For each gene set, collect mean importance and rank for each gene\")\n",
    "        for gene_perturbation_group_name, gene_perturbation_group in gene_perturbation_groups.items():\n",
    "            for gene in gene_perturbation_group:\n",
    "                if gene in processed_imps.index and gene in ranks.index:\n",
    "                    records.append({\n",
    "                        \"run_id\": run_id,\n",
    "                        \"gene_perturbation_group\": gene_perturbation_group_name,\n",
    "                        \"gene\": gene,\n",
    "                        \"rank\": ranks[gene],\n",
    "                        \"imp\": processed_imps[gene],                        \n",
    "                    })\n",
    "                else:\n",
    "                    raise ValueError(f\"Gene {gene} not found in importances for run {run_id}, skipping. Processed imps index: {processed_imps.index.tolist()}, ranks index: {ranks.index.tolist()}\")\n",
    "\n",
    "    final_df = pd.DataFrame(records)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "########## Functions to plot top-k recovery ##########\n",
    "def filter_runs(df, sample_binary=False, n_samples=10000, model_type=\"pnet\"):\n",
    "    \"\"\"Filter runs for a given sample_binary, n_samples, and model_type.\"\"\"\n",
    "    return df[\n",
    "        (df[\"sample_binary\"] == sample_binary) &\n",
    "        (df[\"n_samples\"] == n_samples) &\n",
    "        (df[\"model_type\"] == model_type)\n",
    "    ].copy()\n",
    "\n",
    "def get_group_identifier(row):\n",
    "    \"\"\"Create a group identifier string for a run.\"\"\"\n",
    "    return f\"OR-{row['odds_ratio']}_sigma-{row['sigma']}_nSamples-{row['n_samples']}_sampleBinary-{row['sample_binary']}\"\n",
    "\n",
    "def assign_group_identifier(df):\n",
    "    \"\"\"Assign group_identifier column to a DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"group_identifier\"] = df.apply(get_group_identifier, axis=1)\n",
    "    return df\n",
    "\n",
    "def get_gene_sets(row):\n",
    "    \"\"\"Return dict of gene sets for a run.\"\"\"\n",
    "    delta = set(row[\"deltaMuGenes\"])\n",
    "    mod0 = set(row[\"mod0_genes\"])\n",
    "    mod1 = set(row[\"mod1_genes\"])\n",
    "    corr = mod0 | mod1 # set union\n",
    "    only_linear = delta - corr\n",
    "    only_nonlinear = corr - delta\n",
    "    both = delta & corr\n",
    "    return {\n",
    "        \"only linear\": only_linear,\n",
    "        \"only nonlinear\": only_nonlinear,\n",
    "        \"both\": both\n",
    "    }\n",
    "\n",
    "def add_gene_set_column(per_run_imps_df, df):\n",
    "    \"\"\"Add a 'gene_set_type' column to per_run_imps_df: only linear, only nonlinear, both.\"\"\"\n",
    "    # Build a mapping from run_id to gene sets\n",
    "    runid_to_sets = {}\n",
    "    for _, row in df.iterrows():\n",
    "        runid_to_sets[row[\"run_id\"]] = get_gene_sets(row)\n",
    "    # Assign gene_set_type for each row in per_run_imps_df\n",
    "    def which_set(row):\n",
    "        sets = runid_to_sets.get(row[\"run_id\"], {})\n",
    "        for set_name, genes in sets.items():\n",
    "            if row[\"gene\"] in genes:\n",
    "                return set_name\n",
    "        return None\n",
    "    per_run_imps_df = per_run_imps_df.copy()\n",
    "    per_run_imps_df[\"gene_set_type\"] = per_run_imps_df.apply(which_set, axis=1)\n",
    "    return per_run_imps_df\n",
    "\n",
    "def aggregate_heatmap(per_run_imps_df, df, value_col, gene_set_type):\n",
    "    \"\"\"\n",
    "    Aggregate median value_col (rank or imp) for each group_identifier, odds_ratio, sigma, and gene_set_type.\n",
    "    Returns a DataFrame with odds_ratio as rows, sigma as columns.\n",
    "    \"\"\"\n",
    "    # Merge group_identifier and odds_ratio/sigma into per_run_imps_df\n",
    "    meta = df[[\"run_id\", \"group_identifier\", \"odds_ratio\", \"sigma\"]]\n",
    "    merged = per_run_imps_df.merge(meta, on=\"run_id\")\n",
    "    # Filter for the gene_set_type\n",
    "    merged = merged[merged[\"gene_set_type\"] == gene_set_type]\n",
    "    # Group by group_identifier, odds_ratio, sigma, then aggregate median across genes and runs\n",
    "    agg = merged.groupby([\"odds_ratio\", \"sigma\"])[value_col].median().unstack()\n",
    "    return agg\n",
    "\n",
    "\n",
    "def aggregate_topk_recovery(per_run_imps_df, df, gene_set_type):\n",
    "    \"\"\"\n",
    "    For each run, compute the fraction of signal genes (of this gene_set_type) recovered in the top K genes,\n",
    "    where K = total number of signal genes for that run (across all three sets).\n",
    "    Returns a DataFrame with odds_ratio as rows, sigma as columns, values are median top-K recovery across runs.\n",
    "    \"\"\"\n",
    "    # Merge group_identifier and odds_ratio/sigma into per_run_imps_df\n",
    "    meta = df[[\"run_id\", \"group_identifier\", \"odds_ratio\", \"sigma\", \"deltaMuGenes\", \"mod0_genes\", \"mod1_genes\"]]\n",
    "    merged = per_run_imps_df.merge(meta, on=\"run_id\")\n",
    "    records = []\n",
    "    for run_id, group in merged.groupby(\"run_id\"):\n",
    "        # Get all signal genes for this run\n",
    "        row = group.iloc[0]\n",
    "        delta = set(row[\"deltaMuGenes\"])\n",
    "        mod0 = set(row[\"mod0_genes\"])\n",
    "        mod1 = set(row[\"mod1_genes\"])\n",
    "        only_linear = delta - (mod0 | mod1)\n",
    "        only_nonlinear = (mod0 | mod1) - delta\n",
    "        both = delta & (mod0 | mod1)\n",
    "        all_signal_genes = only_linear | only_nonlinear | both\n",
    "        K = len(all_signal_genes)\n",
    "        if K == 0:\n",
    "            continue\n",
    "        # Get the genes of the current gene_set_type for this run\n",
    "        if gene_set_type == \"only linear\":\n",
    "            signal_genes = only_linear\n",
    "        elif gene_set_type == \"only nonlinear\":\n",
    "            signal_genes = only_nonlinear\n",
    "        elif gene_set_type == \"both\":\n",
    "            signal_genes = both\n",
    "        else:\n",
    "            continue\n",
    "        if len(signal_genes) == 0:\n",
    "            continue\n",
    "        # Get all genes and their ranks for this run\n",
    "        run_genes = group[[\"gene\", \"rank\"]].drop_duplicates().set_index(\"gene\")[\"rank\"]\n",
    "        # For each gene in the set, check if its rank <= K\n",
    "        n_in_topk = sum(run_genes.get(gene, K+1) <= K for gene in signal_genes)\n",
    "        # Compute recovery: fraction of signal genes in top K\n",
    "        recovery = n_in_topk / len(signal_genes)\n",
    "        records.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"gene_set_type\": gene_set_type,\n",
    "            \"odds_ratio\": row[\"odds_ratio\"],\n",
    "            \"sigma\": row[\"sigma\"],\n",
    "            \"topk_recovery\": recovery\n",
    "        })\n",
    "    # Aggregate by odds_ratio, sigma (median across runs)\n",
    "    df = pd.DataFrame(records)\n",
    "    heatmap_df = df.groupby([\"odds_ratio\", \"sigma\"])[\"topk_recovery\"].median().unstack()\n",
    "    stdev_df = df.groupby([\"odds_ratio\", \"sigma\"])[\"topk_recovery\"].std().unstack()\n",
    "    return heatmap_df, stdev_df, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Get run-level rank and importance information for all the relevant gene sets\n",
    "Each row in df_feature_importance_paths corresponds to a run. I would like my final output to be a DF with columns [run_id, gene_set, gene, rank, imp]. The run_id is the identifer for a given run (wandb run id). The gene set is one of \"deltaMuGenes\", \"mod1_genes\", \"mod0_genes\". The rank is the mean rank of the gene across all samples in that run. The imp is the mean importance of the gene across all samples in that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = \"test\"\n",
    "per_run_imps_df = build_runwise_gene_importance_rank_df(df, importance_path_column=f'{eval_set}_gene_importances_path')\n",
    "per_run_imps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Build heatmap of top-k recovery\n",
    "switch metric to top-k recovery/top-k accuracy (not median rank). \"How many of the true signal genes did my model recover among the top K?\"\n",
    "Also possible to use recall at K (same as accuracy but plotted as a curve for varied K) or Precision at K (\"how 'pure' is my top K list?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN LOGIC ---\n",
    "\n",
    "# 1. Filter runs and per_run_imps_df\n",
    "# df_filt = filter_runs(df, sample_binary=False, n_samples=10000) # continuous 10k\n",
    "\n",
    "df_filts = {\n",
    "    \"Binary, N=1k\": filter_runs(df, sample_binary=True, n_samples=1000),\n",
    "    \"Binary, N=10k\": filter_runs(df, sample_binary=True, n_samples=10000),\n",
    "    \"Continuous, N=1k\": filter_runs(df, sample_binary=False, n_samples=1000),\n",
    "    \"Continuous, N=10k\": filter_runs(df, sample_binary=False, n_samples=10000),\n",
    "    \n",
    "}\n",
    "\n",
    "for df_filt_name, df_filt in df_filts.items():\n",
    "    if df_filt.shape[0] == 0:\n",
    "        logger.info(f\"{df_filt_name} had nothing left in the filtered runs\")\n",
    "        continue # skip this figure\n",
    "    run_ids = set(df_filt[\"run_id\"])\n",
    "    per_run_filt = per_run_imps_df[per_run_imps_df[\"run_id\"].isin(run_ids)].copy()\n",
    "\n",
    "    # 2. Assign group_identifier to both DFs\n",
    "    df_filt = assign_group_identifier(df_filt)\n",
    "\n",
    "    # 3. Add gene_set_type column to per_run_filt\n",
    "    per_run_filt = add_gene_set_column(per_run_filt, df_filt)\n",
    "    # drop duplicates in per_run_filt based on run_id, gene, and gene_set_type, because only want to count each gene once per run (even if it occurs in multiple gene set categories)\n",
    "    per_run_filt = per_run_filt.drop_duplicates(subset=[\"run_id\", \"gene\", \"gene_set_type\"])\n",
    "\n",
    "    # 4. Aggregate for each gene set and value type (recovery heatmap)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(figsize_tuple[0]*3, figsize_tuple[1]), sharey=True)\n",
    "    for col_idx, gene_set in enumerate([\"only linear\", \"only nonlinear\", \"both\"]):\n",
    "        heatmap_df, _, _ = aggregate_topk_recovery(per_run_filt, df_filt, gene_set)\n",
    "        ax = axes[col_idx]\n",
    "        sns.heatmap(heatmap_df, annot=True, cmap=cmap_style, ax=ax, fmt=\".2f\", vmin=0, vmax=1, cbar=False)\n",
    "        # ax.set_title(f\"Top-K Recovery: {gene_set} ({eval_set} set)\")\n",
    "        ax.set_aspect(aspect_ratio_value)\n",
    "        ax.set_title(f\"{gene_set}\")\n",
    "        ax.set_xlabel(\"sigma\")\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(\"odds ratio\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "    axes[0].set_yticklabels(get_smart_odds_ratio_labels(axes[0]))\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Top-K recovery of perturbed gene sets ({df_filt_name}, {eval_set} set)\")\n",
    "    plt.savefig(os.path.join(FIGDIR, f\"2D_simulations_heatmap_topk_recovery_{df_filt_name.replace(',', '').replace(' ', '_')}_{eval_set}.png\"), format='png', dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    # 5. Std dev heatmap\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(figsize_tuple[0]*3, figsize_tuple[1]), sharey=True)\n",
    "    for col_idx, gene_set in enumerate([\"only linear\", \"only nonlinear\", \"both\"]):\n",
    "        _, stdev_df, _ = aggregate_topk_recovery(per_run_filt, df_filt, gene_set)\n",
    "        ax = axes[col_idx]\n",
    "        sns.heatmap(stdev_df, annot=True, cmap=cmap_style, ax=ax, fmt=\".2f\", vmin=0, vmax=1, cbar=False)\n",
    "        ax.set_aspect(aspect_ratio_value)\n",
    "        ax.set_title(f\"{gene_set}\")\n",
    "        ax.set_xlabel(\"sigma\")\n",
    "        if col_idx == 0:\n",
    "            ax.set_ylabel(\"odds ratio\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "    axes[0].set_yticklabels(get_smart_odds_ratio_labels(axes[0]))\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Std dev of top-K Recovery ({df_filt_name}, {eval_set} set)\")\n",
    "    plt.savefig(os.path.join(FIGDIR, f\"2D_simulations_heatmap_topk_recovery_stddev_{df_filt_name.replace(',', '').replace(' ', '_')}_{eval_set}.png\"), format='png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, tmp1 = aggregate_topk_recovery(per_run_filt, df_filt, \"both\")\n",
    "_, _, tmp2 = aggregate_topk_recovery(per_run_filt, df_filt, \"only nonlinear\")\n",
    "_, _, tmp3 = aggregate_topk_recovery(per_run_filt, df_filt, \"only linear\")\n",
    "df_topk = pd.concat([tmp1, tmp2, tmp3], ignore_index=True)\n",
    "df_topk = df_topk[(df_topk[\"odds_ratio\"] == 1.1) ]\n",
    "df_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 5. Plot\n",
    "# color_mapping = {\n",
    "#     \"only linear\": '#0173B2',\n",
    "#     \"only nonlinear\":  '#029E73',\n",
    "#     \"both\": '#E69F00',\n",
    "# }\n",
    "color_mapping = {\n",
    "    \"only linear\": '#56B4E9',\n",
    "    \"only nonlinear\":  '#009E73',\n",
    "    \"both\": '#E69F00'#'#CC79A7',\n",
    "}\n",
    "\n",
    "# plt.figure(figsize=(7,4))\n",
    "plt.figure(figsize=(figsize_tuple[0]*2, figsize_tuple[1]*1.))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df_topk,\n",
    "    x=\"sigma\",\n",
    "    y=\"topk_recovery\",\n",
    "    hue=\"gene_set_type\",\n",
    "    showfliers=True,\n",
    "    palette=color_mapping\n",
    ")\n",
    "\n",
    "# sns.stripplot(\n",
    "#     data=df_topk,\n",
    "#     x=\"sigma\",\n",
    "#     y=\"topk_recovery\",\n",
    "#     hue=\"gene_set_type\",\n",
    "#     dodge=True,\n",
    "#     color=\"black\",\n",
    "#     alpha=0.25,\n",
    "#     legend=False\n",
    "# )\n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.title(f\"Top-K recovery at OR=1.1 ({df_filt_name}, {eval_set} set)\")\n",
    "plt.ylabel(\"Top-K Recovery\")\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.legend(title=\"Gene Set\", bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0.)\n",
    "# plt.legend(title=\"Gene Set\", loc=\"best\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(FIGDIR, f\"2D_simulations_boxplot_topk_recovery_or1.1_{df_filt_name.replace(',', '').replace(' ', '_')}_{eval_set}.png\"), format='png', dpi=600)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Logic for generating feature importance and rank heatmaps\n",
    "\n",
    "1. Filter the relevant runs\n",
    "- Start by filtering df_feature_importance_paths for the subset you want (e.g., Continuous, N=10k).\n",
    "- Make this filtering modular so you can easily swap to other subsets (e.g., N=1k).\n",
    "\n",
    "2. Extract gene sets for each run\n",
    "- For each row/run, parse the gene lists:\n",
    "    - deltaMuGenes\n",
    "    - mod0_genes and mod1_genes (combine for \"corr module\")\n",
    "- Compute the three gene sets for each run:\n",
    "    - deltaMuGenes\n",
    "    - only in corr module (mod0+mod1 minus deltaMu)\n",
    "    - intersection (deltaMu âˆ© (mod0+mod1))\n",
    "\n",
    "3. Aggregate gene ranks/importances\n",
    "\n",
    "- For each run, use the corresponding group_identifier to get the gene importance/rank DataFrame from df_ranks_by_key (or df_imps_by_key).\n",
    "- For each gene set, extract the ranks/importances for the genes in that set.\n",
    "- Aggregate (e.g., median) across genes in the set, for each run.\n",
    "\n",
    "4. Aggregate across runs with the same group_identifier\n",
    "- For each group_identifier (i.e., each cell in the heatmap), aggregate the median ranks/importances across all runs with that group_identifier.\n",
    "\n",
    "5. Build heatmap DataFrames\n",
    "- For each gene set, build a DataFrame with odds_ratio as rows and sigma as columns, values are the top K recovery (alternatives: aggregated median ranks/importances.)\n",
    "\n",
    "6. Plot heatmaps\n",
    "- Use the same plotting style as elsewhere in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
