{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pnet.data_processing import filter_variants, prostate_data_loaders, utils\n",
    "\n",
    "sys.path.insert(0, '../..') # add project_config to path\n",
    "import project_config\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    _wandb_available = True\n",
    "except ImportError:\n",
    "    _wandb_available = False\n",
    "    print(\"Warning: wandb is not installed. W&B functionality will be unavailable.\")\n",
    "\n",
    "logging.basicConfig(\n",
    "            format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "            level=logging.INFO,\n",
    "            datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb setup\n",
    "# if _wandb_available:\n",
    "#     os.environ['WANDB_NOTEBOOK_NAME'] = \"pipeline_full_vcf_to_germline_patho_vcfs.ipynb\"\n",
    "#     wandb.login()\n",
    "#     run = wandb.init(\n",
    "#         project=\"prostate_met_status\",\n",
    "#         group=\"data_prep_germline_tier12_and_somatic\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "dry_run = True\n",
    "DATADIR = project_config.GERMLINE_DATA_DIR # data directory containing input files for the empirical analysis with P1000 data\n",
    "FIGDIR = project_config.FIGURE_DIR\n",
    "PROCESSED_GERMLINE_VCFS_DIR = project_config.PROCESSED_GERMLINE_VCFS_DIR # os.path.join(DATADIR, \"processed_germline_vcfs\") # location to save processed germline VCFs\n",
    "\n",
    "FULL_VCF_F = os.path.join(DATADIR, \"raw/germline_samples_final_post_ancestry_max_ratio_1330_vt2_VEP_Genotypes.txt\") # may only have the compressed .gz version\n",
    "GENE_LIST_F = os.path.join(DATADIR, \"pathogenic_germline/germline_tier_12_and_somatic.csv\")\n",
    "\n",
    "PROPORTION_THRESHOLD = 0.05\n",
    "ID_COL=\"Uploaded_variation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset the full VCF by a list of genes\n",
    "Since the full VCF is quite large, we will load it in chunks at a time. This initial filtering process can be time consuming, so we will save down the intermediate files to prevent needing to re-run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_id = utils.filename(GENE_LIST_F) # use the filename as the subset identifier\n",
    "genes = utils.read_gene_list_from_csv(GENE_LIST_F) # get the gene list\n",
    "savepath = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f\"prostate_germline_vcf_subset_to_{subset_id}.txt\") # make the savepath\n",
    "\n",
    "# start process of making VCF subset\n",
    "logging.info(f\"working on file {GENE_LIST_F}, with genes: {genes}\")\n",
    "logging.info(f\"will save to {savepath}\")\n",
    "\n",
    "\n",
    "# utils.filter_annotated_vcf_by_gene_list_chunking(annot_vcf_f = FULL_VCF_F, \n",
    "#                                             gene_list=genes, \n",
    "#                                             save_filtered_df_path=savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply universal variant quality filters\n",
    "- variant quality (dp, gq, VAF)\n",
    "- remove artifacts, e.g. exact variant in more than 5% of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VCF_F = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, \"prostate_germline_vcf_subset_to_germline_tier_12_and_somatic.txt\")\n",
    "\n",
    "save_name = utils.filename(VCF_F)\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing universal variant filtering steps\n",
    "logging.info(\"\\nFilter out low-quality variants (universal variant filtering steps)\")\n",
    "\n",
    "vcf_df = pd.read_csv(VCF_F, sep=\"\\t\", low_memory=False)\n",
    "vcf_df = filter_variants.variant_quality_filter(vcf_df, min_dp=10, min_gq=20, min_vaf=0.25, failed_qc_fill=\"./.\")\n",
    "\n",
    "logging.info(\"Filter out benign/likely benign variants using ClinVar annotations\")\n",
    "vcf_df = filter_variants.subset_to_non_benign(vcf_df, clinsig_col = 'ClinVar_updated_2021Jun_CLNSIG')\n",
    "\n",
    "logging.info(\"Merge near-duplicate rows\")\n",
    "vcf_df = filter_variants.merge_near_duplicate_rows_in_vcf(vcf_df, ID_COL)\n",
    "\n",
    "logging.info(f\"Filter out likely artifactual variants (in >{PROPORTION_THRESHOLD} proportion of the dataset's samples)\")\n",
    "vcf_df = filter_variants.remove_vars_too_common_in_dataset_from_annotated_vcf(vcf_df, proportion_threshold = PROPORTION_THRESHOLD)\n",
    "\n",
    "save_name += \"_passed-universal-filters\"\n",
    "save_f = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{save_name}.txt')\n",
    "logging.info(f\"Saving the VCF after universal filtering steps to {save_f}\\n\")\n",
    "passed_universal_filters_vcf = vcf_df.copy()\n",
    "display(passed_universal_filters_vcf)\n",
    "\n",
    "if not dry_run:\n",
    "    logging.info(f\"Saving the VCF after universal filtering steps to {save_f}\\n\")\n",
    "    passed_universal_filters_vcf.to_csv(save_f, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've already run and don't want to re-run the filtering steps, you can load the saved file instead\n",
    "passed_universal_filters_vcf = pd.read_csv(os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{utils.filename(VCF_F)}_passed-universal-filters.txt'), sep=\"\\t\", low_memory=False)\n",
    "passed_universal_filters_vcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply pathogenicity filtration workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varQC_passed_and_patho_only_vcf = filter_variants.variant_selection_workflow(passed_universal_filters_vcf, genes_to_subset=passed_universal_filters_vcf.SYMBOL.unique().tolist())\n",
    "display(varQC_passed_and_patho_only_vcf)\n",
    "save_name = f'{utils.filename(VCF_F)}_passed-universal-filters_patho-vars-only'\n",
    "\n",
    "save_f = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{save_name}.txt')\n",
    "if not dry_run:\n",
    "    logging.info(f\"Saving the VCF after variant QC and pathogenicity filtering steps (shape: {varQC_passed_and_patho_only_vcf.shape}) to: \\n{save_f}\")\n",
    "    varQC_passed_and_patho_only_vcf.to_csv(save_f, index=False, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset variants by some combination of prevalence and impact\n",
    "- rare vs common\n",
    "- high-impact vs moderate impact (LOF vs missense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_vcf(vcf_df, restrict_to_rare, restrict_to_common, keep_high_impact, keep_moderate_impact, base_save_name=\"\", save_dir=\"\"):\n",
    "    final_vcf = vcf_df.copy()\n",
    "    to_add = []\n",
    "    save_name = base_save_name\n",
    "    logging.info(\"Filtering VCFs by variant MAF\")\n",
    "    if restrict_to_rare:\n",
    "        logging.info(\"Filter to rare variants (<1% in gnomad)\")\n",
    "        save_name += \"_rare\"\n",
    "        rare_vcf = filter_variants.subset_to_low_frequency(final_vcf)\n",
    "        logging.debug(f'rare_vcf.shape: {rare_vcf.shape}')\n",
    "        to_add.append(rare_vcf)\n",
    "\n",
    "    if restrict_to_common:\n",
    "        logging.info(\"Filter to common variants (>=1% in gnomad)\")\n",
    "        save_name += \"_common\"\n",
    "        common_vcf = filter_variants.subset_to_high_frequency(final_vcf)\n",
    "        logging.debug(f'common_vcf.shape: {common_vcf.shape}')\n",
    "        to_add.append(common_vcf)\n",
    "\n",
    "    # combine what we have so far - want the impact to be a subset of what we have already\n",
    "    logging.info(f\"Merge {len(to_add)} VCF subsets into one VCF; we will now further subset by variant impact\")\n",
    "    final_vcf = pd.concat(to_add, ignore_index=True)\n",
    "    logging.debug(\"shape before drop dups: {}\".format(final_vcf.shape))\n",
    "    final_vcf = final_vcf.drop_duplicates(subset=[ID_COL])\n",
    "    logging.debug(\"shape after drop dups: {}\".format(final_vcf.shape))\n",
    "    to_add=[]\n",
    "\n",
    "    logging.info(\"Filtering VCFs by variant predicted impact (VEP)\")\n",
    "    if keep_high_impact: # keep high impact aka LOF\n",
    "        logging.info(\"Get the VEP high-impact variants (mostly LOF)\")\n",
    "        save_name += \"_high-impact\"\n",
    "        high_impact_vcf = filter_variants.subset_to_severe_consequence(final_vcf)\n",
    "        logging.debug(f'high_impact_vcf.shape: {high_impact_vcf.shape}')\n",
    "        to_add.append(high_impact_vcf)\n",
    "\n",
    "    if keep_moderate_impact: # keep moderate aka missense\n",
    "        logging.info(\"Get the VEP moderate-impact variants (mostly missense)\")\n",
    "        save_name += \"_moderate-impact\"\n",
    "        moderate_impact_vcf = filter_variants.subset_to_moderate_consequence(final_vcf)\n",
    "        logging.debug(f'moderate_impact_vcf.shape: {moderate_impact_vcf.shape}')\n",
    "        to_add.append(moderate_impact_vcf)\n",
    "\n",
    "    logging.info(f\"Merge {len(to_add)} VCF subsets into one VCF\")\n",
    "    final_vcf = pd.concat(to_add, ignore_index=True)\n",
    "    logging.debug(\"shape before drop dups: {}\".format(final_vcf.shape))\n",
    "    final_vcf = final_vcf.drop_duplicates(subset=[ID_COL])\n",
    "    final_vcf = filter_variants.add_consolidated_consequence(final_vcf)\n",
    "    logging.debug(\"shape after drop dups: {}\".format(final_vcf.shape))\n",
    "\n",
    "    if final_vcf.shape[0] == 0:\n",
    "        logging.info(\"No variants left after filtering by impact\")\n",
    "        return None\n",
    "\n",
    "    if save_dir != \"\":\n",
    "        save_f = os.path.join(save_dir, f'{save_name}.txt')\n",
    "        logging.info(f\"Saving the filtered VCF to {save_f}\\n\")\n",
    "        final_vcf.to_csv(save_f, index=False, sep=\"\\t\")\n",
    "    logging.info(\"Final VCF shape: {}\".format(final_vcf.shape))\n",
    "    return final_vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_save_name = f'{utils.filename(VCF_F)}_passed-universal-filters_patho-vars-only'\n",
    "save_dir = PROCESSED_GERMLINE_VCFS_DIR\n",
    "print(save_dir, '\\n', base_save_name)\n",
    "\n",
    "# NOTE: no common LOF exists in the P1000 dataset. However, leaving this in for completeness in case other datasets have common LOF variants.\n",
    "# TODO: add check for input DF equivalence to avoid redundant runs? Raise warning, maybe if merging with an empty DF during the creation process \n",
    "all_combos_to_run = [\n",
    "    {'rare': val[0], 'common': val[1], 'high-impact': val[2], 'moderate-impact': val[3]}\n",
    "        for val in [\n",
    "            [True, False, True, False], # rare LOF\n",
    "            [True, False, False, True], # rare missense\n",
    "            [False, True, True, False], # common LOF \n",
    "            [False, True, False, True], # common missense\n",
    "            [True, True, True, False], # all LOF (rare + common LOF)\n",
    "            [True, True, False, True], # all missense (rare + common missense)\n",
    "            [True, False, True, True], # all rare (rare LOF + rare missense)\n",
    "            [False, True, True, True], # all common (common LOF + common missense)\n",
    "            [True, True, True, True], # all LOF and missense, common and rare\n",
    "    ]            \n",
    "]\n",
    "\n",
    "for d in all_combos_to_run:\n",
    "    logging.info(\"\\nWorking on {}\".format(d))\n",
    "    if not dry_run:\n",
    "        make_final_vcf(varQC_passed_and_patho_only_vcf, d['rare'], d['common'], d['high-impact'], d['moderate-impact'], \n",
    "                   base_save_name=base_save_name, save_dir=save_dir)\n",
    "    else:\n",
    "        make_final_vcf(varQC_passed_and_patho_only_vcf, d['rare'], d['common'], d['high-impact'], d['moderate-impact'], \n",
    "                   base_save_name=base_save_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done with generating filtered, pathogenic-only VCFs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream exploration and work\n",
    "## Convert variant-level VCF to gene-level binarized genotype matrix (samples x genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_VCF_F = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{base_save_name}_rare_common_high-impact_moderate-impact.txt')\n",
    "germline_df = prostate_data_loaders.get_germline_mutation(FINAL_VCF_F)\n",
    "display(germline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene-level attrition\n",
    "Note: initial gene set had 824 genes, but only 335 remain after our quality filtration and pathogenicity filtration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading this takes time, so comment out if you don't need it\n",
    "# vcf_df = pd.read_csv(VCF_F, sep=\"\\t\", low_memory=False)\n",
    "# passed_universal_filters_vcf = pd.read_csv(os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{utils.filename(VCF_F)}_passed-universal-filters.txt'), sep=\"\\t\", low_memory=False)\n",
    "FINAL_VCF_F = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{base_save_name}_rare_common_high-impact_moderate-impact.txt')\n",
    "v3 = prostate_data_loaders.load_germline_mut(FINAL_VCF_F)\n",
    "\n",
    "print(f\"Initial: {vcf_df.SYMBOL.nunique()} genes, shape: {vcf_df.shape}\")\n",
    "print(f\"After quality filtering: {passed_universal_filters_vcf.SYMBOL.nunique()} genes left, shape: {passed_universal_filters_vcf.shape}\")\n",
    "print(f\"After quality and pathogenicity filtering: {v3.SYMBOL.nunique()} genes left, shape: {v3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _wandb_available:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: variants per sample and samples per variant pre and post pathogenicity filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf_pre_patho_f = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{utils.filename(VCF_F)}_passed-universal-filters.txt')\n",
    "vcf_post_patho_f = os.path.join(PROCESSED_GERMLINE_VCFS_DIR, f'{utils.filename(VCF_F)}_passed-universal-filters_patho-vars-only.txt') # before filtering to _rare_common_high-impact_moderate-impact\n",
    "\n",
    "logging.info(f\"Loading pre-pathogenicity filtering VCF from {vcf_pre_patho_f}\")\n",
    "vcf = pd.read_csv(vcf_pre_patho_f, sep=\"\\t\", low_memory=False)\n",
    "vcf = utils.get_sample_cols_from_VCF(vcf)\n",
    "vcf = vcf.applymap(utils.binarize)\n",
    "\n",
    "logging.info(f\"Loading post-pathogenicity filtering VCF from {vcf_post_patho_f}\")\n",
    "patho_vcf = pd.read_csv(vcf_post_patho_f, sep=\"\\t\", low_memory=False)\n",
    "patho_vcf = utils.get_sample_cols_from_VCF(patho_vcf)\n",
    "patho_vcf = patho_vcf.applymap(utils.binarize)\n",
    "\n",
    "logging.info(f\"Pre-pathogenicity filtering VCF shape: {vcf.shape}\")\n",
    "logging.info(f\"Post-pathogenicity filtering VCF shape: {patho_vcf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_variants_per_sample = vcf.sum(axis=0)\n",
    "n_samples_per_variant = vcf.sum(axis=1)\n",
    "n_variants_per_sample_patho = patho_vcf.sum(axis=0)\n",
    "n_samples_per_variant_patho = patho_vcf.sum(axis=1)\n",
    "\n",
    "logging.info(f\"max # of variants in a single sample: {np.max(n_variants_per_sample)} vs {np.max(n_variants_per_sample_patho)}\")\n",
    "logging.info(f\"max # of samples with a particular variant: {np.max(n_samples_per_variant)} vs {np.max(n_samples_per_variant_patho)}\")\n",
    "\n",
    "logging.info(f\"min # of variants in a single sample: {np.min(n_variants_per_sample)} vs {np.min(n_variants_per_sample_patho)}\")\n",
    "logging.info(f\"min # of samples with a particular variant: {np.min(n_samples_per_variant)} vs {np.min(n_samples_per_variant_patho)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Number of variants with no samples before patho filtration: {np.sum(n_samples_per_variant == 0)}\") # these variants didn't have any samples where they passed my universal filters\n",
    "logging.info(f\"Number of variants with no samples after patho filtration: {np.sum(n_samples_per_variant_patho == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of subplots for visualization\n",
    "patho_color = '#d62728'  # red for pathogenic-only plots\n",
    "before_color = '#7f7f7f'  # gray for pre-pathogenicity filtration plots\n",
    "\n",
    "def add_mean_vline(ax, values, color='#4d4d4d', ls='-', lw=2, offset_frac=0.02):\n",
    "    \"\"\"\n",
    "    Draws a vertical line at the mean of `values` and labels it slightly to the right.\n",
    "    offset_frac: fraction of x-axis width to offset the label from the mean\n",
    "    \"\"\"\n",
    "    m = np.mean(values)\n",
    "    ax.axvline(m, color=color, linestyle=ls, linewidth=lw, label=f'Mean = {m:.2f}')\n",
    "    \n",
    "    # Compute offset in data units\n",
    "    x_range = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "    offset = offset_frac * x_range\n",
    "    \n",
    "    ax.text(\n",
    "        m + offset,\n",
    "        ax.get_ylim()[1] * 0.95,  # 90% up the y-axis\n",
    "        f'{m:.2f}',\n",
    "        ha='left', va='top',\n",
    "        fontsize=10,\n",
    "        color=color\n",
    "    )\n",
    "    return m\n",
    "\n",
    "def add_row_labels(fig, axs, labels, pad=0.02, **textkw):\n",
    "    \"\"\"\n",
    "    Put one label per row to the RIGHT of a subplot grid.\n",
    "    pad: horizontal gap in figure coords from the right edge of the last column.\n",
    "    Extra text properties can be passed via **textkw (e.g., fontsize, bbox).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    axs = np.atleast_2d(axs)\n",
    "    last_col = axs[:, -1]\n",
    "\n",
    "    # x position: a bit right of the last column\n",
    "    x = last_col[0].get_position().x1 + pad\n",
    "\n",
    "    # If there's no room, expand the right margin\n",
    "    if x > 1:\n",
    "        fig.subplots_adjust(right=last_col[0].get_position().x1 - pad)\n",
    "        x = last_col[0].get_position().x1 + pad\n",
    "\n",
    "    for ax, label in zip(last_col, labels):\n",
    "        pos = ax.get_position()            # figure coordinates\n",
    "        y = 0.5 * (pos.y0 + pos.y1)        # vertically center on the row\n",
    "        fig.text(x, y, label, ha='left', va='center', transform=fig.transFigure, **textkw)\n",
    "\n",
    "\n",
    "\n",
    "logging.info(\"making histograms of variants per sample and samples per variant pre and post pathogenicity filtration\")\n",
    "fig, axs = plt.subplots(2, 2, figsize=(5, 4))\n",
    "\n",
    "p1 = utils.n_variants_per_sample_from_vcf(vcf, \n",
    "                                          plot_title=\"Variants per sample\",\n",
    "                                          plot_id=f\"{subset_id}\",\n",
    "                                          ax=axs[0, 0])\n",
    "p2 = utils.n_samples_per_variant_from_vcf(vcf, \n",
    "                                            plot_title=\"Samples per variant\",\n",
    "                                            plot_id=f\"{subset_id}\",\n",
    "                                            ax=axs[0, 1], \n",
    "                                            logscale=True)\n",
    "\n",
    "logging.info(\"making plots, now with pathogenic variants only\")\n",
    "patho_p1 = utils.n_variants_per_sample_from_vcf(patho_vcf, \n",
    "                                                plot_title=\"Variants per sample\",\n",
    "                                                plot_id=f\"{subset_id}\",\n",
    "                                                ax=axs[1, 0])\n",
    "patho_p2 = utils.n_samples_per_variant_from_vcf(patho_vcf, \n",
    "                                                plot_title=\"Samples per variant\",\n",
    "                                                plot_id=f\"{subset_id}\",\n",
    "                                                ax=axs[1, 1], \n",
    "                                            logscale=True)\n",
    "\n",
    "logging.debug(\"Add mean num variants per sample line\")\n",
    "add_mean_vline(p1, n_variants_per_sample, offset_frac=0.04)\n",
    "add_mean_vline(patho_p1, n_variants_per_sample_patho, offset_frac=0.04)\n",
    "\n",
    "logging.debug(\"Change the color of pathogenic-only plots to red (and pre-patho filtration to gray)\")\n",
    "[patch.set_fc(patho_color) for ax in axs[1, :] for patch in ax.patches]\n",
    "[patch.set_fc(before_color) for ax in axs[0, :] for patch in ax.patches]\n",
    "\n",
    "logging.debug(\"Add row labels to the right of the second column\")\n",
    "pre_patho_label = f'Pre-pathogenicity filtration\\nn={vcf.shape[0]} variants'\n",
    "patho_label = f'Pathogenic only\\nn={patho_vcf.shape[0]} variants'\n",
    "add_row_labels(\n",
    "    fig, axs,\n",
    "    labels=[pre_patho_label, patho_label],\n",
    "    pad=0.10,\n",
    "    fontsize=12,\n",
    "    # bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='black', alpha=0.8)\n",
    ")\n",
    "\n",
    "\n",
    "# Clear titles for second row\n",
    "for ax in axs[1, :]:\n",
    "    ax.set_title('')\n",
    "\n",
    "# Clear x-axis labels for first row\n",
    "for ax in axs[0, :]:\n",
    "    ax.set_xlabel('')  \n",
    "\n",
    "# fig.suptitle(f\"{subset_id}\\n(red is post pathogenic-only filter)\")\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "\n",
    "# Save figure\n",
    "save_path = os.path.join(FIGDIR, f\"{subset_id}\", f\"variant_and_sample_dists_{subset_id}.png\")\n",
    "utils.savefig(save_path)\n",
    "\n",
    "# Show the figure\n",
    "plt.savefig(save_path, format='png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pnet3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
