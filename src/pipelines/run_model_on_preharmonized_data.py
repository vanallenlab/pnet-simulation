# Script to run germline:somatic experiment P-NET on pre-formatted prostate germline and/or somatic data
# Related to the `run_pnet.py` script, which was generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import logging
import os

import configargparse
import pandas as pd
import torch

import wandb
from pnet import Pnet, pnet_loader, report_and_eval
from pnet.utils import modeling_utils

logging.basicConfig(
    encoding="utf-8",
    format="%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S",
    force=True,
)

logger = logging.getLogger(__name__)


def parse_arguments():
    parser = configargparse.ArgumentParser(description="Description of your script")
    parser.add("--config_f", type=str, required=False, is_config_file=True, help="Path to a config file")
    parser.add("--data_config_f", type=str, required=True, is_config_file=False, help="Path to a config file")
    parser.add(
        "--datasets",
        default=["somatic_amp", "somatic_del", "somatic_mut"],
        help="List of datasets to use",
        action=modeling_utils.ParseAction,
    )  # NOTE: I don't use nargs = "+" because it doesn't work with wandb sweep
    parser.add(
        "--evaluation_set",
        default="validation",
        choices=["validation", "test"],
        help="Evaluation set (validation or test)",
    )
    parser.add("--model_type", default="bdt", choices=["bdt", "rf", "pnet"], help="Type of model")
    parser.add("--wandb_group", default="", help="Wandb group name")
    parser.add("--wandb_project", default="prostate_met_status", help="Wandb group name")
    parser.add("--seed", type=int, default=123, help="Seed value")
    parser.add("--input_data_dir", help="Directory with model-ready data")
    parser.add(
        "--data_split_dir",
        default="../../pnet_germline/data/pnet_database/prostate/splits",
        help="Directory with data split files",
    )
    parser.add(
        "--input_data_wandb_id",
        default="",
        help="W&B run ID that created the data in the input_data_dir, if applicable",
    )
    parser.add(
        "--cpus", type=int, required=False, help="Define the number of CPUs PyTorch uses during parallelization tasks"
    )
    parser.add(
        "--input_dropout", default=0.5, type=float, help="Proportion of dropout between the input layer and gene layer"
    )
    parser.add(
        "--h1_alpha",
        default=0,
        type=float,
        help="Strength of regularization on weights going to the first hidden layer (genes); default to no regularization",
    )
    parser.add(
        "--h1_regularization_method",
        default="l1",
        choices=["l1", "l2", "elasticnet"],
        help="Type of regularization on weights going to the first hidden layer (genes)",
    )
    parser.add(
        "--l1_ratio",
        required=False,
        default=0.5,
        type=float,
        help="Control the amount of L1 vs L2 regularization when using ElasticNet",
    )
    parser.add(
        "--epochs",
        type=int,
        default=500,
        help="Number of epochs to train the model (default: 500)",
    )
    parser.add(
        "--max_depth",
        type=int,
        default=None,
        help="Maximum depth of the tree for Random Forest (default: None, which means no limit. Use with caution as it can lead to overfitting)",
    )
    parser.add(
        "--min_samples_leaf",
        type=int,
        default=1,
        help="Minimum number of samples required to be at a leaf node for Random Forest (default: 1)",
    )
    parser.add(
        "--min_samples_split",
        type=int,
        default=50,
        help="Define the min number of samples used to make RF split (best practice usually 5-10% of dataset)",
    )
    return parser.parse_args()


def load_input_data(config, input_dir):
    """Load genetic, confounder, and target data from disk."""
    genetic_data = {}
    for name, info in config["genetic_data"].items():
        path = os.path.join(input_dir, info["filename"])
        df = pd.read_csv(path, index_col=0)
        genetic_data[name] = df

    confounder_df = pd.read_csv(os.path.join(input_dir, config["confounder_data"]["filename"]), index_col=0)
    target_df = pd.read_csv(os.path.join(input_dir, config["target"]["filename"]), index_col=0)

    return genetic_data, confounder_df, target_df


def main():
    args = parse_arguments()
    wandb.login()
    wandb.init(project=args.wandb_project, group=args.wandb_group)

    logger.info(f"Starting run with ID: {wandb.run.id}")
    # Set environment
    Pnet.set_random_seeds(args.seed, turn_off_cuDNN=False)
    torch.set_num_threads(args.cpus)

    # Load config and data
    config = modeling_utils.read_config(args.data_config_f)
    genetic_data, confounder_df, y = load_input_data(config, args.input_data_dir)
    genetic_data = {k: v for k, v in genetic_data.items() if k in args.datasets}

    # Load splits
    # train_inds, eval_inds, train_f, eval_f = get_train_eval_indices(args.data_split_dir, args.evaluation_set)
    train_inds, validation_inds, test_inds = modeling_utils.get_train_val_test_indices(args.data_split_dir)
    if args.evaluation_set == "validation":
        eval_inds = validation_inds
    elif args.evaluation_set == "test":
        eval_inds = test_inds

    # Setup save path
    save_dir = modeling_utils.setup_save_dir(args.model_type, args.evaluation_set, args.wandb_group, wandb.run.id)

    # Log info
    logger.info(f"Training on datasets: {list(genetic_data.keys())}")
    report_and_eval.report_df_info_with_names(genetic_data, n=5)
    report_and_eval.report_df_info_with_names({"confounders": confounder_df, "y": y}, n=5)

    # Build hparams
    hparams = {
        "wandb_run_id_that_created_inputs": args.input_data_wandb_id,
        "nbr_gene_inputs": len(genetic_data),
        "dropout": 0.2,
        "input_dropout": args.input_dropout,
        "additional_dims": confounder_df.shape[1],
        "output_dim": 1,
        "lr": 1e-3,
        "weight_decay": 1e-3,
        "epochs": args.epochs,
        "early_stopping": True,
        "batch_size": 64,
        "verbose": True,
        "data_split_dir": args.data_split_dir,
        "train_set_indices": train_inds,
        "evaluation_set_indices": eval_inds,
        "test_set_indices": test_inds,
        "datasets": list(genetic_data.keys()),
        "random_seed": args.seed,
        "model_type": args.model_type,
        "evaluation_set": args.evaluation_set,
        "save_dir": save_dir,
        "delete_model_after_training": True,  # Set to True to delete the model file after training
    }
    wandb.config.update(hparams)

    # Training
    logger.info("Prepping datasets & training...")
    test_dataset = pnet_loader.generate_dataset_from_indices(
        genetic_data=genetic_data,
        target=y,
        dset_inds=test_inds,
        additional_data=confounder_df,
        gene_set=None,
        seed=args.seed,
    )
    if args.model_type in ["rf", "bdt"]:
        train_dataset, validation_dataset = pnet_loader.generate_train_test(
            genetic_data,
            additional_data=confounder_df,
            target=y,
            train_inds=train_inds,
            test_inds=eval_inds,
            gene_set=None,
        )
        # concatenate additional data to the input_df and x to match RF/BDT expectations
        train_dataset.input_df = pd.concat([train_dataset.input_df, train_dataset.additional_data], axis=1)
        train_dataset.x = torch.cat([train_dataset.x, train_dataset.additional], dim=1)
        validation_dataset.input_df = pd.concat(
            [validation_dataset.input_df, validation_dataset.additional_data], axis=1
        )
        validation_dataset.x = torch.cat([validation_dataset.x, validation_dataset.additional], dim=1)
        test_dataset.input_df = pd.concat([test_dataset.input_df, test_dataset.additional_data], axis=1)
        test_dataset.x = torch.cat([test_dataset.x, test_dataset.additional], dim=1)

        if args.model_type == "rf":
            rf_params = {"min_samples_leaf": args.min_samples_leaf, "min_samples_split": args.min_samples_split}
            wandb.config.update(rf_params)
            model = modeling_utils.train_model_rf(
                train_dataset,
                min_samples_split=args.min_samples_split,
                max_depth=args.max_depth,
                min_samples_leaf=args.min_samples_leaf,
                random_seed=args.seed,
            )
        else:
            model, _, _ = modeling_utils.train_model_bdt(train_dataset, validation_dataset, args.evaluation_set)

    elif args.model_type == "pnet":
        model, _, _, train_dataset, validation_dataset, model_save_path = modeling_utils.train_model_pnet(
            hparams, genetic_data, confounder_df, y
        )

    # Evaluate and log results
    logger.info("Model training complete. Evaluating model...")
    modeling_utils.evaluate_on_train_val_test(model, train_dataset, validation_dataset, test_dataset, hparams)

    # Clean up bulky pytorch model file if specified
    if args.model_type == "pnet":
        modeling_utils.cleanup(
            model_save_path=model_save_path,
            delete_model_after_training=hparams["delete_model_after_training"],
        )

    logger.info("Run complete. Finishing wandb.")
    wandb.finish()
    return wandb.run.id


if __name__ == "__main__":
    main()
