# Script to run germline:somatic experiment P-NET on pre-formatted prostate germline and/or somatic data
# Author: Gwen Miller
# Related to the `run_pnet.py` script, which was generated by copying over and then editing everything in the file: `pnet/notebooks/prostate_metastatic_prediction_example.ipynb`

import logging
import os
import re

import configargparse
import pandas as pd
import torch

import wandb
from pnet import Pnet, pnet_loader, report_and_eval
from pnet.utils import modeling_utils

logging.basicConfig(
    encoding="utf-8",
    format="%(asctime)s %(levelname)-8s [%(name)s] %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S",
    force=True,
)

logger = logging.getLogger(__name__)


def parse_arguments():
    parser = configargparse.ArgumentParser(description="Description of your script")
    parser.add("--config_f", type=str, required=False, is_config_file=True, help="Path to a config file")
    parser.add("--data_config_f", type=str, required=True, is_config_file=False, help="Path to a config file")
    parser.add(
        "--perturbed_data_config_f",
        type=str,
        required=False,
        is_config_file=False,
        help="Path to a config file for the perturbed data",
    )
    parser.add(
        "--datasets",
        default=["somatic_amp", "somatic_del", "somatic_mut"],
        help="List of datasets to use",
        action=modeling_utils.ParseAction,
    )  # NOTE: I don't use nargs = "+" because it doesn't work with wandb sweep
    parser.add(
        "--evaluation_set",
        default="validation",
        choices=["validation", "test"],
        help="Evaluation set (validation or test)",
    )
    parser.add("--model_type", default="bdt", choices=["bdt", "rf", "pnet"], help="Type of model")
    parser.add("--wandb_group", default="", help="Wandb group name")
    parser.add("--wandb_project", default="prostate_met_status", help="Wandb group name")
    parser.add("--seed", type=int, default=123, help="Seed value")
    parser.add("--input_data_dir", help="Directory with model-ready data")
    parser.add(
        "--data_split_dir",
        default="../../pnet_germline/data/pnet_database/prostate/splits",
        help="Directory with data split files",
    )
    parser.add(
        "--input_data_wandb_id",
        default="",
        help="W&B run ID that created the data in the input_data_dir, if applicable",
    )
    # parser.add('--genetic_data', type=yaml.load, help="TODO: dictionary-style yaml")
    parser.add(
        "--cpus", type=int, required=False, help="Define the number of CPUs PyTorch uses during parallelization tasks"
    )
    parser.add(
        "--min_samples_split",
        type=int,
        default=50,
        help="Define the min number of samples used to make RF split (best practice usually 5-10% of dataset)",
    )
    parser.add(
        "--input_dropout", default=0.5, type=float, help="Proportion of dropout between the input layer and gene layer"
    )
    parser.add(
        "--h1_alpha",
        default=0,
        type=float,
        help="Strength of regularization on weights going to the first hidden layer (genes); default to no regularization",
    )
    parser.add(
        "--h1_regularization_method",
        default="l1",
        choices=["l1", "l2", "elasticnet"],
        help="Type of regularization on weights going to the first hidden layer (genes)",
    )
    parser.add(
        "--l1_ratio",
        required=False,
        default=0.5,
        type=float,
        help="Control the amount of L1 vs L2 regularization when using ElasticNet",
    )
    parser.add(
        "--epochs",
        type=int,
        default=500,
        help="Number of epochs to train the model (default: 500)",
    )
    parser.add("--perturbed_data_dir", type=str, default=None, help="Path to perturbed data directory")
    parser.add(
        "--perturbed_data_wandb_id",
        type=str,
        default=None,
        help="W&B run ID that created the perturbed/simulated data in the perturbed_data_dir, if applicable",
    )
    parser.add(
        "--perturbation_suffix",
        type=str,
        default=None,
        help="Suffix for perturbation setting as defined in the perturbation config file. This suffix is the identifier for the perturbation that was applied to the data.",
    )
    parser.add(
        "--max_depth",
        type=int,
        default=None,
        help="Maximum depth of the tree for Random Forest (default: None, which means no limit. Use with caution as it can lead to overfitting)",
    )
    parser.add(
        "--min_samples_leaf",
        type=int,
        default=1,
        help="Minimum number of samples required to be at a leaf node for Random Forest (default: 1)",
    )

    return parser.parse_args()


def load_input_data(config, input_dir, perturbed_data_dir=None, perturbed_target=None, perturbed_somatic_mut=None):
    """Load genetic, confounder, and target data from disk, with optional overrides for perturbation testing."""
    genetic_data = {}
    for name, info in config["genetic_data"].items():
        if name == "somatic_mut" and perturbed_somatic_mut:
            path = os.path.join(perturbed_data_dir, perturbed_somatic_mut)
            logger.info(f"Using perturbed somatic_mut: {path}")
        else:
            path = os.path.join(input_dir, info["filename"])
            logger.debug(f"Using default {name}: {path}")
        genetic_data[name] = pd.read_csv(path, index_col=0)

    confounder_df = pd.read_csv(os.path.join(input_dir, config["confounder_data"]["filename"]), index_col=0)

    if perturbed_target:
        target_path = os.path.join(perturbed_data_dir, perturbed_target)
        logger.info(f"Using perturbed target: {target_path}")
    else:
        target_path = os.path.join(input_dir, config["target"]["filename"])
        logger.debug(f"Using default target: {target_path}")

    target_df = pd.read_csv(target_path, index_col=0)

    return genetic_data, confounder_df, target_df


def main():
    wandb.login()
    args = parse_arguments()
    wandb.init(group=args.wandb_group)

    # Set environment
    Pnet.set_random_seeds(args.seed, turn_off_cuDNN=False)
    torch.set_num_threads(args.cpus)

    logger.debug("Load configs")
    config = modeling_utils.read_config(
        args.data_config_f
    )  # TODO: this is the way I know how to handle dictinary-style parameters in a config file. Update perturbation_setting based on this?

    if args.perturbed_data_config_f:
        perturbed_data_config = modeling_utils.read_config(args.perturbed_data_config_f)
        logger.debug(f"Loaded perturbed data config: {perturbed_data_config}")
        args.perturbed_somatic_mut = perturbed_data_config["perturbed_somatic_mut"][args.perturbation_suffix][
            "data_file"
        ]
        args.perturbed_target = perturbed_data_config["perturbed_somatic_mut"][args.perturbation_suffix]["target_file"]
    else:
        logger.warning("No perturbed data config file provided. Falling back to normal data.")

    genetic_data, confounder_df, y = load_input_data(
        config,
        args.input_data_dir,
        perturbed_data_dir=args.perturbed_data_dir,
        perturbed_target=args.perturbed_target,
        perturbed_somatic_mut=args.perturbed_somatic_mut,
    )
    genetic_data = {
        k: v for k, v in genetic_data.items() if k in args.datasets
    }  # TODO: this could break if you don't realize you aren't including the perturbed dataset

    # Load splits
    # train_inds, eval_inds, train_f, eval_f = get_train_eval_indices(args.data_split_dir, args.evaluation_set)
    train_inds, validation_inds, test_inds = modeling_utils.get_train_val_test_indices(args.data_split_dir)
    if args.evaluation_set == "validation":
        eval_inds = validation_inds
    elif args.evaluation_set == "test":
        eval_inds = test_inds

    # Setup save path
    save_dir = modeling_utils.setup_save_dir(args.model_type, args.evaluation_set, args.wandb_group, wandb.run.id)

    # Log info
    logger.info(f"Training on datasets: {list(genetic_data.keys())}")
    report_and_eval.report_df_info_with_names(genetic_data, n=5)
    report_and_eval.report_df_info_with_names({"confounders": confounder_df, "y": y}, n=5)

    # Build hparams
    hparams = {
        "wandb_run_id_that_created_inputs": args.input_data_wandb_id,
        "nbr_gene_inputs": len(genetic_data),
        "dropout": 0.2,
        "input_dropout": args.input_dropout,
        "additional_dims": confounder_df.shape[1],
        "output_dim": 1,
        "lr": 1e-3,
        "weight_decay": 1e-3,
        "epochs": args.epochs,
        "early_stopping": True,
        "batch_size": 64,
        "verbose": True,
        "data_split_dir": args.data_split_dir,
        "train_set_indices": train_inds,
        "evaluation_set_indices": eval_inds,
        "test_set_indices": test_inds,
        "datasets": list(genetic_data.keys()),
        "random_seed": args.seed,
        "model_type": args.model_type,
        "evaluation_set": args.evaluation_set,
        "save_dir": save_dir,
        "delete_model_after_training": True,  # Set to True to delete the model file after training
        "min_samples_split": args.min_samples_split,
        "max_depth": args.max_depth,
    }

    if args.perturbed_data_dir:
        hparams["n_features"] = genetic_data["somatic_mut"].shape[1]
        hparams["perturbed_data_dir"] = args.perturbed_data_dir
        hparams["perturbed_target"] = args.perturbed_target
        hparams["perturbed_somatic_mut"] = args.perturbed_somatic_mut
        hparams["perturbation_suffix"] = args.perturbation_suffix
        hparams["wandb.run.id_that_created_perturbed_inputs"] = args.perturbed_data_wandb_id

        match = re.search(
            r"gene-(?P<gene>[A-Za-z0-9]+)_OR-(?P<odds_ratio>[0-9p]+)_ctrlFreq-(?P<control_freq>[0-9p]+)",
            args.perturbation_suffix,
        )
        if match:
            perturbed_gene = match.group("gene")
            odds_ratio = float(match.group("odds_ratio").replace("p", "."))
            control_frequency = float(match.group("control_freq").replace("p", "."))
            hparams["perturbed_gene"] = perturbed_gene
            hparams["odds_ratio"] = odds_ratio
            hparams["control_frequency"] = control_frequency
            logger.debug(
                f"Parsed perturbation_suffix: gene={perturbed_gene}, odds_ratio={odds_ratio}, control_frequency={control_frequency}"
            )
        else:
            logger.warning(f"Could not parse perturbation_suffix: {args.perturbation_suffix}")

    # Adding hparams to wandb config
    wandb.config.update(hparams)

    # Training
    logger.info("Prepping datasets & training...")
    test_dataset = pnet_loader.generate_dataset_from_indices(
        genetic_data=genetic_data,
        target=y,
        dset_inds=test_inds,
        additional_data=confounder_df,
        gene_set=None,
        seed=args.seed,
    )
    if args.model_type in ["rf", "bdt"]:
        train_dataset, validation_dataset = pnet_loader.generate_train_test(
            genetic_data,
            additional_data=confounder_df,
            target=y,
            train_inds=train_inds,
            test_inds=eval_inds,
            gene_set=None,
        )
        # concatenate additional data to the input_df and x to match RF/BDT expectations
        train_dataset.input_df = pd.concat([train_dataset.input_df, train_dataset.additional_data], axis=1)
        train_dataset.x = torch.cat([train_dataset.x, train_dataset.additional], dim=1)
        validation_dataset.input_df = pd.concat(
            [validation_dataset.input_df, validation_dataset.additional_data], axis=1
        )
        validation_dataset.x = torch.cat([validation_dataset.x, validation_dataset.additional], dim=1)
        test_dataset.input_df = pd.concat([test_dataset.input_df, test_dataset.additional_data], axis=1)
        test_dataset.x = torch.cat([test_dataset.x, test_dataset.additional], dim=1)

        if args.model_type == "rf":
            model = modeling_utils.train_model_rf(
                train_dataset,
                min_samples_split=args.min_samples_split,
                max_depth=args.max_depth,
                random_seed=args.seed,
                min_samples_leaf=args.min_samples_leaf,
            )
        else:
            model, _, _ = modeling_utils.train_model_bdt(train_dataset, validation_dataset, args.evaluation_set)

    elif args.model_type == "pnet":
        model, _, _, train_dataset, validation_dataset, model_save_path = modeling_utils.train_model_pnet(
            hparams, genetic_data, confounder_df, y
        )

    # Evaluate and log results
    logger.info("Model training complete. Evaluating model...")
    modeling_utils.evaluate_on_train_val_test(model, train_dataset, validation_dataset, test_dataset, hparams)

    # Clean up bulky pytorch model file if specified
    if args.model_type == "pnet":
        modeling_utils.cleanup(
            model_save_path=model_save_path,
            delete_model_after_training=hparams["delete_model_after_training"],
        )

    logger.info("Run complete. Finishing wandb.")
    wandb.finish()
    return


if __name__ == "__main__":
    main()
